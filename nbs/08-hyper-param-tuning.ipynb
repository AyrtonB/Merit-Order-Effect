{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from moepy import lowess, eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2018'\n",
    "end_date = '2019'\n",
    "    \n",
    "df_EI = eda.load_EI_df('../data/electric_insights.csv')\n",
    "df_EI_model = df_EI[start_date:end_date][['day_ahead_price', 'demand', 'solar', 'wind']].dropna()\n",
    "\n",
    "s_price = df_EI_model['day_ahead_price']\n",
    "s_dispatchable = df_EI_model['demand'] - df_EI_model[['solar', 'wind']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"4\" value=\"4\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">4/4</span>\n",
       "<span class=\"Time-label\">[00:02<00:00, 0.38s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 4/4 [00:02<00:00, 0.38s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ayrto\\desktop\\phd\\analysis\\merit-order-effect\\moepy\\lowess.py:239: RuntimeWarning: invalid value encountered in true_divide\n",
      "  loading_weights = loading_weights/loading_weights.sum(axis=0) # normalising\n"
     ]
    }
   ],
   "source": [
    "x = s_dispatchable\n",
    "y = s_price\n",
    "dt_idx=s_dispatchable.index\n",
    "reg_dates = pd.date_range(f'{start_date}-01-01', f'{end_date}-01-01', freq='13W')\n",
    "\n",
    "smooth_dates = lowess.SmoothDates(frac=0.3, threshold_value=26)\n",
    "\n",
    "smooth_dates.fit(x, y, dt_idx=dt_idx, num_fits=10, reg_dates=reg_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Monkey Patching `skopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import rankdata\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "from ipypb import track\n",
    "from warnings import warn\n",
    "from functools import partial\n",
    "from distutils.dir_util import copy_tree\n",
    "from collections.abc import Iterable, Sized\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import is_classifier, clone\n",
    "from sklearn.utils.validation import indexable\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import check_scoring\n",
    "except ImportError:\n",
    "    from sklearn.metrics.scorer import check_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_search_CV_init(self, estimator, search_spaces, optimizer_kwargs=None,\n",
    "                         n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n",
    "                         n_points=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                         pre_dispatch='2*n_jobs', random_state=None,\n",
    "                         error_score='raise', return_train_score=False):\n",
    "\n",
    "        self.search_spaces = search_spaces\n",
    "        self.n_iter = n_iter\n",
    "        self.n_points = n_points\n",
    "        self.random_state = random_state\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self._check_search_space(self.search_spaces)\n",
    "        self.fit_params = fit_params\n",
    "        self.iid = None\n",
    "\n",
    "        super(BayesSearchCV, self).__init__(\n",
    "             estimator=estimator, scoring=scoring,\n",
    "             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n",
    "             pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "             return_train_score=return_train_score)\n",
    "\n",
    "BayesSearchCV.__init__ = bayes_search_CV_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_search_CV__fit(self, X, y, groups, parameter_iterable):\n",
    "    \"\"\"\n",
    "    Actual fitting,  performing the search over parameters.\n",
    "    Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X\n",
    "                .../sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "    estimator = self.estimator\n",
    "    cv = sklearn.model_selection._validation.check_cv(\n",
    "        self.cv, y, classifier=is_classifier(estimator))\n",
    "    self.scorer_ = check_scoring(\n",
    "        self.estimator, scoring=self.scoring)\n",
    "\n",
    "    X, y, groups = indexable(X, y, groups)\n",
    "    n_splits = cv.get_n_splits(X, y, groups)\n",
    "    if self.verbose > 0 and isinstance(parameter_iterable, Sized):\n",
    "        n_candidates = len(parameter_iterable)\n",
    "        print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "              \" {2} fits\".format(n_splits, n_candidates,\n",
    "                                 n_candidates * n_splits))\n",
    "\n",
    "    base_estimator = clone(self.estimator)\n",
    "    pre_dispatch = self.pre_dispatch\n",
    "\n",
    "    cv_iter = list(cv.split(X, y, groups))\n",
    "    out = Parallel(\n",
    "        n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "        pre_dispatch=pre_dispatch\n",
    "    )(delayed(sklearn.model_selection._validation._fit_and_score)(\n",
    "            clone(base_estimator),\n",
    "            X, y, self.scorer_,\n",
    "            train, test, self.verbose, parameters,\n",
    "            fit_params=self.fit_params,\n",
    "            return_train_score=self.return_train_score,\n",
    "            return_n_test_samples=True,\n",
    "            return_times=True, return_parameters=True,\n",
    "            error_score=self.error_score\n",
    "        )\n",
    "        for parameters in parameter_iterable\n",
    "        for train, test in cv_iter)\n",
    "\n",
    "    # if one choose to see train score, \"out\" will contain train score info\n",
    "    if self.return_train_score:\n",
    "        (train_scores, test_scores, n_test_samples,\n",
    "         fit_time, score_time, parameters) = zip(*out)\n",
    "    else:\n",
    "        from warnings import warn\n",
    "        (fit_failed, test_scores, n_test_samples,\n",
    "         fit_time, score_time, parameters) = zip(*[a.values() for a in out])\n",
    "\n",
    "    candidate_params = parameters[::n_splits]\n",
    "    n_candidates = len(candidate_params)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    def _store(key_name, array, weights=None, splits=False, rank=False):\n",
    "        \"\"\"A small helper to store the scores/times to the cv_results_\"\"\"\n",
    "        array = np.array(array, dtype=np.float64).reshape(n_candidates,\n",
    "                                                          n_splits)\n",
    "        if splits:\n",
    "            for split_i in range(n_splits):\n",
    "                results[\"split%d_%s\"\n",
    "                        % (split_i, key_name)] = array[:, split_i]\n",
    "\n",
    "        array_means = np.average(array, axis=1, weights=weights)\n",
    "        results['mean_%s' % key_name] = array_means\n",
    "        # Weighted std is not directly available in numpy\n",
    "        array_stds = np.sqrt(np.average((array -\n",
    "                                         array_means[:, np.newaxis]) ** 2,\n",
    "                                        axis=1, weights=weights))\n",
    "        results['std_%s' % key_name] = array_stds\n",
    "\n",
    "        if rank:\n",
    "            results[\"rank_%s\" % key_name] = np.asarray(\n",
    "                rankdata(-array_means, method='min'), dtype=np.int32)\n",
    "\n",
    "    # Computed the (weighted) mean and std for test scores alone\n",
    "    # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples\n",
    "    n_test_samples = np.array(n_test_samples[:n_splits],\n",
    "                                  dtype=np.int)\n",
    "\n",
    "    _store('test_score', test_scores, splits=True, rank=True,\n",
    "           weights=n_test_samples if self.iid else None)\n",
    "    if self.return_train_score:\n",
    "        _store('train_score', train_scores, splits=True)\n",
    "    _store('fit_time', fit_time)\n",
    "    _store('score_time', score_time)\n",
    "\n",
    "    best_index = np.flatnonzero(results[\"rank_test_score\"] == 1)[0]\n",
    "    best_parameters = candidate_params[best_index]\n",
    "\n",
    "    # Use one MaskedArray and mask all the places where the param is not\n",
    "    # applicable for that candidate. Use defaultdict as each candidate may\n",
    "    # not contain all the params\n",
    "    param_results = defaultdict(partial(np.ma.array,\n",
    "                                        np.empty(n_candidates,),\n",
    "                                        mask=True,\n",
    "                                        dtype=object))\n",
    "    for cand_i, params in enumerate(candidate_params):\n",
    "        for name, value in params.items():\n",
    "            # An all masked empty array gets created for the key\n",
    "            # `\"param_%s\" % name` at the first occurence of `name`.\n",
    "            # Setting the value at an index also unmasks that index\n",
    "            param_results[\"param_%s\" % name][cand_i] = value\n",
    "\n",
    "    results.update(param_results)\n",
    "\n",
    "    # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params'\n",
    "    results['params'] = candidate_params\n",
    "\n",
    "    self.cv_results_ = results\n",
    "    self.best_index_ = best_index\n",
    "    self.n_splits_ = n_splits\n",
    "\n",
    "    if self.refit:\n",
    "        # fit the best estimator using the entire dataset\n",
    "        # clone first to work around broken estimators\n",
    "        best_estimator = clone(base_estimator).set_params(\n",
    "            **best_parameters)\n",
    "        if y is not None:\n",
    "            best_estimator.fit(X, y, **self.fit_params)\n",
    "        else:\n",
    "            best_estimator.fit(X, **self.fit_params)\n",
    "        self.best_estimator_ = best_estimator\n",
    "    return self\n",
    "\n",
    "BayesSearchCV._fit = bayes_search_CV__fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 1 candidates, totalling 8 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   10.6s remaining:   32.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1!=4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 620, in _fit_and_score\n    test_scores = _score(estimator, X_test, y_test, scorer, error_score)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 674, in _score\n    scores = scorer(estimator, X_test, y_test)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 397, in _passthrough_scorer\n    return estimator.score(*args, **kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\base.py\", line 554, in score\n    return r2_score(y, y_pred, sample_weight=sample_weight)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 676, in r2_score\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n  File \"C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 99, in _check_reg_targets\n    raise ValueError(\"y_true and y_pred have different number of output \"\nValueError: y_true and y_pred have different number of output (1!=4)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9857aca98615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfit_BayesSearchCV\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Cross-validation score: {opt.best_score_:.2f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, callback)\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[0mn_points_adjusted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m                 optim_result = self._step(\n\u001b[0m\u001b[0;32m    697\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                     \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_points_adjusted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\searchcv.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, X, y, search_space, optimizer, groups, n_points)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0mrefit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-07e5e60b06c8>\u001b[0m in \u001b[0;36mbayes_search_CV__fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mcv_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     out = Parallel(\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\MOE\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (1!=4)"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "search_spaces = {\n",
    "        'frac': Real(0.3, 1, 'uniform'),\n",
    "        'threshold_value': Integer(1, 52, 'uniform')\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    'reg_dates': reg_dates, \n",
    "    'num_fits': 10\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    smooth_dates,\n",
    "    search_spaces,\n",
    "    n_iter=2,\n",
    "    verbose=1,\n",
    "    cv=8, # 8 works well for me as that's how many concurrent workers I can use\n",
    "    #scoring=make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    fit_params=fit_params,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "fit_BayesSearchCV = True\n",
    "\n",
    "if fit_BayesSearchCV == True:\n",
    "    opt.fit(x, y)\n",
    "\n",
    "    print(f'Cross-validation score: {opt.best_score_:.2f}')\n",
    "    #print(f'Hold-out score: {opt.score(x_test, y_test):.2f}')\n",
    "    print(f'\\nBest params: \\n{opt.best_params_}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOE",
   "language": "python",
   "name": "moe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines the hyper-parameter optimisation procedure used to tune the models\n",
    "\n",
    "<br>\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from moepy import lowess, eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Data Loading\n",
    "\n",
    "We'll start with the GB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local_datetime\n",
       "2009-01-01 00:00:00+00:00    38.181\n",
       "2009-01-01 00:30:00+00:00    38.304\n",
       "2009-01-01 01:00:00+00:00    37.839\n",
       "2009-01-01 01:30:00+00:00    36.716\n",
       "2009-01-01 02:00:00+00:00    36.020\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_EI = eda.load_EI_df('../data/raw/electric_insights.csv')\n",
    "df_EI_model = df_EI[['day_ahead_price', 'demand', 'solar', 'wind']].dropna()\n",
    "\n",
    "s_price = df_EI_model['day_ahead_price']\n",
    "s_dispatchable = df_EI_model['demand'] - df_EI_model[['solar', 'wind']].sum(axis=1)\n",
    "\n",
    "s_dispatchable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "then also load in the DE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DE = eda.load_DE_df('../data/raw/energy_charts.csv', '../data/raw/ENTSOE_DE_price.csv')\n",
    "\n",
    "df_DE_model = df_DE[['price', 'demand', 'Solar', 'Wind']].dropna()\n",
    "\n",
    "s_DE_demand = df_DE_model['demand']\n",
    "s_DE_price = df_DE_model['price']\n",
    "s_DE_dispatchable = df_DE_model['demand'] - df_DE_model[['Solar', 'Wind']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Monkey Patching `skopt`\n",
    "\n",
    "Due to some changes in the latest release of `scikit-learn` several classes and functions in `skopt` were broken at the time this research was carried out. This section provides code for monkey-patching `skopt` to ensure that it continues working.\n",
    "\n",
    "We'll start by loading in the relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import rankdata\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "from ipypb import track\n",
    "from warnings import warn\n",
    "from functools import partial\n",
    "from distutils.dir_util import copy_tree\n",
    "from collections.abc import Iterable, Sized\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import is_classifier, clone\n",
    "from sklearn.utils.validation import indexable\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import check_scoring\n",
    "except ImportError:\n",
    "    from sklearn.metrics.scorer import check_scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll re-define the `bayes_search_CV_init` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_search_CV_init(self, estimator, search_spaces, optimizer_kwargs=None,\n",
    "                         n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n",
    "                         n_points=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                         pre_dispatch='2*n_jobs', random_state=None,\n",
    "                         error_score='raise', return_train_score=False):\n",
    "\n",
    "        self.search_spaces = search_spaces\n",
    "        self.n_iter = n_iter\n",
    "        self.n_points = n_points\n",
    "        self.random_state = random_state\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self._check_search_space(self.search_spaces)\n",
    "        self.fit_params = fit_params\n",
    "        self.iid = None\n",
    "\n",
    "        super(BayesSearchCV, self).__init__(\n",
    "             estimator=estimator, scoring=scoring,\n",
    "             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n",
    "             pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "             return_train_score=return_train_score)\n",
    "\n",
    "BayesSearchCV.__init__ = bayes_search_CV_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As well as the `bayes_search_CV__fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_search_CV__fit(self, X, y, groups, parameter_iterable):\n",
    "    \"\"\"\n",
    "    Actual fitting,  performing the search over parameters.\n",
    "    Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X\n",
    "                .../sklearn/model_selection/_search.py\n",
    "    \"\"\"\n",
    "    estimator = self.estimator\n",
    "    cv = sklearn.model_selection._validation.check_cv(\n",
    "        self.cv, y, classifier=is_classifier(estimator))\n",
    "    self.scorer_ = check_scoring(\n",
    "        self.estimator, scoring=self.scoring)\n",
    "\n",
    "    X, y, groups = indexable(X, y, groups)\n",
    "    n_splits = cv.get_n_splits(X, y, groups)\n",
    "    if self.verbose > 0 and isinstance(parameter_iterable, Sized):\n",
    "        n_candidates = len(parameter_iterable)\n",
    "        print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "              \" {2} fits\".format(n_splits, n_candidates,\n",
    "                                 n_candidates * n_splits))\n",
    "\n",
    "    base_estimator = clone(self.estimator)\n",
    "    pre_dispatch = self.pre_dispatch\n",
    "\n",
    "    cv_iter = list(cv.split(X, y, groups))\n",
    "    out = Parallel(\n",
    "        n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "        pre_dispatch=pre_dispatch\n",
    "    )(delayed(sklearn.model_selection._validation._fit_and_score)(\n",
    "            clone(base_estimator),\n",
    "            X, y, self.scorer_,\n",
    "            train, test, self.verbose, parameters,\n",
    "            fit_params=self.fit_params,\n",
    "            return_train_score=self.return_train_score,\n",
    "            return_n_test_samples=True,\n",
    "            return_times=True, return_parameters=True,\n",
    "            error_score=self.error_score\n",
    "        )\n",
    "        for parameters in parameter_iterable\n",
    "        for train, test in cv_iter)\n",
    "\n",
    "    # if one choose to see train score, \"out\" will contain train score info\n",
    "    if self.return_train_score:\n",
    "        (train_scores, test_scores, n_test_samples,\n",
    "         fit_time, score_time, parameters) = zip(*out)\n",
    "    else:\n",
    "        from warnings import warn\n",
    "        (fit_failed, test_scores, n_test_samples,\n",
    "         fit_time, score_time, parameters) = zip(*[a.values() for a in out])\n",
    "\n",
    "    candidate_params = parameters[::n_splits]\n",
    "    n_candidates = len(candidate_params)\n",
    "\n",
    "    results = dict()\n",
    "\n",
    "    def _store(key_name, array, weights=None, splits=False, rank=False):\n",
    "        \"\"\"A small helper to store the scores/times to the cv_results_\"\"\"\n",
    "        array = np.array(array, dtype=np.float64).reshape(n_candidates,\n",
    "                                                          n_splits)\n",
    "        if splits:\n",
    "            for split_i in range(n_splits):\n",
    "                results[\"split%d_%s\"\n",
    "                        % (split_i, key_name)] = array[:, split_i]\n",
    "\n",
    "        array_means = np.average(array, axis=1, weights=weights)\n",
    "        results['mean_%s' % key_name] = array_means\n",
    "        # Weighted std is not directly available in numpy\n",
    "        array_stds = np.sqrt(np.average((array -\n",
    "                                         array_means[:, np.newaxis]) ** 2,\n",
    "                                        axis=1, weights=weights))\n",
    "        results['std_%s' % key_name] = array_stds\n",
    "\n",
    "        if rank:\n",
    "            results[\"rank_%s\" % key_name] = np.asarray(\n",
    "                rankdata(-array_means, method='min'), dtype=np.int32)\n",
    "\n",
    "    # Computed the (weighted) mean and std for test scores alone\n",
    "    # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples\n",
    "    n_test_samples = np.array(n_test_samples[:n_splits],\n",
    "                                  dtype=np.int)\n",
    "\n",
    "    _store('test_score', test_scores, splits=True, rank=True,\n",
    "           weights=n_test_samples if self.iid else None)\n",
    "    if self.return_train_score:\n",
    "        _store('train_score', train_scores, splits=True)\n",
    "    _store('fit_time', fit_time)\n",
    "    _store('score_time', score_time)\n",
    "\n",
    "    best_index = np.flatnonzero(results[\"rank_test_score\"] == 1)[0]\n",
    "    best_parameters = candidate_params[best_index]\n",
    "\n",
    "    # Use one MaskedArray and mask all the places where the param is not\n",
    "    # applicable for that candidate. Use defaultdict as each candidate may\n",
    "    # not contain all the params\n",
    "    param_results = defaultdict(partial(np.ma.array,\n",
    "                                        np.empty(n_candidates,),\n",
    "                                        mask=True,\n",
    "                                        dtype=object))\n",
    "    for cand_i, params in enumerate(candidate_params):\n",
    "        for name, value in params.items():\n",
    "            # An all masked empty array gets created for the key\n",
    "            # `\"param_%s\" % name` at the first occurence of `name`.\n",
    "            # Setting the value at an index also unmasks that index\n",
    "            param_results[\"param_%s\" % name][cand_i] = value\n",
    "\n",
    "    results.update(param_results)\n",
    "\n",
    "    # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params'\n",
    "    results['params'] = candidate_params\n",
    "\n",
    "    self.cv_results_ = results\n",
    "    self.best_index_ = best_index\n",
    "    self.n_splits_ = n_splits\n",
    "\n",
    "    if self.refit:\n",
    "        # fit the best estimator using the entire dataset\n",
    "        # clone first to work around broken estimators\n",
    "        best_estimator = clone(base_estimator).set_params(\n",
    "            **best_parameters)\n",
    "        if y is not None:\n",
    "            best_estimator.fit(X, y, **self.fit_params)\n",
    "        else:\n",
    "            best_estimator.fit(X, **self.fit_params)\n",
    "        self.best_estimator_ = best_estimator\n",
    "    return self\n",
    "\n",
    "BayesSearchCV._fit = bayes_search_CV__fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Optimisation\n",
    "\n",
    "We're now ready to carry out our model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2019-01-01'\n",
    "    \n",
    "x = s_DE_dispatchable[start_date:end_date]\n",
    "y = s_DE_price[start_date:end_date]\n",
    "pred_reg_dates = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "lowess_dates = lowess.LowessDates(frac=0.5, threshold_value=26, pred_reg_dates=pred_reg_dates)\n",
    "\n",
    "search_spaces = {\n",
    "        'frac': Real(0.35, 1, 'uniform'),\n",
    "        'threshold_value': Integer(10, 52, 'uniform')\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    'reg_dates': pd.date_range(start_date, end_date, freq='7W'),\n",
    "    'num_fits': 10,\n",
    "    'reg_anchors': np.round(np.arange(np.floor(x.min())-5, np.ceil(x.max())+5, 0.1), 1)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    lowess_dates,\n",
    "    search_spaces,\n",
    "    optimizer_kwargs={\n",
    "        'random_state': 42\n",
    "    },\n",
    "    n_iter=15,\n",
    "    verbose=1,\n",
    "    cv=4, # 8 works well for me as that's how many concurrent workers I can use\n",
    "    fit_params=fit_params,\n",
    "    n_jobs=5 # -1\n",
    ")\n",
    "\n",
    "fit_BayesSearchCV = True\n",
    "\n",
    "if fit_BayesSearchCV == True:\n",
    "    opt.fit(x.round(1), y)\n",
    "\n",
    "    print(f'Cross-validation score: {opt.best_score_:.2f}')\n",
    "    print(f'\\nBest params: \\n{opt.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We'll visualise the fitted objective surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = plot_objective(opt.optimizer_results_[0], cmap='magma_r', show_points=False)\n",
    "\n",
    "fig =  plt.gcf()\n",
    "fig.set_dpi(250)\n",
    "fig.delaxes(axs[0][0])\n",
    "fig.delaxes(axs[0][1])\n",
    "fig.delaxes(axs[1][1])\n",
    "\n",
    "ax = axs[1][0]\n",
    "ax.set_xlabel('Dispatchable Generation\\nBandwidth (Fraction)')\n",
    "ax.set_ylabel('Date Smoothing\\nBandwidth (Weeks)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOE",
   "language": "python",
   "name": "moe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Merit-Order-Effect \u00b6 This repository/site outlines the development and usage of code and analysis used in calculating the Merit-Order-Effect (MOE) of renewables on price and carbon intensity of electricity markets. Beyond MOE analysis the moepy library can be used more generally for standard, quantile, and bootstrapped LOWESS estimation. The particular implementation of LOWESS in this software has been extended to significantly reduce the computational resource required. You can install the library using: pip install moepy moepy makes it simple to fit a LOWESS curve, a quick start example to generate the plot below can be found here . The library also includes the option to ensemble LOWESS models together and smooth them over time, an example is shown below for the marginal cost curve of dispatchable generation in Great Britain. Paper \u00b6 The moepy library was developed to enable new research into the Merit-Order-Effect of renewables in the British and German power systems. The full paper can be found here 1 , the abstract is shown below: This paper presents an empirical analysis of the reduction in day-ahead market prices and CO \\(_{2}\\) emissions due to increased renewable generation on both the British and German electricity markets. This research aim is delivered through ex-post analysis of the Merit Order Effect (MOE) using a hybrid statistical/simulation approach. Existing research focuses on linear methods for modelling the merit order stack, masking the larger MOE seen in the steeper top/bottom regions. In this work a blended LOWESS model is used to capture the non-linear relationship between electricity price and dispatchable generation, with historical renewable output data then used to simulate the MOE. The stationary nature of many existing methodologies means they struggle to adapt to changes in the system such as the effect of the Covid-19 pandemic, we use a time-adaptive model to effectively address this limitation. Alongside an extension to the standard LOWESS implementation the use of a time-adaptive model significantly reduces the computational resource required. Our results indicate that renewables delivered reductions equal to 318M tonnes of CO \\(_{2}\\) between 2010 and 2020, and 56B EUR between 2015 and 2020 in Germany. In GB the reductions amounted to a 442M tonnes of CO \\(_{2}\\) and \u00a317B saving between 2010 and 2020. We identified a strong relationship between increasing renewable penetration and the Merit-Order-Effect: Referencing \u00b6 If you use this software please cite it using the following: @software{bourn_moepy_2021, title = {moepy}, url = {https://ayrtonb.github.io/Merit-Order-Effect/}, abstract = {This repository outlines the development and usage of code and analysis used in calculating the Merit-Order-Effect (MOE) of renewables on price and carbon intensity of electricity markets. Beyond MOE analysis the `moepy` library can be used more generally for standard, quantile, and bootstrapped LOWESS estimation. The particular implementation of LOWESS in this software has been extended to significantly reduce the computational resource required.}, author = {Bourn, Ayrton}, month = mar, year = {2021}, doi = {10.5281/zenodo.4642896}, } This will be made available once the paper has been submitted. \u21a9","title":"Home"},{"location":"#merit-order-effect","text":"This repository/site outlines the development and usage of code and analysis used in calculating the Merit-Order-Effect (MOE) of renewables on price and carbon intensity of electricity markets. Beyond MOE analysis the moepy library can be used more generally for standard, quantile, and bootstrapped LOWESS estimation. The particular implementation of LOWESS in this software has been extended to significantly reduce the computational resource required. You can install the library using: pip install moepy moepy makes it simple to fit a LOWESS curve, a quick start example to generate the plot below can be found here . The library also includes the option to ensemble LOWESS models together and smooth them over time, an example is shown below for the marginal cost curve of dispatchable generation in Great Britain.","title":"Merit-Order-Effect"},{"location":"#paper","text":"The moepy library was developed to enable new research into the Merit-Order-Effect of renewables in the British and German power systems. The full paper can be found here 1 , the abstract is shown below: This paper presents an empirical analysis of the reduction in day-ahead market prices and CO \\(_{2}\\) emissions due to increased renewable generation on both the British and German electricity markets. This research aim is delivered through ex-post analysis of the Merit Order Effect (MOE) using a hybrid statistical/simulation approach. Existing research focuses on linear methods for modelling the merit order stack, masking the larger MOE seen in the steeper top/bottom regions. In this work a blended LOWESS model is used to capture the non-linear relationship between electricity price and dispatchable generation, with historical renewable output data then used to simulate the MOE. The stationary nature of many existing methodologies means they struggle to adapt to changes in the system such as the effect of the Covid-19 pandemic, we use a time-adaptive model to effectively address this limitation. Alongside an extension to the standard LOWESS implementation the use of a time-adaptive model significantly reduces the computational resource required. Our results indicate that renewables delivered reductions equal to 318M tonnes of CO \\(_{2}\\) between 2010 and 2020, and 56B EUR between 2015 and 2020 in Germany. In GB the reductions amounted to a 442M tonnes of CO \\(_{2}\\) and \u00a317B saving between 2010 and 2020. We identified a strong relationship between increasing renewable penetration and the Merit-Order-Effect:","title":"Paper"},{"location":"#referencing","text":"If you use this software please cite it using the following: @software{bourn_moepy_2021, title = {moepy}, url = {https://ayrtonb.github.io/Merit-Order-Effect/}, abstract = {This repository outlines the development and usage of code and analysis used in calculating the Merit-Order-Effect (MOE) of renewables on price and carbon intensity of electricity markets. Beyond MOE analysis the `moepy` library can be used more generally for standard, quantile, and bootstrapped LOWESS estimation. The particular implementation of LOWESS in this software has been extended to significantly reduce the computational resource required.}, author = {Bourn, Ayrton}, month = mar, year = {2021}, doi = {10.5281/zenodo.4642896}, } This will be made available once the paper has been submitted. \u21a9","title":"Referencing"},{"location":"dev-01-retrieval/","text":"Data Retrieval \u00b6 This notebook outlines the retrieval of the fuel generation and price data required for the merit-order-effect analyses. Imports \u00b6 #exports import json import numpy as np import pandas as pd import requests import xmltodict from datetime import date from warnings import warn from itertools import product from dotenv import load_dotenv from entsoe import EntsoePandasClient , EntsoeRawClient from ipypb import track from IPython.display import JSON Electric Insights \u00b6 Electric Insights provides a site to \"Take a closer look at the supply, demand, price and environmental impact of Britain\u2019s electricity\", it also exposes an API that contains the data we require for this analysis Single Stream \u00b6 We'll being by retrieving data for a single stream only, starting with just the raw JSON response #exports def query_API ( start_date : str , end_date : str , stream : str , time_group = '30m' ): \"\"\" 'Query API' makes the call to Electric Insights and returns the JSON response Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' stream: One of 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' \"\"\" # Checking stream is an EI endpoint possible_streams = [ 'prices_ahead' , 'prices' , 'temperatures' , 'emissions' , 'generation-mix' ] assert stream in possible_streams , f \"Stream must be one of { '' . join ([ stream + ', ' for stream in possible_streams ])[: - 2 ] } \" # Checking time_group will be accepted by API possible_time_groups = [ '30m' , '1h' , '1d' , '7d' ] assert time_group in possible_time_groups , f \"Time group must be one of { '' . join ([ time_group + ', ' for time_group in possible_time_groups ])[: - 2 ] } \" # Formatting dates format_dt = lambda dt : date . strftime ( dt , '%Y-%m- %d ' ) if isinstance ( dt , date ) else dt start_date = format_dt ( start_date ) end_date = format_dt ( end_date ) # Running query and parsing response response = requests . get ( f 'http://drax-production.herokuapp.com/api/1/ { stream } ?date_from= { start_date } &date_to= { end_date } &group_by= { time_group } ' ) r_json = response . json () return r_json start_date = '2019-01-01' end_date = '2019-01-31' stream = 'generation-mix' r_json = query_API ( start_date , end_date , stream ) JSON ([ r_json ]) <IPython.core.display.JSON object> We can convert this response to a dataframe, however this doesn't handle the nested columns well df = pd . DataFrame . from_dict ( r_json ) df . head () Unnamed: 0 start end value valueSum 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... 3 2019-01-01T01:30:00Z 2019-01-01T02:00:00Z {'nuclear': 6.83, 'biomass': 1.085, 'coal': 0,... {'nuclear': 6.83, 'biomass': 1.085, 'coal': 0,... 4 2019-01-01T02:00:00Z 2019-01-01T02:30:00Z {'nuclear': 6.827, 'biomass': 1.081, 'coal': 0... {'nuclear': 6.827, 'biomass': 1.081, 'coal': 0... We can create a function that will take a specified column and extract the nested dataframe #exports def dict_col_2_cols ( df : pd . DataFrame , value_col = 'value' ): \"\"\"Checks the `value_col`, if it contains dictionaries these are transformed into new columns which then replace it\"\"\" ## Checks the value col is found in the dataframe if value_col not in df . columns : return df if isinstance ( df . loc [ 0 , value_col ], dict ): df_values = pd . DataFrame ( df [ value_col ] . to_dict ()) . T df [ df_values . columns ] = df_values df = df . drop ( columns = [ value_col ]) return df df = dict_col_2_cols ( df ) df . head ( 3 ) Unnamed: 0 start end valueSum nuclear biomass coal gas hydro wind windTotal solar demand pumpedStorage imports exports balance 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... 6.924 1.116 0 5.853 0.405 11.304 {'windOnshore': 8.054581, 'windOffshore': 3.14... 0 27.336 0 {'belgian': 0, 'dutch': 0.182, 'french': 1.552... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.552, 'dutch': 0.182, 'irish': -0.... 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... 6.838 1.103 0 6.292 0.388 11.327 {'windOnshore': 7.860487, 'windOffshore': 3.25... 0 27.722 0.024 {'belgian': 0, 'dutch': 0.196, 'french': 1.554... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.554, 'dutch': 0.196, 'irish': -0.... 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... 6.834 1.09 0 5.719 0.372 11.335 {'windOnshore': 7.879198000000001, 'windOffsho... 0 27.442 0 {'belgian': 0, 'dutch': 0.588, 'french': 1.504... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.504, 'dutch': 0.588, 'irish': -0.... Unfortunately however this doesn't handle repeated nesting of dictionaries, we'll create a wrapper that does #exports def clean_nested_dict_cols ( df ): \"\"\"Unpacks columns contining nested dictionaries\"\"\" # Calculating columns that are still dictionaries s_types = df . iloc [ 0 ] . apply ( lambda val : type ( val )) cols_with_dicts = s_types [ s_types == dict ] . index while len ( cols_with_dicts ) > 0 : for col_with_dicts in cols_with_dicts : # Extracting dataframes from dictionary columns df = dict_col_2_cols ( df , col_with_dicts ) # Recalculating columns that are still dictionaries s_types = df . iloc [ 0 ] . apply ( lambda val : type ( val )) cols_with_dicts = s_types [ s_types == dict ] . index return df df = clean_nested_dict_cols ( df ) df . head () Unnamed: 0 start end nuclear biomass coal gas hydro wind solar demand pumpedStorage windOnshore windOffshore belgian dutch french ireland northernIreland irish 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01T01:30:00Z 2019-01-01T02:00:00Z 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01T02:00:00Z 2019-01-01T02:30:00Z 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 Next we'll process the datetime index #exports def set_dt_idx ( df : pd . DataFrame , idx_name = 'local_datetime' ): \"\"\" Converts the start datetime to UK local time, then sets it as the index and removes the original datetime columns \"\"\" idx_dt = pd . DatetimeIndex ( pd . to_datetime ( df [ 'start' ], utc = True )) . tz_convert ( 'Europe/London' ) idx_dt . name = idx_name df . index = idx_dt df = df . drop ( columns = [ 'start' , 'end' ]) return df def create_df_dt_rng ( start_date , end_date , freq = '30T' , tz = 'Europe/London' , dt_str_template = '%Y-%m- %d ' ): \"\"\" Creates a dataframe mapping between local datetimes and electricity market dates/settlement periods \"\"\" # Creating localised datetime index s_dt_rng = pd . date_range ( start_date , end_date , freq = freq , tz = tz ) s_dt_SP_count = pd . Series ( 0 , index = s_dt_rng ) . resample ( 'D' ) . count () # Creating SP column SPs = [] for num_SPs in list ( s_dt_SP_count ): SPs += list ( range ( 1 , num_SPs + 1 )) # Creating datetime dataframe df_dt_rng = pd . DataFrame ( index = s_dt_rng ) df_dt_rng . index . name = 'local_datetime' # Adding query call cols df_dt_rng [ 'SP' ] = SPs df_dt_rng [ 'date' ] = df_dt_rng . index . strftime ( dt_str_template ) return df_dt_rng def clean_df_dts ( df ): \"\"\"Cleans the datetime index of the passed DataFrame\"\"\" df = set_dt_idx ( df ) df = df [ ~ df . index . duplicated ()] df_dt_rng = create_df_dt_rng ( df . index . min (), df . index . max ()) df = df . reindex ( df_dt_rng . index ) df [ 'SP' ] = df_dt_rng [ 'SP' ] # Adding settlement period designation return df df = clean_df_dts ( df ) df . head () local_datetime nuclear biomass coal gas hydro wind solar demand pumpedStorage windOnshore windOffshore belgian dutch french ireland northernIreland irish SP 2019-01-01 00:00:00+00:00 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01 00:30:00+00:00 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01 01:00:00+00:00 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01 01:30:00+00:00 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01 02:00:00+00:00 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 5 We'll now combine all of the previous steps and add some column renaming where we want to tidy them up a bit #exports def retrieve_stream_df ( start_date : str , end_date : str , stream : str , time_group = '30m' , renaming_dict = {}): \"\"\" Makes the call to Electric Insights and parses the response into a dataframe which is returned Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' stream: One of 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' renaming_dict: Mapping from old to new column names \"\"\" # Calling data and parsing into dataframe r_json = query_API ( start_date , end_date , stream , time_group ) df = pd . DataFrame . from_dict ( r_json ) # Handling entrys which are dictionarys df = clean_nested_dict_cols ( df ) # Setting index as localised datetime, reindexing with all intervals and adding SP df = clean_df_dts ( df ) # Renaming value col if 'value' in df . columns : df = df . rename ( columns = { 'value' : stream }) if 'referenceOnly' in df . columns : df = df . drop ( columns = [ 'referenceOnly' ]) df = df . rename ( columns = renaming_dict ) return df start_date = '2019-01-01' end_date = '2019-01-31' stream = 'generation-mix' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' } df = retrieve_stream_df ( start_date , end_date , stream , renaming_dict = renaming_dict ) df . head () local_datetime nuclear biomass coal gas hydro wind solar demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish SP 2019-01-01 00:00:00+00:00 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01 00:30:00+00:00 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01 01:00:00+00:00 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01 01:30:00+00:00 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01 02:00:00+00:00 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 5 Multiple Streams \u00b6 We'll now create further functionality for retrieving all of the streams and combining them, before doing so we'll create a helper function for checking the streams are allowed #exports def check_streams ( streams = '*' ): \"\"\" Checks that the streams given are a list containing only possible streams, or is all streams - '*'. \"\"\" possible_streams = [ 'prices_ahead' , 'prices' , 'temperatures' , 'emissions' , 'generation-mix' ] if isinstance ( streams , list ): unrecognised_streams = list ( set ( streams ) - set ( possible_streams )) if len ( unrecognised_streams ) == 0 : return streams else : unrecognised_streams_2_print = '' . join ([ \"'\" + stream + \"', \" for stream in unrecognised_streams ])[: - 2 ] raise ValueError ( f \"Streams { unrecognised_streams_2_print } could not be recognised, must be one of: { ', ' . join ( possible_streams ) } \" ) elif streams == '*' : return possible_streams else : raise ValueError ( f \"Streams could not be recognised, must be one of: { ', ' . join ( possible_streams ) } \" ) streams = check_streams () streams ['prices_ahead', 'prices', 'temperatures', 'emissions', 'generation-mix'] By default all streams are returned but if we provide a list it will be checked streams = check_streams ([ 'prices' , 'emissions' ]) streams ['prices', 'emissions'] However, if we try to check a list containing a stream that doesn't exist we should receive an error try : _ = check_streams ([ 'not_a_stream' ]) print ( 'Success!' ) except Exception as e : print ( 'Error! \\n\\n ' + str ( e )) Error! Streams 'not_a_stream' could not be recognised, must be one of: prices_ahead, prices, temperatures, emissions, generation-mix Next we'll create a wrapper for downloading and combining all of the streams together #exports def retrieve_streams_df ( start_date : str , end_date : str , streams = '*' , time_group = '30m' , renaming_dict = {}): \"\"\" Makes the calls to Electric Insights for the given streams and parses the responses into a dataframe which is returned Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' streams: Contains 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions', or is given as all, '*' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' \"\"\" df = pd . DataFrame () streams = check_streams ( streams ) for stream in streams : df_stream = retrieve_stream_df ( start_date , end_date , stream , renaming_dict = renaming_dict ) df [ df_stream . columns ] = df_stream return df streams = '*' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } df = retrieve_streams_df ( start_date , end_date , streams , renaming_dict = renaming_dict ) df . head () local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2019-01-01 00:00:00+00:00 48.81 1 15 15 9.1 2287.01 83.6629 6.924 1.116 0 ... 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 2019-01-01 00:30:00+00:00 50.24 2 15 15 9.1 2467.91 89.0234 6.838 1.103 0 ... 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2019-01-01 01:00:00+00:00 41.9 3 16 16 9.1 2411.83 87.8884 6.834 1.09 0 ... 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 2019-01-01 01:30:00+00:00 39.32 4 16 16 9.1 2119.53 80.073 6.83 1.085 0 ... 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 2019-01-01 02:00:00+00:00 34.09 5 16 16 9.1 2069.84 79.0166 6.827 1.081 0 ... 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 Now we're ready to retrieve all of the streams in one, which we'll do for all years that data is available, then we'll save the resulting DataFrame. %% time streams = '*' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } retrieve_save_EI_data = False if retrieve_save_EI_data == True : df = pd . DataFrame () for year in track ( range ( 2010 , 2021 )): start_date = f ' { year } -01-01' end_date = f ' { year } -12-31' df_year = retrieve_streams_df ( start_date , end_date , streams , renaming_dict = renaming_dict ) df = df . append ( df_year ) df . to_csv ( '../data/raw/electric_insights.csv' ) Wall time: 0 ns Energy-Charts \u00b6 We'll start by requesting the JSON that is used to populate the fuel-type output chart on the website def year_week_2_prod_url ( year , week , data_prefix = '' ): \"\"\"Given a specified year and week the relevant `production_url` for energy-charts is returned\"\"\" if year < 2019 : data_prefix = 'raw_' production_url = f 'https://energy-charts.info/charts/power/ { data_prefix } data/de/week_ { year } _ { str ( week ) . zfill ( 2 ) } .json' return production_url year = 2018 week = 12 production_url = year_week_2_prod_url ( year , week ) r = requests . get ( production_url ) r <Response [200]> Next we focus on parsing the only non-uniform column which relates to the balancing required on the system def fuel_json_2_net_balance ( r_json ): \"\"\"Extracts the balance time-series\"\"\" if 'values' in r_json [ 0 ] . keys (): # pre-2019 format df_balance = pd . DataFrame ( r_json [ 0 ][ 'values' ]) s_balance = ( df_balance . assign ( datetime = pd . to_datetime ( df_balance [ 0 ] * 1000000 , utc = True )) . drop ( columns = 0 ) . set_index ( 'datetime' ) . rename ( columns = { 1 : 'value' , }) [ 'value' ] ) else : s_balance = pd . Series ( r_json [ 0 ][ 'data' ], index = pd . to_datetime ( np . array ( r_json [ 0 ][ 'xAxisValues' ]) * 1000000 , utc = True )) s_balance . index = s_balance . index . tz_convert ( 'Europe/Berlin' ) return s_balance # Retrieving net balance data r_json = r . json () s_balance = fuel_json_2_net_balance ( r_json ) # Getting comments data_src = r_json [ 0 ][ 'datasource' ] comment = ' \\n ' . join ([ item [ 'en' ] for item in r_json [ 0 ][ 'comment' ]]) print ( data_src ) print ( comment ) s_balance 50 Hertz, Amprion, Tennet, TransnetBW, EEX Net generation of power plants for public power supply. datetime 2018-03-19 00:00:00+01:00 -9.044 2018-03-19 01:00:00+01:00 -9.522 2018-03-19 02:00:00+01:00 -9.576 2018-03-19 03:00:00+01:00 -9.978 2018-03-19 04:00:00+01:00 -9.787 ... 2018-03-25 19:00:00+02:00 -6.319 2018-03-25 20:00:00+02:00 -5.968 2018-03-25 21:00:00+02:00 -6.329 2018-03-25 22:00:00+02:00 -6.134 2018-03-25 23:00:00+02:00 -6.839 Name: value, Length: 167, dtype: float64 We combine this with a parser for the main columns def response_2_df ( r ): \"\"\"Parses the json response to a DataFrame\"\"\" r_json = r . json () s_balance = fuel_json_2_net_balance ( r_json ) if 'key' in r_json [ 1 ] . keys (): # pre-2019 keys = [ x [ 'key' ][ 0 ][ 'en' ] for x in r_json [ 1 :]] values = [ x [ 'values' ] for x in r_json [ 1 :]] df_fuels = ( pd . DataFrame ( dict ( zip ( keys , values )), index = s_balance . index ) . apply ( lambda s : s . apply ( lambda x : x [ - 1 ])) . assign ( net_balance = s_balance ) ) else : cols = [ x [ 'name' ][ 0 ][ 'en' ] for x in r_json [ 1 :]] data = [ x [ 'data' ] for x in r_json [ 1 :]] df_fuels = pd . DataFrame ( np . array ( data ) . T , columns = cols , index = s_balance . index ) . assign ( net_balance = s_balance ) return df_fuels df_fuels = response_2_df ( r ) df_fuels . head ( 2 ) datetime Hydro Power Biomass Uranium Brown Coal Hard Coal Oil Gas Others Pumped Storage Seasonal Storage Wind Solar net_balance 2018-03-19 00:00:00+01:00 2.292 5.82 7.721 14.499 8.508 0.18 3.207 0.068 0.42 0.078 20.754 0 -9.044 2018-03-19 01:00:00+01:00 2.291 5.82 7.829 14.49 8.941 0.179 3.287 0.068 0.245 0.064 19.012 0 -9.522 And wrap them in a single function which can accept a year and week before returning the parsed dataframe def year_week_2_fuel_df ( year , week ): \"\"\"Given a specified year and week the relevant `df_fuels` dataset for energy-charts is returned\"\"\" production_url = year_week_2_prod_url ( year , week ) r = requests . get ( production_url ) # This is a general catch-all but mainly added to account # for years with 53 weeks rather than 52 if r . status_code == 404 : df_fuels = pd . DataFrame () else : df_fuels = response_2_df ( r ) return df_fuels year = 2015 week = 53 df_fuels = year_week_2_fuel_df ( year , week ) df_fuels . head ( 2 ) datetime Hydro Power Biomass Uranium Brown Coal Hard Coal Oil Gas Others Pumped Storage Seasonal Storage Wind Solar net_balance 2015-12-28 00:00:00+01:00 1.356 6 10.567 13.247 3.437 0.19 2.338 0.15 0.154 0.013 8.156 0 -3.923 2015-12-28 01:00:00+01:00 1.348 6 10.611 13.744 3.109 0.191 2.533 0.124 0.157 0.012 7.059 0 -4.139 Now we can iterate over all year and week pairs to retrieve the full dataset years = list ( range ( 2010 , 2021 )) weeks = list ( range ( 1 , 54 )) df_fuels = pd . DataFrame () for year , week in track ( product ( years , weeks ), total = len ( years ) * len ( weeks )): try : df_fuels_yr_wk = year_week_2_fuel_df ( year , week ) df_fuels = df_fuels . append ( df_fuels_yr_wk , sort = True ) except : warn ( f 'Failed to retrieve week { week } in { year } ' ) df_fuels . head ( 3 ) 100% 580/583 [03:16 < 00:00, 0.34s/it] Unnamed: 0 Biomass Biomass planned Brown Coal Brown Coal planned Gas Gas planned Hard Coal Hard Coal planned Hydro Power Hydro Power planned ... Residual load forecast Seasonal Storage Seasonal Storage ante Solar Solar forecast Uranium Uranium planned Wind Wind forecast net_balance 2010-01-04 00:00:00+01:00 3.637 nan 16.533 nan 4.726 nan 10.078 nan 2.331 nan ... nan 0.068 nan 0 nan 16.826 nan 0.635 nan -1.229 2010-01-04 01:00:00+01:00 3.637 nan 16.544 nan 4.856 nan 8.816 nan 2.293 nan ... nan 0.003 nan 0 nan 16.841 nan 0.528 nan -1.593 2010-01-04 02:00:00+01:00 3.637 nan 16.368 nan 5.275 nan 7.954 nan 2.299 nan ... nan 0 nan 0 nan 16.846 nan 0.616 nan -1.378 We'll remove the null columns and reorder the remaining ones to clean it up a little bit columns = [ 'Biomass' , 'Brown Coal' , 'Gas' , 'Hard Coal' , 'Hydro Power' , 'Oil' , 'Others' , 'Pumped Storage' , 'Seasonal Storage' , #'Import Balance', 'Solar' , 'Uranium' , 'Wind' , 'net_balance' ] df_fuels_clean = ( df_fuels [ columns ] . astype ( float ) . resample ( 'H' ) . mean () . dropna ( how = 'all' , axis = 1 ) ) df_fuels_clean . head () Unnamed: 0 Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance 2010-01-04 00:00:00+01:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 2010-01-04 01:00:00+01:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 2010-01-04 02:00:00+01:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 2010-01-04 03:00:00+01:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 2010-01-04 04:00:00+01:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 We'll quickly confirm that there are no duplicates assert df_fuels_clean . index . duplicated () . sum () == 0 , 'There are multiple entries for a datetime' Next we check for any missing datetimes s_null_vals = df_fuels_clean . isnull () . sum ( axis = 1 ) == df_fuels_clean . shape [ 1 ] print ( f ' { round ( 100 * s_null_vals . mean (), 2 ) } % of the datetimes are missing data' ) print ( f \"There are { ( s_null_vals . resample ( 'D' ) . sum () > 0 ) . sum () } days with missing data, totalling { s_null_vals . sum () } hours of individual missing hours\" ) 0.16% of the datetimes are missing data There are 7 days with missing data, totalling 157 hours of individual missing hours Because there are only a few missing datetimes we'll choose to drop them from the dataset df_fuels_clean = df_fuels_clean [ ~ s_null_vals ] Finally we'll save the cleaned dataframe df_fuels_clean . index . name = 'local_datetime' df_fuels_clean . to_csv ( '../data/raw/energy_charts.csv' ) ENTSOE \u00b6 We'll use ENTSOE as the source for our German price data, we'll also have to use it to get the exact RES and demand volumes for the bidding zone that Germany is within. Prices \u00b6 We'll begin by initialising our client API for ENTSOE You will need to supply your own API key, we recommend you store it in a .env file load_dotenv () ENTSOE_API_KEY = os . environ [ 'ENTSOE_API_KEY' ] client = EntsoePandasClient ( api_key = api_key ) client <entsoe.entsoe.EntsoePandasClient at 0x216d1b88d60> We'll then make a test request for price data from the German market params = { 'documentType' : 'A44' , 'in_Domain' : '10Y1001A1001A63L' , 'out_Domain' : '10Y1001A1001A63L' } start = pd . Timestamp ( '2018-09-01' , tz = 'UTC' ) end = pd . Timestamp ( '2018-09-30' , tz = 'UTC' ) r = client . _base_request ( params = params , start = start , end = end ) r <Response [200]> We'll extract the price time-series from the returned JSON #exports def parse_A44_response ( r , freq = 'H' , tz = 'UTC' ): \"\"\"Extracts the price time-series\"\"\" s_price = pd . Series ( dtype = float ) parsed_r = xmltodict . parse ( r . text ) for timeseries in parsed_r [ 'Publication_MarketDocument' ][ 'TimeSeries' ]: dt_rng = pd . date_range ( timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ], timeseries [ 'Period' ][ 'timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] s_dt_price = pd . DataFrame ( timeseries [ 'Period' ][ 'Point' ])[ 'price.amount' ] . astype ( float ) s_dt_price . index = dt_rng s_price = s_price . append ( s_dt_price ) assert s_price . index . duplicated () . sum () == 0 , 'There are duplicate date indexes' return s_price s_price = parse_A44_response ( r ) s_price . head () 2018-08-31 22:00:00+00:00 57.58 2018-08-31 23:00:00+00:00 55.99 2018-09-01 00:00:00+00:00 55.56 2018-09-01 01:00:00+00:00 54.02 2018-09-01 02:00:00+00:00 52.69 Freq: H, dtype: float64 We can't query very large date ranges so we'll break up our requests into quarterly batches. We'll also account for the fact that the German market changes to exclude Austria after 2018-10-01 by creating two sets of date pairs. DE_AT_LU_dt_pairs = list ( zip ( pd . date_range ( '2015' , '2018-07' , freq = '3MS' ) . strftime ( '%Y-%m- %d %H:%M' ), pd . date_range ( '2015-03-31 23:55' , '2018-09-30 23:55' , freq = '3M' ) . strftime ( '%Y-%m- %d %H:%M' ) )) DE_AT_LU_dt_pairs [: 5 ] [('2015-01-01 00:00', '2015-03-31 23:55'), ('2015-04-01 00:00', '2015-06-30 23:55'), ('2015-07-01 00:00', '2015-09-30 23:55'), ('2015-10-01 00:00', '2015-12-31 23:55'), ('2016-01-01 00:00', '2016-03-31 23:55')] We're now ready to create a wrapper to collate the date from each date batch #exports def retreive_DAM_prices ( dt_pairs , domain = '10Y1001A1001A63L' ): \"\"\"Retrieves and collates the day-ahead prices for the specified date ranges\"\"\" params = { 'documentType' : 'A44' , 'in_Domain' : domain , 'out_Domain' : domain } s_price = pd . Series ( dtype = float ) for dt_pair in track ( dt_pairs ): start = pd . Timestamp ( dt_pair [ 0 ], tz = 'UTC' ) end = pd . Timestamp ( dt_pair [ 1 ], tz = 'UTC' ) try : r = client . _base_request ( params = params , start = start , end = end ) s_price_dt_rng = parse_A44_response ( r ) s_price = s_price . append ( s_price_dt_rng ) except : warn ( f \" { start . strftime ( '%Y-%m- %d ' ) } - { end . strftime ( '%Y-%m- %d ' ) } failed\" ) return s_price s_price_DE_AT_LU = retreive_DAM_prices ( DE_AT_LU_dt_pairs ) s_price_DE_AT_LU . head () 100% 15/15 [00:20 < 00:01, 1.31s/it] 2015-01-04 23:00:00+00:00 22.34 2015-01-05 00:00:00+00:00 17.93 2015-01-05 01:00:00+00:00 15.17 2015-01-05 02:00:00+00:00 16.38 2015-01-05 03:00:00+00:00 17.38 dtype: float64 We'll repeat this for the market excluding Austria DE_LU_dt_pairs = list ( zip ( pd . date_range ( '2018-10' , '2021-01' , freq = '3MS' ) . strftime ( '%Y-%m- %d %H:%M' ), pd . date_range ( '2018-12-31 23:55' , '2021-02-28 23:55' , freq = '3M' ) . strftime ( '%Y-%m- %d %H:%M' ) )) s_price_DE_LU = retreive_DAM_prices ( DE_LU_dt_pairs , domain = '10Y1001A1001A82H' ) s_price_DE_LU . head () 100% 9/9 [00:11 < 00:01, 1.25s/it] 2018-09-30 22:00:00+00:00 59.53 2018-09-30 23:00:00+00:00 56.10 2018-10-01 00:00:00+00:00 51.41 2018-10-01 01:00:00+00:00 47.38 2018-10-01 02:00:00+00:00 47.59 dtype: float64 We'll now combine and visualise the time-series s_price = s_price_DE_AT_LU . append ( s_price_DE_LU ) s_price . index = s_price . index . tz_convert ( 'Europe/Berlin' ) s_price . plot () <AxesSubplot:> Before moving on we'll save this series as a csv s_price . name = 'DE_price' s_price . index . name = 'local_datetime' s_price . to_csv ( '../data/raw/ENTSOE_DE_price.csv' ) Generation by Fuel-Type \u00b6 We'll now create the functions for retrieving and parsing the fuel data #exports def parse_A75_response ( r , freq = '15T' , tz = 'UTC' , warn_on_failure = False ): \"\"\"Extracts the production data by fuel-type from the JSON response\"\"\" psr_code_to_type = { 'A03' : 'Mixed' , 'A04' : 'Generation' , 'A05' : 'Load' , 'B01' : 'Biomass' , 'B02' : 'Fossil Brown coal/Lignite' , 'B03' : 'Fossil Coal-derived gas' , 'B04' : 'Fossil Gas' , 'B05' : 'Fossil Hard coal' , 'B06' : 'Fossil Oil' , 'B07' : 'Fossil Oil shale' , 'B08' : 'Fossil Peat' , 'B09' : 'Geothermal' , 'B10' : 'Hydro Pumped Storage' , 'B11' : 'Hydro Run-of-river and poundage' , 'B12' : 'Hydro Water Reservoir' , 'B13' : 'Marine' , 'B14' : 'Nuclear' , 'B15' : 'Other renewable' , 'B16' : 'Solar' , 'B17' : 'Waste' , 'B18' : 'Wind Offshore' , 'B19' : 'Wind Onshore' , 'B20' : 'Other' , 'B21' : 'AC Link' , 'B22' : 'DC Link' , 'B23' : 'Substation' , 'B24' : 'Transformer' } parsed_r = xmltodict . parse ( r . text ) columns = [ f 'B { str ( fuel_idx ) . zfill ( 2 ) } ' for fuel_idx in np . arange ( 1 , 24 )] index = pd . date_range ( parsed_r [ 'GL_MarketDocument' ][ 'time_Period.timeInterval' ][ 'start' ], parsed_r [ 'GL_MarketDocument' ][ 'time_Period.timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] df_production = pd . DataFrame ( dtype = float , columns = columns , index = index ) for timeseries in parsed_r [ 'GL_MarketDocument' ][ 'TimeSeries' ]: try : psr_type = timeseries [ 'MktPSRType' ][ 'psrType' ] dt_rng = pd . date_range ( timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ], timeseries [ 'Period' ][ 'timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] s_psr_type = pd . DataFrame ( timeseries [ 'Period' ][ 'Point' ])[ 'quantity' ] . astype ( float ) s_psr_type . index = dt_rng df_production [ psr_type ] = s_psr_type except : if warn_on_failure == True : warn ( f \" { timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ] } - { timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ] } failed for { psr_type } \" ) assert df_production . index . duplicated () . sum () == 0 , 'There are duplicate date indexes' df_production = df_production . dropna ( how = 'all' ) . dropna ( how = 'all' , axis = 1 ) df_production = df_production . rename ( columns = psr_code_to_type ) return df_production def retrieve_production ( dt_pairs , domain = '10Y1001A1001A63L' , warn_on_failure = False ): \"\"\"Retrieves and collates the production data for the specified date ranges\"\"\" params = { 'documentType' : 'A75' , 'processType' : 'A16' , 'in_Domain' : domain } df_production = pd . DataFrame ( dtype = float ) for dt_pair in track ( dt_pairs ): start = pd . Timestamp ( dt_pair [ 0 ], tz = 'UTC' ) end = pd . Timestamp ( dt_pair [ 1 ], tz = 'UTC' ) try : r = client . _base_request ( params = params , start = start , end = end ) df_production_dt_rng = parse_A75_response ( r , warn_on_failure = warn_on_failure ) df_production = df_production . append ( df_production_dt_rng ) except : if warn_on_failure == True : warn ( f \" { start . strftime ( '%Y-%m- %d ' ) } - { end . strftime ( '%Y-%m- %d ' ) } failed\" ) return df_production df_production_DE_AT_LU = retrieve_production ( DE_AT_LU_dt_pairs ) df_production_DE_LU = retrieve_production ( DE_LU_dt_pairs , domain = '10Y1001A1001A82H' ) df_production = df_production_DE_AT_LU . append ( df_production_DE_LU ) df_production . head () 100% 15/15 [05:37 < 00:18, 22.45s/it] 100% 9/9 [02:37 < 00:14, 17.45s/it] Unnamed: 0 Biomass Fossil Brown coal/Lignite Fossil Coal-derived gas Fossil Gas Fossil Hard coal Fossil Oil Geothermal Hydro Pumped Storage Hydro Run-of-river and poundage Hydro Water Reservoir Nuclear Other renewable Solar Waste Wind Offshore Wind Onshore Other Marine 2015-01-01 23:00:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 391 0 nan nan 2015-01-01 23:15:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 292 0 nan nan 2015-01-01 23:30:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 271 0 nan nan 2015-01-01 23:45:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 266 0 nan nan 2015-01-02 00:00:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 268 0 nan nan We'll quickly inspect for the presence of null values, due to the large number found we'll use the energy-charts data when it comes to fuel generation df_production . isnull () . mean () Biomass 0.068175 Fossil Brown coal/Lignite 0.068175 Fossil Coal-derived gas 0.998617 Fossil Gas 0.385562 Fossil Hard coal 0.068175 Fossil Oil 0.362709 Geothermal 0.068175 Hydro Pumped Storage 0.068175 Hydro Run-of-river and poundage 0.068175 Hydro Water Reservoir 0.250576 Nuclear 0.269564 Other renewable 0.068175 Solar 0.178234 Waste 0.068175 Wind Offshore 0.157710 Wind Onshore 0.141564 Other 0.068175 Marine 0.979668 dtype: float64","title":"Data Retrieval"},{"location":"dev-01-retrieval/#data-retrieval","text":"This notebook outlines the retrieval of the fuel generation and price data required for the merit-order-effect analyses.","title":"Data Retrieval"},{"location":"dev-01-retrieval/#imports","text":"#exports import json import numpy as np import pandas as pd import requests import xmltodict from datetime import date from warnings import warn from itertools import product from dotenv import load_dotenv from entsoe import EntsoePandasClient , EntsoeRawClient from ipypb import track from IPython.display import JSON","title":"Imports"},{"location":"dev-01-retrieval/#electric-insights","text":"Electric Insights provides a site to \"Take a closer look at the supply, demand, price and environmental impact of Britain\u2019s electricity\", it also exposes an API that contains the data we require for this analysis","title":"Electric Insights"},{"location":"dev-01-retrieval/#single-stream","text":"We'll being by retrieving data for a single stream only, starting with just the raw JSON response #exports def query_API ( start_date : str , end_date : str , stream : str , time_group = '30m' ): \"\"\" 'Query API' makes the call to Electric Insights and returns the JSON response Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' stream: One of 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' \"\"\" # Checking stream is an EI endpoint possible_streams = [ 'prices_ahead' , 'prices' , 'temperatures' , 'emissions' , 'generation-mix' ] assert stream in possible_streams , f \"Stream must be one of { '' . join ([ stream + ', ' for stream in possible_streams ])[: - 2 ] } \" # Checking time_group will be accepted by API possible_time_groups = [ '30m' , '1h' , '1d' , '7d' ] assert time_group in possible_time_groups , f \"Time group must be one of { '' . join ([ time_group + ', ' for time_group in possible_time_groups ])[: - 2 ] } \" # Formatting dates format_dt = lambda dt : date . strftime ( dt , '%Y-%m- %d ' ) if isinstance ( dt , date ) else dt start_date = format_dt ( start_date ) end_date = format_dt ( end_date ) # Running query and parsing response response = requests . get ( f 'http://drax-production.herokuapp.com/api/1/ { stream } ?date_from= { start_date } &date_to= { end_date } &group_by= { time_group } ' ) r_json = response . json () return r_json start_date = '2019-01-01' end_date = '2019-01-31' stream = 'generation-mix' r_json = query_API ( start_date , end_date , stream ) JSON ([ r_json ]) <IPython.core.display.JSON object> We can convert this response to a dataframe, however this doesn't handle the nested columns well df = pd . DataFrame . from_dict ( r_json ) df . head () Unnamed: 0 start end value valueSum 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... 3 2019-01-01T01:30:00Z 2019-01-01T02:00:00Z {'nuclear': 6.83, 'biomass': 1.085, 'coal': 0,... {'nuclear': 6.83, 'biomass': 1.085, 'coal': 0,... 4 2019-01-01T02:00:00Z 2019-01-01T02:30:00Z {'nuclear': 6.827, 'biomass': 1.081, 'coal': 0... {'nuclear': 6.827, 'biomass': 1.081, 'coal': 0... We can create a function that will take a specified column and extract the nested dataframe #exports def dict_col_2_cols ( df : pd . DataFrame , value_col = 'value' ): \"\"\"Checks the `value_col`, if it contains dictionaries these are transformed into new columns which then replace it\"\"\" ## Checks the value col is found in the dataframe if value_col not in df . columns : return df if isinstance ( df . loc [ 0 , value_col ], dict ): df_values = pd . DataFrame ( df [ value_col ] . to_dict ()) . T df [ df_values . columns ] = df_values df = df . drop ( columns = [ value_col ]) return df df = dict_col_2_cols ( df ) df . head ( 3 ) Unnamed: 0 start end valueSum nuclear biomass coal gas hydro wind windTotal solar demand pumpedStorage imports exports balance 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z {'nuclear': 6.924, 'biomass': 1.116, 'coal': 0... 6.924 1.116 0 5.853 0.405 11.304 {'windOnshore': 8.054581, 'windOffshore': 3.14... 0 27.336 0 {'belgian': 0, 'dutch': 0.182, 'french': 1.552... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.552, 'dutch': 0.182, 'irish': -0.... 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z {'nuclear': 6.838, 'biomass': 1.103, 'coal': 0... 6.838 1.103 0 6.292 0.388 11.327 {'windOnshore': 7.860487, 'windOffshore': 3.25... 0 27.722 0.024 {'belgian': 0, 'dutch': 0.196, 'french': 1.554... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.554, 'dutch': 0.196, 'irish': -0.... 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z {'nuclear': 6.834, 'biomass': 1.09, 'coal': 0,... 6.834 1.09 0 5.719 0.372 11.335 {'windOnshore': 7.879198000000001, 'windOffsho... 0 27.442 0 {'belgian': 0, 'dutch': 0.588, 'french': 1.504... {'french': 0, 'dutch': 0, 'irish': 0, 'pumpedS... {'french': 1.504, 'dutch': 0.588, 'irish': -0.... Unfortunately however this doesn't handle repeated nesting of dictionaries, we'll create a wrapper that does #exports def clean_nested_dict_cols ( df ): \"\"\"Unpacks columns contining nested dictionaries\"\"\" # Calculating columns that are still dictionaries s_types = df . iloc [ 0 ] . apply ( lambda val : type ( val )) cols_with_dicts = s_types [ s_types == dict ] . index while len ( cols_with_dicts ) > 0 : for col_with_dicts in cols_with_dicts : # Extracting dataframes from dictionary columns df = dict_col_2_cols ( df , col_with_dicts ) # Recalculating columns that are still dictionaries s_types = df . iloc [ 0 ] . apply ( lambda val : type ( val )) cols_with_dicts = s_types [ s_types == dict ] . index return df df = clean_nested_dict_cols ( df ) df . head () Unnamed: 0 start end nuclear biomass coal gas hydro wind solar demand pumpedStorage windOnshore windOffshore belgian dutch french ireland northernIreland irish 0 2019-01-01T00:00:00Z 2019-01-01T00:30:00Z 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01T00:30:00Z 2019-01-01T01:00:00Z 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01T01:00:00Z 2019-01-01T01:30:00Z 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01T01:30:00Z 2019-01-01T02:00:00Z 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01T02:00:00Z 2019-01-01T02:30:00Z 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 Next we'll process the datetime index #exports def set_dt_idx ( df : pd . DataFrame , idx_name = 'local_datetime' ): \"\"\" Converts the start datetime to UK local time, then sets it as the index and removes the original datetime columns \"\"\" idx_dt = pd . DatetimeIndex ( pd . to_datetime ( df [ 'start' ], utc = True )) . tz_convert ( 'Europe/London' ) idx_dt . name = idx_name df . index = idx_dt df = df . drop ( columns = [ 'start' , 'end' ]) return df def create_df_dt_rng ( start_date , end_date , freq = '30T' , tz = 'Europe/London' , dt_str_template = '%Y-%m- %d ' ): \"\"\" Creates a dataframe mapping between local datetimes and electricity market dates/settlement periods \"\"\" # Creating localised datetime index s_dt_rng = pd . date_range ( start_date , end_date , freq = freq , tz = tz ) s_dt_SP_count = pd . Series ( 0 , index = s_dt_rng ) . resample ( 'D' ) . count () # Creating SP column SPs = [] for num_SPs in list ( s_dt_SP_count ): SPs += list ( range ( 1 , num_SPs + 1 )) # Creating datetime dataframe df_dt_rng = pd . DataFrame ( index = s_dt_rng ) df_dt_rng . index . name = 'local_datetime' # Adding query call cols df_dt_rng [ 'SP' ] = SPs df_dt_rng [ 'date' ] = df_dt_rng . index . strftime ( dt_str_template ) return df_dt_rng def clean_df_dts ( df ): \"\"\"Cleans the datetime index of the passed DataFrame\"\"\" df = set_dt_idx ( df ) df = df [ ~ df . index . duplicated ()] df_dt_rng = create_df_dt_rng ( df . index . min (), df . index . max ()) df = df . reindex ( df_dt_rng . index ) df [ 'SP' ] = df_dt_rng [ 'SP' ] # Adding settlement period designation return df df = clean_df_dts ( df ) df . head () local_datetime nuclear biomass coal gas hydro wind solar demand pumpedStorage windOnshore windOffshore belgian dutch french ireland northernIreland irish SP 2019-01-01 00:00:00+00:00 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01 00:30:00+00:00 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01 01:00:00+00:00 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01 01:30:00+00:00 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01 02:00:00+00:00 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 5 We'll now combine all of the previous steps and add some column renaming where we want to tidy them up a bit #exports def retrieve_stream_df ( start_date : str , end_date : str , stream : str , time_group = '30m' , renaming_dict = {}): \"\"\" Makes the call to Electric Insights and parses the response into a dataframe which is returned Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' stream: One of 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' renaming_dict: Mapping from old to new column names \"\"\" # Calling data and parsing into dataframe r_json = query_API ( start_date , end_date , stream , time_group ) df = pd . DataFrame . from_dict ( r_json ) # Handling entrys which are dictionarys df = clean_nested_dict_cols ( df ) # Setting index as localised datetime, reindexing with all intervals and adding SP df = clean_df_dts ( df ) # Renaming value col if 'value' in df . columns : df = df . rename ( columns = { 'value' : stream }) if 'referenceOnly' in df . columns : df = df . drop ( columns = [ 'referenceOnly' ]) df = df . rename ( columns = renaming_dict ) return df start_date = '2019-01-01' end_date = '2019-01-31' stream = 'generation-mix' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' } df = retrieve_stream_df ( start_date , end_date , stream , renaming_dict = renaming_dict ) df . head () local_datetime nuclear biomass coal gas hydro wind solar demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish SP 2019-01-01 00:00:00+00:00 6.924 1.116 0 5.853 0.405 11.304 0 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 1 2019-01-01 00:30:00+00:00 6.838 1.103 0 6.292 0.388 11.327 0 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2 2019-01-01 01:00:00+00:00 6.834 1.09 0 5.719 0.372 11.335 0 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 3 2019-01-01 01:30:00+00:00 6.83 1.085 0 5.02 0.368 11.063 0 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 4 2019-01-01 02:00:00+00:00 6.827 1.081 0 4.964 0.355 10.786 0 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 5","title":"Single Stream"},{"location":"dev-01-retrieval/#multiple-streams","text":"We'll now create further functionality for retrieving all of the streams and combining them, before doing so we'll create a helper function for checking the streams are allowed #exports def check_streams ( streams = '*' ): \"\"\" Checks that the streams given are a list containing only possible streams, or is all streams - '*'. \"\"\" possible_streams = [ 'prices_ahead' , 'prices' , 'temperatures' , 'emissions' , 'generation-mix' ] if isinstance ( streams , list ): unrecognised_streams = list ( set ( streams ) - set ( possible_streams )) if len ( unrecognised_streams ) == 0 : return streams else : unrecognised_streams_2_print = '' . join ([ \"'\" + stream + \"', \" for stream in unrecognised_streams ])[: - 2 ] raise ValueError ( f \"Streams { unrecognised_streams_2_print } could not be recognised, must be one of: { ', ' . join ( possible_streams ) } \" ) elif streams == '*' : return possible_streams else : raise ValueError ( f \"Streams could not be recognised, must be one of: { ', ' . join ( possible_streams ) } \" ) streams = check_streams () streams ['prices_ahead', 'prices', 'temperatures', 'emissions', 'generation-mix'] By default all streams are returned but if we provide a list it will be checked streams = check_streams ([ 'prices' , 'emissions' ]) streams ['prices', 'emissions'] However, if we try to check a list containing a stream that doesn't exist we should receive an error try : _ = check_streams ([ 'not_a_stream' ]) print ( 'Success!' ) except Exception as e : print ( 'Error! \\n\\n ' + str ( e )) Error! Streams 'not_a_stream' could not be recognised, must be one of: prices_ahead, prices, temperatures, emissions, generation-mix Next we'll create a wrapper for downloading and combining all of the streams together #exports def retrieve_streams_df ( start_date : str , end_date : str , streams = '*' , time_group = '30m' , renaming_dict = {}): \"\"\" Makes the calls to Electric Insights for the given streams and parses the responses into a dataframe which is returned Parameters: start_date: Start date for data given as a string in the form '%Y-%m-%d' end_date: End date for data given as a string in the form '%Y-%m-%d' streams: Contains 'prices_ahead', 'prices_ahead', 'prices', 'temperatures' or 'emissions', or is given as all, '*' time_group: One of '30m', '1h', '1d' or '7d'. The default is '30m' \"\"\" df = pd . DataFrame () streams = check_streams ( streams ) for stream in streams : df_stream = retrieve_stream_df ( start_date , end_date , stream , renaming_dict = renaming_dict ) df [ df_stream . columns ] = df_stream return df streams = '*' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } df = retrieve_streams_df ( start_date , end_date , streams , renaming_dict = renaming_dict ) df . head () local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2019-01-01 00:00:00+00:00 48.81 1 15 15 9.1 2287.01 83.6629 6.924 1.116 0 ... 27.336 0 8.05458 3.14171 0 0.182 1.552 0 0 -0.702 2019-01-01 00:30:00+00:00 50.24 2 15 15 9.1 2467.91 89.0234 6.838 1.103 0 ... 27.722 0.024 7.86049 3.25389 0 0.196 1.554 0 0 -0.696 2019-01-01 01:00:00+00:00 41.9 3 16 16 9.1 2411.83 87.8884 6.834 1.09 0 ... 27.442 0 7.8792 3.34085 0 0.588 1.504 0 0 -0.722 2019-01-01 01:30:00+00:00 39.32 4 16 16 9.1 2119.53 80.073 6.83 1.085 0 ... 26.47 0 7.70887 3.2137 0 0.6 1.504 0 0 -0.77 2019-01-01 02:00:00+00:00 34.09 5 16 16 9.1 2069.84 79.0166 6.827 1.081 0 ... 26.195 0 7.47943 3.12271 0 0.678 1.504 0 0 -0.91 Now we're ready to retrieve all of the streams in one, which we'll do for all years that data is available, then we'll save the resulting DataFrame. %% time streams = '*' renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } retrieve_save_EI_data = False if retrieve_save_EI_data == True : df = pd . DataFrame () for year in track ( range ( 2010 , 2021 )): start_date = f ' { year } -01-01' end_date = f ' { year } -12-31' df_year = retrieve_streams_df ( start_date , end_date , streams , renaming_dict = renaming_dict ) df = df . append ( df_year ) df . to_csv ( '../data/raw/electric_insights.csv' ) Wall time: 0 ns","title":"Multiple Streams"},{"location":"dev-01-retrieval/#energy-charts","text":"We'll start by requesting the JSON that is used to populate the fuel-type output chart on the website def year_week_2_prod_url ( year , week , data_prefix = '' ): \"\"\"Given a specified year and week the relevant `production_url` for energy-charts is returned\"\"\" if year < 2019 : data_prefix = 'raw_' production_url = f 'https://energy-charts.info/charts/power/ { data_prefix } data/de/week_ { year } _ { str ( week ) . zfill ( 2 ) } .json' return production_url year = 2018 week = 12 production_url = year_week_2_prod_url ( year , week ) r = requests . get ( production_url ) r <Response [200]> Next we focus on parsing the only non-uniform column which relates to the balancing required on the system def fuel_json_2_net_balance ( r_json ): \"\"\"Extracts the balance time-series\"\"\" if 'values' in r_json [ 0 ] . keys (): # pre-2019 format df_balance = pd . DataFrame ( r_json [ 0 ][ 'values' ]) s_balance = ( df_balance . assign ( datetime = pd . to_datetime ( df_balance [ 0 ] * 1000000 , utc = True )) . drop ( columns = 0 ) . set_index ( 'datetime' ) . rename ( columns = { 1 : 'value' , }) [ 'value' ] ) else : s_balance = pd . Series ( r_json [ 0 ][ 'data' ], index = pd . to_datetime ( np . array ( r_json [ 0 ][ 'xAxisValues' ]) * 1000000 , utc = True )) s_balance . index = s_balance . index . tz_convert ( 'Europe/Berlin' ) return s_balance # Retrieving net balance data r_json = r . json () s_balance = fuel_json_2_net_balance ( r_json ) # Getting comments data_src = r_json [ 0 ][ 'datasource' ] comment = ' \\n ' . join ([ item [ 'en' ] for item in r_json [ 0 ][ 'comment' ]]) print ( data_src ) print ( comment ) s_balance 50 Hertz, Amprion, Tennet, TransnetBW, EEX Net generation of power plants for public power supply. datetime 2018-03-19 00:00:00+01:00 -9.044 2018-03-19 01:00:00+01:00 -9.522 2018-03-19 02:00:00+01:00 -9.576 2018-03-19 03:00:00+01:00 -9.978 2018-03-19 04:00:00+01:00 -9.787 ... 2018-03-25 19:00:00+02:00 -6.319 2018-03-25 20:00:00+02:00 -5.968 2018-03-25 21:00:00+02:00 -6.329 2018-03-25 22:00:00+02:00 -6.134 2018-03-25 23:00:00+02:00 -6.839 Name: value, Length: 167, dtype: float64 We combine this with a parser for the main columns def response_2_df ( r ): \"\"\"Parses the json response to a DataFrame\"\"\" r_json = r . json () s_balance = fuel_json_2_net_balance ( r_json ) if 'key' in r_json [ 1 ] . keys (): # pre-2019 keys = [ x [ 'key' ][ 0 ][ 'en' ] for x in r_json [ 1 :]] values = [ x [ 'values' ] for x in r_json [ 1 :]] df_fuels = ( pd . DataFrame ( dict ( zip ( keys , values )), index = s_balance . index ) . apply ( lambda s : s . apply ( lambda x : x [ - 1 ])) . assign ( net_balance = s_balance ) ) else : cols = [ x [ 'name' ][ 0 ][ 'en' ] for x in r_json [ 1 :]] data = [ x [ 'data' ] for x in r_json [ 1 :]] df_fuels = pd . DataFrame ( np . array ( data ) . T , columns = cols , index = s_balance . index ) . assign ( net_balance = s_balance ) return df_fuels df_fuels = response_2_df ( r ) df_fuels . head ( 2 ) datetime Hydro Power Biomass Uranium Brown Coal Hard Coal Oil Gas Others Pumped Storage Seasonal Storage Wind Solar net_balance 2018-03-19 00:00:00+01:00 2.292 5.82 7.721 14.499 8.508 0.18 3.207 0.068 0.42 0.078 20.754 0 -9.044 2018-03-19 01:00:00+01:00 2.291 5.82 7.829 14.49 8.941 0.179 3.287 0.068 0.245 0.064 19.012 0 -9.522 And wrap them in a single function which can accept a year and week before returning the parsed dataframe def year_week_2_fuel_df ( year , week ): \"\"\"Given a specified year and week the relevant `df_fuels` dataset for energy-charts is returned\"\"\" production_url = year_week_2_prod_url ( year , week ) r = requests . get ( production_url ) # This is a general catch-all but mainly added to account # for years with 53 weeks rather than 52 if r . status_code == 404 : df_fuels = pd . DataFrame () else : df_fuels = response_2_df ( r ) return df_fuels year = 2015 week = 53 df_fuels = year_week_2_fuel_df ( year , week ) df_fuels . head ( 2 ) datetime Hydro Power Biomass Uranium Brown Coal Hard Coal Oil Gas Others Pumped Storage Seasonal Storage Wind Solar net_balance 2015-12-28 00:00:00+01:00 1.356 6 10.567 13.247 3.437 0.19 2.338 0.15 0.154 0.013 8.156 0 -3.923 2015-12-28 01:00:00+01:00 1.348 6 10.611 13.744 3.109 0.191 2.533 0.124 0.157 0.012 7.059 0 -4.139 Now we can iterate over all year and week pairs to retrieve the full dataset years = list ( range ( 2010 , 2021 )) weeks = list ( range ( 1 , 54 )) df_fuels = pd . DataFrame () for year , week in track ( product ( years , weeks ), total = len ( years ) * len ( weeks )): try : df_fuels_yr_wk = year_week_2_fuel_df ( year , week ) df_fuels = df_fuels . append ( df_fuels_yr_wk , sort = True ) except : warn ( f 'Failed to retrieve week { week } in { year } ' ) df_fuels . head ( 3 ) 100% 580/583 [03:16 < 00:00, 0.34s/it] Unnamed: 0 Biomass Biomass planned Brown Coal Brown Coal planned Gas Gas planned Hard Coal Hard Coal planned Hydro Power Hydro Power planned ... Residual load forecast Seasonal Storage Seasonal Storage ante Solar Solar forecast Uranium Uranium planned Wind Wind forecast net_balance 2010-01-04 00:00:00+01:00 3.637 nan 16.533 nan 4.726 nan 10.078 nan 2.331 nan ... nan 0.068 nan 0 nan 16.826 nan 0.635 nan -1.229 2010-01-04 01:00:00+01:00 3.637 nan 16.544 nan 4.856 nan 8.816 nan 2.293 nan ... nan 0.003 nan 0 nan 16.841 nan 0.528 nan -1.593 2010-01-04 02:00:00+01:00 3.637 nan 16.368 nan 5.275 nan 7.954 nan 2.299 nan ... nan 0 nan 0 nan 16.846 nan 0.616 nan -1.378 We'll remove the null columns and reorder the remaining ones to clean it up a little bit columns = [ 'Biomass' , 'Brown Coal' , 'Gas' , 'Hard Coal' , 'Hydro Power' , 'Oil' , 'Others' , 'Pumped Storage' , 'Seasonal Storage' , #'Import Balance', 'Solar' , 'Uranium' , 'Wind' , 'net_balance' ] df_fuels_clean = ( df_fuels [ columns ] . astype ( float ) . resample ( 'H' ) . mean () . dropna ( how = 'all' , axis = 1 ) ) df_fuels_clean . head () Unnamed: 0 Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance 2010-01-04 00:00:00+01:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 2010-01-04 01:00:00+01:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 2010-01-04 02:00:00+01:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 2010-01-04 03:00:00+01:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 2010-01-04 04:00:00+01:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 We'll quickly confirm that there are no duplicates assert df_fuels_clean . index . duplicated () . sum () == 0 , 'There are multiple entries for a datetime' Next we check for any missing datetimes s_null_vals = df_fuels_clean . isnull () . sum ( axis = 1 ) == df_fuels_clean . shape [ 1 ] print ( f ' { round ( 100 * s_null_vals . mean (), 2 ) } % of the datetimes are missing data' ) print ( f \"There are { ( s_null_vals . resample ( 'D' ) . sum () > 0 ) . sum () } days with missing data, totalling { s_null_vals . sum () } hours of individual missing hours\" ) 0.16% of the datetimes are missing data There are 7 days with missing data, totalling 157 hours of individual missing hours Because there are only a few missing datetimes we'll choose to drop them from the dataset df_fuels_clean = df_fuels_clean [ ~ s_null_vals ] Finally we'll save the cleaned dataframe df_fuels_clean . index . name = 'local_datetime' df_fuels_clean . to_csv ( '../data/raw/energy_charts.csv' )","title":"Energy-Charts"},{"location":"dev-01-retrieval/#entsoe","text":"We'll use ENTSOE as the source for our German price data, we'll also have to use it to get the exact RES and demand volumes for the bidding zone that Germany is within.","title":"ENTSOE"},{"location":"dev-01-retrieval/#prices","text":"We'll begin by initialising our client API for ENTSOE You will need to supply your own API key, we recommend you store it in a .env file load_dotenv () ENTSOE_API_KEY = os . environ [ 'ENTSOE_API_KEY' ] client = EntsoePandasClient ( api_key = api_key ) client <entsoe.entsoe.EntsoePandasClient at 0x216d1b88d60> We'll then make a test request for price data from the German market params = { 'documentType' : 'A44' , 'in_Domain' : '10Y1001A1001A63L' , 'out_Domain' : '10Y1001A1001A63L' } start = pd . Timestamp ( '2018-09-01' , tz = 'UTC' ) end = pd . Timestamp ( '2018-09-30' , tz = 'UTC' ) r = client . _base_request ( params = params , start = start , end = end ) r <Response [200]> We'll extract the price time-series from the returned JSON #exports def parse_A44_response ( r , freq = 'H' , tz = 'UTC' ): \"\"\"Extracts the price time-series\"\"\" s_price = pd . Series ( dtype = float ) parsed_r = xmltodict . parse ( r . text ) for timeseries in parsed_r [ 'Publication_MarketDocument' ][ 'TimeSeries' ]: dt_rng = pd . date_range ( timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ], timeseries [ 'Period' ][ 'timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] s_dt_price = pd . DataFrame ( timeseries [ 'Period' ][ 'Point' ])[ 'price.amount' ] . astype ( float ) s_dt_price . index = dt_rng s_price = s_price . append ( s_dt_price ) assert s_price . index . duplicated () . sum () == 0 , 'There are duplicate date indexes' return s_price s_price = parse_A44_response ( r ) s_price . head () 2018-08-31 22:00:00+00:00 57.58 2018-08-31 23:00:00+00:00 55.99 2018-09-01 00:00:00+00:00 55.56 2018-09-01 01:00:00+00:00 54.02 2018-09-01 02:00:00+00:00 52.69 Freq: H, dtype: float64 We can't query very large date ranges so we'll break up our requests into quarterly batches. We'll also account for the fact that the German market changes to exclude Austria after 2018-10-01 by creating two sets of date pairs. DE_AT_LU_dt_pairs = list ( zip ( pd . date_range ( '2015' , '2018-07' , freq = '3MS' ) . strftime ( '%Y-%m- %d %H:%M' ), pd . date_range ( '2015-03-31 23:55' , '2018-09-30 23:55' , freq = '3M' ) . strftime ( '%Y-%m- %d %H:%M' ) )) DE_AT_LU_dt_pairs [: 5 ] [('2015-01-01 00:00', '2015-03-31 23:55'), ('2015-04-01 00:00', '2015-06-30 23:55'), ('2015-07-01 00:00', '2015-09-30 23:55'), ('2015-10-01 00:00', '2015-12-31 23:55'), ('2016-01-01 00:00', '2016-03-31 23:55')] We're now ready to create a wrapper to collate the date from each date batch #exports def retreive_DAM_prices ( dt_pairs , domain = '10Y1001A1001A63L' ): \"\"\"Retrieves and collates the day-ahead prices for the specified date ranges\"\"\" params = { 'documentType' : 'A44' , 'in_Domain' : domain , 'out_Domain' : domain } s_price = pd . Series ( dtype = float ) for dt_pair in track ( dt_pairs ): start = pd . Timestamp ( dt_pair [ 0 ], tz = 'UTC' ) end = pd . Timestamp ( dt_pair [ 1 ], tz = 'UTC' ) try : r = client . _base_request ( params = params , start = start , end = end ) s_price_dt_rng = parse_A44_response ( r ) s_price = s_price . append ( s_price_dt_rng ) except : warn ( f \" { start . strftime ( '%Y-%m- %d ' ) } - { end . strftime ( '%Y-%m- %d ' ) } failed\" ) return s_price s_price_DE_AT_LU = retreive_DAM_prices ( DE_AT_LU_dt_pairs ) s_price_DE_AT_LU . head () 100% 15/15 [00:20 < 00:01, 1.31s/it] 2015-01-04 23:00:00+00:00 22.34 2015-01-05 00:00:00+00:00 17.93 2015-01-05 01:00:00+00:00 15.17 2015-01-05 02:00:00+00:00 16.38 2015-01-05 03:00:00+00:00 17.38 dtype: float64 We'll repeat this for the market excluding Austria DE_LU_dt_pairs = list ( zip ( pd . date_range ( '2018-10' , '2021-01' , freq = '3MS' ) . strftime ( '%Y-%m- %d %H:%M' ), pd . date_range ( '2018-12-31 23:55' , '2021-02-28 23:55' , freq = '3M' ) . strftime ( '%Y-%m- %d %H:%M' ) )) s_price_DE_LU = retreive_DAM_prices ( DE_LU_dt_pairs , domain = '10Y1001A1001A82H' ) s_price_DE_LU . head () 100% 9/9 [00:11 < 00:01, 1.25s/it] 2018-09-30 22:00:00+00:00 59.53 2018-09-30 23:00:00+00:00 56.10 2018-10-01 00:00:00+00:00 51.41 2018-10-01 01:00:00+00:00 47.38 2018-10-01 02:00:00+00:00 47.59 dtype: float64 We'll now combine and visualise the time-series s_price = s_price_DE_AT_LU . append ( s_price_DE_LU ) s_price . index = s_price . index . tz_convert ( 'Europe/Berlin' ) s_price . plot () <AxesSubplot:> Before moving on we'll save this series as a csv s_price . name = 'DE_price' s_price . index . name = 'local_datetime' s_price . to_csv ( '../data/raw/ENTSOE_DE_price.csv' )","title":"Prices"},{"location":"dev-01-retrieval/#generation-by-fuel-type","text":"We'll now create the functions for retrieving and parsing the fuel data #exports def parse_A75_response ( r , freq = '15T' , tz = 'UTC' , warn_on_failure = False ): \"\"\"Extracts the production data by fuel-type from the JSON response\"\"\" psr_code_to_type = { 'A03' : 'Mixed' , 'A04' : 'Generation' , 'A05' : 'Load' , 'B01' : 'Biomass' , 'B02' : 'Fossil Brown coal/Lignite' , 'B03' : 'Fossil Coal-derived gas' , 'B04' : 'Fossil Gas' , 'B05' : 'Fossil Hard coal' , 'B06' : 'Fossil Oil' , 'B07' : 'Fossil Oil shale' , 'B08' : 'Fossil Peat' , 'B09' : 'Geothermal' , 'B10' : 'Hydro Pumped Storage' , 'B11' : 'Hydro Run-of-river and poundage' , 'B12' : 'Hydro Water Reservoir' , 'B13' : 'Marine' , 'B14' : 'Nuclear' , 'B15' : 'Other renewable' , 'B16' : 'Solar' , 'B17' : 'Waste' , 'B18' : 'Wind Offshore' , 'B19' : 'Wind Onshore' , 'B20' : 'Other' , 'B21' : 'AC Link' , 'B22' : 'DC Link' , 'B23' : 'Substation' , 'B24' : 'Transformer' } parsed_r = xmltodict . parse ( r . text ) columns = [ f 'B { str ( fuel_idx ) . zfill ( 2 ) } ' for fuel_idx in np . arange ( 1 , 24 )] index = pd . date_range ( parsed_r [ 'GL_MarketDocument' ][ 'time_Period.timeInterval' ][ 'start' ], parsed_r [ 'GL_MarketDocument' ][ 'time_Period.timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] df_production = pd . DataFrame ( dtype = float , columns = columns , index = index ) for timeseries in parsed_r [ 'GL_MarketDocument' ][ 'TimeSeries' ]: try : psr_type = timeseries [ 'MktPSRType' ][ 'psrType' ] dt_rng = pd . date_range ( timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ], timeseries [ 'Period' ][ 'timeInterval' ][ 'end' ], freq = freq , tz = tz )[: - 1 ] s_psr_type = pd . DataFrame ( timeseries [ 'Period' ][ 'Point' ])[ 'quantity' ] . astype ( float ) s_psr_type . index = dt_rng df_production [ psr_type ] = s_psr_type except : if warn_on_failure == True : warn ( f \" { timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ] } - { timeseries [ 'Period' ][ 'timeInterval' ][ 'start' ] } failed for { psr_type } \" ) assert df_production . index . duplicated () . sum () == 0 , 'There are duplicate date indexes' df_production = df_production . dropna ( how = 'all' ) . dropna ( how = 'all' , axis = 1 ) df_production = df_production . rename ( columns = psr_code_to_type ) return df_production def retrieve_production ( dt_pairs , domain = '10Y1001A1001A63L' , warn_on_failure = False ): \"\"\"Retrieves and collates the production data for the specified date ranges\"\"\" params = { 'documentType' : 'A75' , 'processType' : 'A16' , 'in_Domain' : domain } df_production = pd . DataFrame ( dtype = float ) for dt_pair in track ( dt_pairs ): start = pd . Timestamp ( dt_pair [ 0 ], tz = 'UTC' ) end = pd . Timestamp ( dt_pair [ 1 ], tz = 'UTC' ) try : r = client . _base_request ( params = params , start = start , end = end ) df_production_dt_rng = parse_A75_response ( r , warn_on_failure = warn_on_failure ) df_production = df_production . append ( df_production_dt_rng ) except : if warn_on_failure == True : warn ( f \" { start . strftime ( '%Y-%m- %d ' ) } - { end . strftime ( '%Y-%m- %d ' ) } failed\" ) return df_production df_production_DE_AT_LU = retrieve_production ( DE_AT_LU_dt_pairs ) df_production_DE_LU = retrieve_production ( DE_LU_dt_pairs , domain = '10Y1001A1001A82H' ) df_production = df_production_DE_AT_LU . append ( df_production_DE_LU ) df_production . head () 100% 15/15 [05:37 < 00:18, 22.45s/it] 100% 9/9 [02:37 < 00:14, 17.45s/it] Unnamed: 0 Biomass Fossil Brown coal/Lignite Fossil Coal-derived gas Fossil Gas Fossil Hard coal Fossil Oil Geothermal Hydro Pumped Storage Hydro Run-of-river and poundage Hydro Water Reservoir Nuclear Other renewable Solar Waste Wind Offshore Wind Onshore Other Marine 2015-01-01 23:00:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 391 0 nan nan 2015-01-01 23:15:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 292 0 nan nan 2015-01-01 23:30:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 271 0 nan nan 2015-01-01 23:45:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 266 0 nan nan 2015-01-02 00:00:00+00:00 nan nan nan nan nan 5 nan nan nan 0 nan nan nan nan 268 0 nan nan We'll quickly inspect for the presence of null values, due to the large number found we'll use the energy-charts data when it comes to fuel generation df_production . isnull () . mean () Biomass 0.068175 Fossil Brown coal/Lignite 0.068175 Fossil Coal-derived gas 0.998617 Fossil Gas 0.385562 Fossil Hard coal 0.068175 Fossil Oil 0.362709 Geothermal 0.068175 Hydro Pumped Storage 0.068175 Hydro Run-of-river and poundage 0.068175 Hydro Water Reservoir 0.250576 Nuclear 0.269564 Other renewable 0.068175 Solar 0.178234 Waste 0.068175 Wind Offshore 0.157710 Wind Onshore 0.141564 Other 0.068175 Marine 0.979668 dtype: float64","title":"Generation by Fuel-Type"},{"location":"dev-02-eda/","text":"Exploratory Data Analysis \u00b6 This notebook includes some visualisation and exploration of the price and fuel data for Germany and Great Britain Imports \u00b6 #exports import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import matplotlib.transforms as mtf Loading Data \u00b6 #exports def load_EI_df ( EI_fp ): \"\"\"Loads the electric insights data and returns a DataFrame\"\"\" df = pd . read_csv ( EI_fp ) df [ 'local_datetime' ] = pd . to_datetime ( df [ 'local_datetime' ], utc = True ) df = df . set_index ( 'local_datetime' ) return df %% time df = load_EI_df ( '../data/raw/electric_insights.csv' ) df . head () Wall time: 7.95 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll do the same for the German Energy-Charts and ENTSOE data #exports def load_DE_df ( EC_fp , ENTSOE_fp ): \"\"\"Loads the energy-charts and ENTSOE data and returns a DataFrame\"\"\" # Energy-Charts df_DE = pd . read_csv ( EC_fp ) df_DE [ 'local_datetime' ] = pd . to_datetime ( df_DE [ 'local_datetime' ], utc = True ) df_DE = df_DE . set_index ( 'local_datetime' ) # ENTSOE df_ENTSOE = pd . read_csv ( ENTSOE_fp ) df_ENTSOE [ 'local_datetime' ] = pd . to_datetime ( df_ENTSOE [ 'local_datetime' ], utc = True ) df_ENTSOE = df_ENTSOE . set_index ( 'local_datetime' ) # Combining data df_DE [ 'demand' ] = df_DE . sum ( axis = 1 ) s_price = df_ENTSOE [ 'DE_price' ] df_DE [ 'price' ] = s_price [ ~ s_price . index . duplicated ( keep = 'first' )] return df_DE df_DE = load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan Stacked-Fuels Time-Series \u00b6 We'll create a stacked plot of the different generation types over time. We'll begin by cleaning the dataframe and merging columns so that it's ready for plotting, we'll also take the 7-day rolling average to make long-term trends clearer. #exports def clean_df_for_plot ( df , freq = '7D' ): \"\"\"Cleans the electric insights dataframe for plotting\"\"\" fuel_order = [ 'Imports & Storage' , 'nuclear' , 'biomass' , 'gas' , 'coal' , 'hydro' , 'wind' , 'solar' ] interconnectors = [ 'french' , 'irish' , 'dutch' , 'belgian' , 'ireland' , 'northern_ireland' ] df = ( df . copy () . assign ( imports_storage = df [ interconnectors + [ 'pumped_storage' ]] . sum ( axis = 1 )) . rename ( columns = { 'imports_storage' : 'Imports & Storage' }) . drop ( columns = interconnectors + [ 'demand' , 'pumped_storage' ]) [ fuel_order ] ) df_resampled = df . astype ( 'float' ) . resample ( freq ) . mean () return df_resampled df_plot = clean_df_for_plot ( df ) df_plot . head () local_datetime Imports & Storage nuclear biomass gas coal hydro wind solar 2009-01-01 00:00:00+00:00 -0.039018 5.76854 0 16.2951 20.1324 0.35589 0.390015 0 2009-01-08 00:00:00+00:00 -0.921768 5.5829 0 16.3811 21.6997 0.551753 1.15155 0 2009-01-15 00:00:00+00:00 -0.024241 5.55999 0 14.84 20.4463 0.704382 1.483 0 2009-01-22 00:00:00+00:00 0.18283 6.22841 0 14.4678 20.5907 0.562277 0.938827 0 2009-01-29 00:00:00+00:00 0.120204 6.79959 0 13.9657 21.3497 0.519632 1.36261 0 We'll also define the colours we'll use for each fuel-type N.b. the colour palette used is from this paper fuel_colour_dict_rgb = { 'Imports & Storage' : ( 121 , 68 , 149 ), 'nuclear' : ( 77 , 157 , 87 ), 'biomass' : ( 168 , 125 , 81 ), 'gas' : ( 254 , 156 , 66 ), 'coal' : ( 122 , 122 , 122 ), 'hydro' : ( 50 , 120 , 196 ), 'wind' : ( 72 , 194 , 227 ), 'solar' : ( 255 , 219 , 65 ), } However we need to convert from rgb to matplotlib plotting colours (0-1 not 0-255) #exports def rgb_2_plt_tuple ( rgb_tuple ): \"\"\"converts a standard rgb set from a 0-255 range to 0-1\"\"\" plt_tuple = tuple ([ x / 255 for x in rgb_tuple ]) return plt_tuple def convert_fuel_colour_dict_to_plt_tuple ( fuel_colour_dict_rgb ): \"\"\"Converts a dictionary of fuel colours to matplotlib colour values\"\"\" fuel_colour_dict_plt = fuel_colour_dict_rgb . copy () fuel_colour_dict_plt = { fuel : rgb_2_plt_tuple ( rgb_tuple ) for fuel , rgb_tuple in fuel_colour_dict_plt . items () } return fuel_colour_dict_plt fuel_colour_dict_plt = convert_fuel_colour_dict_to_plt_tuple ( fuel_colour_dict_rgb ) sns . palplot ( fuel_colour_dict_plt . values ()) Finally we can plot the stacked fuel plot itself #exports def hide_spines ( ax , positions = [ \"top\" , \"right\" ]): \"\"\" Pass a matplotlib axis and list of positions with spines to be removed Parameters: ax: Matplotlib axis object positions: Python list e.g. ['top', 'bottom'] \"\"\" assert isinstance ( positions , list ), \"Position must be passed as a list \" for position in positions : ax . spines [ position ] . set_visible ( False ) def stacked_fuel_plot ( df , fuel_colour_dict , ax = None , save_path = None , dpi = 150 ): \"\"\"Plots the electric insights fuel data as a stacked area graph\"\"\" df = df [ fuel_colour_dict . keys ()] if ax == None : fig = plt . figure ( figsize = ( 10 , 5 ), dpi = dpi ) ax = plt . subplot () ax . stackplot ( df . index . values , df . values . T , labels = df . columns . str . capitalize (), linewidth = 0.25 , edgecolor = 'white' , colors = list ( fuel_colour_dict . values ())) plt . rcParams [ 'axes.ymargin' ] = 0 ax . spines [ 'bottom' ] . set_position ( 'zero' ) hide_spines ( ax ) ax . set_xlim ( df . index . min (), df . index . max ()) ax . legend ( ncol = 4 , bbox_to_anchor = ( 0.85 , 1.15 ), frameon = False ) ax . set_ylabel ( 'Generation (GW)' ) if save_path : fig . savefig ( save_path ) return ax stacked_fuel_plot ( df_plot , fuel_colour_dict_plt , dpi = 250 ) <AxesSubplot:ylabel='Generation (GW)'>","title":"Exploratory Data Analysis"},{"location":"dev-02-eda/#exploratory-data-analysis","text":"This notebook includes some visualisation and exploration of the price and fuel data for Germany and Great Britain","title":"Exploratory Data Analysis"},{"location":"dev-02-eda/#imports","text":"#exports import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import matplotlib.transforms as mtf","title":"Imports"},{"location":"dev-02-eda/#loading-data","text":"#exports def load_EI_df ( EI_fp ): \"\"\"Loads the electric insights data and returns a DataFrame\"\"\" df = pd . read_csv ( EI_fp ) df [ 'local_datetime' ] = pd . to_datetime ( df [ 'local_datetime' ], utc = True ) df = df . set_index ( 'local_datetime' ) return df %% time df = load_EI_df ( '../data/raw/electric_insights.csv' ) df . head () Wall time: 7.95 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll do the same for the German Energy-Charts and ENTSOE data #exports def load_DE_df ( EC_fp , ENTSOE_fp ): \"\"\"Loads the energy-charts and ENTSOE data and returns a DataFrame\"\"\" # Energy-Charts df_DE = pd . read_csv ( EC_fp ) df_DE [ 'local_datetime' ] = pd . to_datetime ( df_DE [ 'local_datetime' ], utc = True ) df_DE = df_DE . set_index ( 'local_datetime' ) # ENTSOE df_ENTSOE = pd . read_csv ( ENTSOE_fp ) df_ENTSOE [ 'local_datetime' ] = pd . to_datetime ( df_ENTSOE [ 'local_datetime' ], utc = True ) df_ENTSOE = df_ENTSOE . set_index ( 'local_datetime' ) # Combining data df_DE [ 'demand' ] = df_DE . sum ( axis = 1 ) s_price = df_ENTSOE [ 'DE_price' ] df_DE [ 'price' ] = s_price [ ~ s_price . index . duplicated ( keep = 'first' )] return df_DE df_DE = load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan","title":"Loading Data"},{"location":"dev-02-eda/#stacked-fuels-time-series","text":"We'll create a stacked plot of the different generation types over time. We'll begin by cleaning the dataframe and merging columns so that it's ready for plotting, we'll also take the 7-day rolling average to make long-term trends clearer. #exports def clean_df_for_plot ( df , freq = '7D' ): \"\"\"Cleans the electric insights dataframe for plotting\"\"\" fuel_order = [ 'Imports & Storage' , 'nuclear' , 'biomass' , 'gas' , 'coal' , 'hydro' , 'wind' , 'solar' ] interconnectors = [ 'french' , 'irish' , 'dutch' , 'belgian' , 'ireland' , 'northern_ireland' ] df = ( df . copy () . assign ( imports_storage = df [ interconnectors + [ 'pumped_storage' ]] . sum ( axis = 1 )) . rename ( columns = { 'imports_storage' : 'Imports & Storage' }) . drop ( columns = interconnectors + [ 'demand' , 'pumped_storage' ]) [ fuel_order ] ) df_resampled = df . astype ( 'float' ) . resample ( freq ) . mean () return df_resampled df_plot = clean_df_for_plot ( df ) df_plot . head () local_datetime Imports & Storage nuclear biomass gas coal hydro wind solar 2009-01-01 00:00:00+00:00 -0.039018 5.76854 0 16.2951 20.1324 0.35589 0.390015 0 2009-01-08 00:00:00+00:00 -0.921768 5.5829 0 16.3811 21.6997 0.551753 1.15155 0 2009-01-15 00:00:00+00:00 -0.024241 5.55999 0 14.84 20.4463 0.704382 1.483 0 2009-01-22 00:00:00+00:00 0.18283 6.22841 0 14.4678 20.5907 0.562277 0.938827 0 2009-01-29 00:00:00+00:00 0.120204 6.79959 0 13.9657 21.3497 0.519632 1.36261 0 We'll also define the colours we'll use for each fuel-type N.b. the colour palette used is from this paper fuel_colour_dict_rgb = { 'Imports & Storage' : ( 121 , 68 , 149 ), 'nuclear' : ( 77 , 157 , 87 ), 'biomass' : ( 168 , 125 , 81 ), 'gas' : ( 254 , 156 , 66 ), 'coal' : ( 122 , 122 , 122 ), 'hydro' : ( 50 , 120 , 196 ), 'wind' : ( 72 , 194 , 227 ), 'solar' : ( 255 , 219 , 65 ), } However we need to convert from rgb to matplotlib plotting colours (0-1 not 0-255) #exports def rgb_2_plt_tuple ( rgb_tuple ): \"\"\"converts a standard rgb set from a 0-255 range to 0-1\"\"\" plt_tuple = tuple ([ x / 255 for x in rgb_tuple ]) return plt_tuple def convert_fuel_colour_dict_to_plt_tuple ( fuel_colour_dict_rgb ): \"\"\"Converts a dictionary of fuel colours to matplotlib colour values\"\"\" fuel_colour_dict_plt = fuel_colour_dict_rgb . copy () fuel_colour_dict_plt = { fuel : rgb_2_plt_tuple ( rgb_tuple ) for fuel , rgb_tuple in fuel_colour_dict_plt . items () } return fuel_colour_dict_plt fuel_colour_dict_plt = convert_fuel_colour_dict_to_plt_tuple ( fuel_colour_dict_rgb ) sns . palplot ( fuel_colour_dict_plt . values ()) Finally we can plot the stacked fuel plot itself #exports def hide_spines ( ax , positions = [ \"top\" , \"right\" ]): \"\"\" Pass a matplotlib axis and list of positions with spines to be removed Parameters: ax: Matplotlib axis object positions: Python list e.g. ['top', 'bottom'] \"\"\" assert isinstance ( positions , list ), \"Position must be passed as a list \" for position in positions : ax . spines [ position ] . set_visible ( False ) def stacked_fuel_plot ( df , fuel_colour_dict , ax = None , save_path = None , dpi = 150 ): \"\"\"Plots the electric insights fuel data as a stacked area graph\"\"\" df = df [ fuel_colour_dict . keys ()] if ax == None : fig = plt . figure ( figsize = ( 10 , 5 ), dpi = dpi ) ax = plt . subplot () ax . stackplot ( df . index . values , df . values . T , labels = df . columns . str . capitalize (), linewidth = 0.25 , edgecolor = 'white' , colors = list ( fuel_colour_dict . values ())) plt . rcParams [ 'axes.ymargin' ] = 0 ax . spines [ 'bottom' ] . set_position ( 'zero' ) hide_spines ( ax ) ax . set_xlim ( df . index . min (), df . index . max ()) ax . legend ( ncol = 4 , bbox_to_anchor = ( 0.85 , 1.15 ), frameon = False ) ax . set_ylabel ( 'Generation (GW)' ) if save_path : fig . savefig ( save_path ) return ax stacked_fuel_plot ( df_plot , fuel_colour_dict_plt , dpi = 250 ) <AxesSubplot:ylabel='Generation (GW)'>","title":"Stacked-Fuels Time-Series"},{"location":"dev-03-lowess/","text":"Lowess \u00b6 Outlines the development of the Scikit-Learn compatible Lowess model, as well as its extension LowessDates used for time-adaptive LOWESS regression. Included are functions for extending both models to generate prediction and confidence intervals. Imports \u00b6 #exports import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from collections.abc import Iterable from sklearn import linear_model from sklearn.base import BaseEstimator , RegressorMixin from scipy.optimize import minimize from scipy import linalg from timeit import timeit from ipypb import track from moepy import eda LOWESS Development \u00b6 Before we go ahead any further we'll create some sample data for fitting and also define the fraction of the data over which we'll do the localised regression. x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) frac = 0.5 We can see that we've just created a simple sin curve plt . plot ( x , y ) [<matplotlib.lines.Line2D at 0x1fd650f86d0>] Weights \u00b6 In order to do localised regression we need to know what points are local, for this reason we'll start by creating a function for calculating the distance between one point and all of the other points #exports get_dist = lambda X , x : np . abs ( X - x ) dist = get_dist ( x , x [ 0 ]) dist array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. , 4.2, 4.4, 4.6, 4.8, 5. ]) We've defined our selection of local based on the fraction of surrounding data, this means we need to calculate the distance around any given point that contains the fraction of data specified #exports def get_dist_threshold ( dist , frac = 0.4 ): \"\"\"Identifies the minimum distance that contains the desired data fraction\"\"\" frac_idx = int ( np . ceil ( len ( dist ) * frac )) dist_threshold = sorted ( dist )[ frac_idx ] return dist_threshold dist_threshold = get_dist_threshold ( dist , frac = frac ) dist_threshold 2.6 We'll now define a function that will map from the distances to their relative weights according to a tricube kernel \\[ \\begin{equation} \\label{eqn:tricube_kernel} w(x) = \\left\\{ \\begin{array}{ll} (1 - |x|^3)^3 & \\mbox{for $|x| < 1$} \\\\ 0 & \\mbox{for $|x| \\geq 1$} \\end{array} \\right. \\end{equation} \\] #exports dist_to_weights = lambda dist , dist_threshold = 1 : ( 1 - (( np . abs ( dist ) / dist_threshold ) . clip ( 0 , 1 ) ** 3 )) ** 3 dist_threshold = 5 dist_sample = np . linspace ( 0 , 10 ) weights_sample = dist_to_weights ( dist_sample , dist_threshold ) # Plotting plt . plot ( dist_sample , weights_sample ) plt . xlabel ( 'Distance' ) plt . ylabel ( 'Weight' ) plt . xlim ( 0 , max ( dist_sample )) plt . ylim ( 0 , 1.1 ) (0.0, 1.1) We can now use the distance matrix and threshold to create a vector of the relative weights of all data points for the local regression at a specified location weights = dist_to_weights ( dist , dist_threshold ) weights array([1. , 0.99980801, 0.99846479, 0.99482495, 0.98776226, 0.97619149, 0.95909867, 0.93557909, 0.90488204, 0.86646079, 0.82002586, 0.76559882, 0.70356317, 0.63470792, 0.56025877, 0.4818903 , 0.40171203, 0.32221935, 0.24619951, 0.17658114, 0.11621427, 0.06756635, 0.03231788, 0.01083964, 0.00153137, 0. ]) We'll wrap these steps into a single function and see how long it takes to compute def get_weights ( x , loc , frac = 0.4 ): \"\"\"Calculates the weightings at each data point for a single localised regression\"\"\" dist = get_dist ( x , loc ) dist_threshold = get_dist_threshold ( dist , frac = frac ) weights = dist_to_weights ( dist , dist_threshold ) return weights timeit ( lambda : get_weights ( x , x [ 5 ]), number = 10000 ) 0.6126709999999989 We've successfully calculated the weights with respect to a single point but we need to repeat this across each of value locations in our dataset. #exports def get_all_weights ( x , frac = 0.4 ): \"\"\"Calculates the weightings at each data point for a LOWESS regression\"\"\" all_weights = [] for i in range ( len ( x )): weights = get_weights ( x , x [ i ], frac = frac ) all_weights += [ weights ] all_weights = np . array ( all_weights ) return all_weights all_weights = get_all_weights ( x , frac = frac ) all_weights [: 5 , : 5 ] array([[1. , 0.99863512, 0.98911574, 0.96358278, 0.91512916], [0.99826489, 1. , 0.99826489, 0.98617531, 0.95385361], [0.98207661, 0.99774775, 1. , 0.99774775, 0.98207661], [0.92116732, 0.97619149, 0.997003 , 1. , 0.997003 ], [0.75907091, 0.89295331, 0.96743815, 0.99589042, 1. ]]) Not too bad at all, we could now use this to weight the fitting of the polynomials in the LOWESS. However, we've carried out most of these operations as part of for loops over vectors, what if we could store our data in matrices and do single operations over them? Thankfully Numpy has lots of helpful functions to aid us in this. We'll start by creating a matrix with the distances, to do this we can reshape the vectors into matrices of shape (25, 1) and (1, 25), then deduct the matrix with only one row from the matrix with only one column. #exports vector_to_dist_matrix = lambda x : np . abs ( x . reshape ( - 1 , 1 ) - x . reshape ( 1 , - 1 )) dist_matrix = vector_to_dist_matrix ( x ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( dist_matrix , cmap = 'cool' , cbar_kws = { 'label' : 'Distance' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') This approach brings an order of magnitude speed-up to the operation %% timeit timeit ( lambda : [ get_dist ( x , x [ x_idx ]) for x_idx in range ( len ( x ))], number = 10000 ) 825 ms \u00b1 163 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit timeit ( lambda : vector_to_dist_matrix ( x ), number = 10000 ) 55.5 ms \u00b1 3.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Now we need to calculate the distance threshold (max distance away from point of interest that is within the data fraction specified). Alexandre Gramfort lays out one approach to determine the distances using a list comprehension. #exports get_frac_idx = lambda x , frac : int ( np . ceil ( len ( x ) * frac )) - 1 gramfort_get_dist_thresholds = lambda x , frac_idx : [ np . sort ( np . abs ( x - x [ i ]))[ frac_idx ] for i in range ( len ( x ))] frac_idx = get_frac_idx ( x , frac ) timeit ( lambda : gramfort_get_dist_thresholds ( x , frac_idx ), number = 10000 ) 1.7785535999999986 Pretty quick, lets see if we could do better though. We'll try keeping the distance matrix intact rather than breaking it up in each iteration. This enables us to do the absolute conversion, sorting and indexing over a matrix rather than looping the operations over vectors. These changes gave us an order of magnitude speed-up. #exports get_dist_thresholds = lambda x , frac_idx , dist_matrix : np . sort ( dist_matrix )[:, frac_idx ] dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) timeit ( lambda : get_dist_thresholds ( x , frac_idx , dist_matrix ), number = 10000 ) 0.10906620000000089 Now we have both the distance matrix and thresholds we can start to calculate the weightings, the first step to this is scale and clip the distances based on their threshold values. inv_linear_weights = np . clip ( dist_matrix / dist_thresholds . reshape ( - 1 , 1 ), 0 , 1 ) inv_linear_weights [: 5 , : 5 ] array([[0. , 0.08333333, 0.16666667, 0.25 , 0.33333333], [0.09090909, 0. , 0.09090909, 0.18181818, 0.27272727], [0.2 , 0.1 , 0. , 0.1 , 0.2 ], [0.33333333, 0.22222222, 0.11111111, 0. , 0.11111111], [0.5 , 0.375 , 0.25 , 0.125 , 0. ]]) We can now calculate the tri-cubic weighting. We repeat this using both base Python and Numpy to see which is faster, surprisingly base Python comes out on top. timeit ( lambda : ( 1 - inv_linear_weights ** 3 ) ** 3 , number = 10000 ) 0.4310475999999994 timeit ( lambda : np . power ( 1 - np . power ( inv_linear_weights , 3 ), 3 ), number = 10000 ) 0.44873660000000015 After a little more exploration it appears that Numpy's power function only offers improvements over base Python when the exponent is very high. For a good discussion on why this is the case you can read more here . timeit ( lambda : ( 1 - inv_linear_weights ** 50000 ) ** 50000 , number = 10000 ) 0.5387637000000005 timeit ( lambda : np . power ( 1 - np . power ( inv_linear_weights , 50000 ), 50000 ), number = 10000 ) 0.526861499999999 We'll now wrap these two steps up into a single function #exports def clean_weights ( weights ): \"\"\"Normalises each models weightings and removes non-finite values\"\"\" with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): weights = weights / weights . sum ( axis = 0 ) # We'll then normalise the weights so that for each model they sum to 1 for a single data point weights = np . where ( ~ np . isfinite ( weights ), 0 , weights ) # And remove any non-finite values return weights def dist_2_weights_matrix ( dist_matrix , dist_thresholds ): \"\"\"Converts distance matrix and thresholds to weightings\"\"\" weights = dist_to_weights ( dist_matrix , dist_thresholds . reshape ( - 1 , 1 )) weights = clean_weights ( weights ) return weights weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) weights [: 5 , : 5 ] array([[0.20861363, 0.18398963, 0.16064453, 0.13720751, 0.11473407], [0.20814378, 0.18430943, 0.16252964, 0.14126726, 0.12082652], [0.20364685, 0.18375705, 0.16289653, 0.14341436, 0.1254292 ], [0.18628223, 0.17830797, 0.16222709, 0.14384546, 0.12796029], [0.13975483, 0.15666172, 0.15537944, 0.14300426, 0.12848832]]) And then combine them with the creation of the distance matrix and threshold #exports def get_full_dataset_weights_matrix ( x , frac = 0.4 ): \"\"\"Wrapper for calculating weights from the raw data and LOWESS fraction\"\"\" frac_idx = get_frac_idx ( x , frac ) dist_matrix = vector_to_dist_matrix ( x ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) return weights weights = get_full_dataset_weights_matrix ( x , frac ) We'll do a quick visual check to see if they look reasonable weights [: 5 , : 5 ] array([[0.20861363, 0.18398963, 0.16064453, 0.13720751, 0.11473407], [0.20814378, 0.18430943, 0.16252964, 0.14126726, 0.12082652], [0.20364685, 0.18375705, 0.16289653, 0.14341436, 0.1254292 ], [0.18628223, 0.17830797, 0.16222709, 0.14384546, 0.12796029], [0.13975483, 0.15666172, 0.15537944, 0.14300426, 0.12848832]]) Looks good, we'll also time it timeit ( lambda : get_full_dataset_weights_matrix ( x , frac = frac ), number = 10000 ) 1.4529582999999988 Currently we have created a weights matrix that scales in size as the square of the dataset length, this could quickly become prohibitively computationally expensive for large datasets. Instead we'll create a new function that lets us either specify a vector of locations ( reg_anchors ) where the regressions will be centered, or alternatively if num_fits is passed that will be the number of local regressions. #exports num_fits_2_reg_anchors = lambda x , num_fits : np . linspace ( x . min (), x . max (), num = num_fits ) def get_weighting_locs ( x , reg_anchors = None , num_fits = None ): \"\"\"Identifies the weighting locations for the provided dataset\"\"\" num_type_2_dist_rows = { type ( None ) : lambda x , num_fits : x . reshape ( - 1 , 1 ), int : lambda x , num_fits : num_fits_2_reg_anchors ( x , num_fits ) . reshape ( - 1 , 1 ), } if reg_anchors is None : weighting_locs = num_type_2_dist_rows [ type ( num_fits )]( x , num_fits ) else : weighting_locs = reg_anchors . reshape ( - 1 , 1 ) return weighting_locs def create_dist_matrix ( x , reg_anchors = None , num_fits = None ): \"\"\"Constructs the distance matrix for the desired weighting locations\"\"\" weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) dist_matrix = np . abs ( weighting_locs - x . reshape ( 1 , - 1 )) return dist_matrix dist_matrix = create_dist_matrix ( x ) dist_matrix . shape (26, 26) When neither reg_anchors nor num_fits are passed it defaults to using all data-points in the matrix which can be seen below dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') However if we specify num_fits as 10 we can see that only 10 regression nodes are defined in the weights matrix num_fits = 10 dist_matrix = create_dist_matrix ( x , num_fits = num_fits ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') But what about if you were only really interested in getting highly localised regressions for a specific part of your variable space? Using the reg_anchors variable we can now specify our own grid over which to carry out the regressions. reg_anchors = np . array ([ 0 , 0.25 , 0.5 , 0.75 , 1 , 1.25 , 1.5 , 1.75 , 2 , 2.5 , 3 , 3.5 , 4 , 4.5 , 5 ]) dist_matrix = create_dist_matrix ( x , reg_anchors = reg_anchors ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') We'll wrap these steps up in a couple of functions and add in some syntactic sugar to allow the user to more flexibily specify the distance matrix kwargs #exports def get_weights_matrix ( x , frac = 0.4 , weighting_locs = None , reg_anchors = None , num_fits = None ): \"\"\"Wrapper for calculating weights from the raw data and LOWESS fraction\"\"\" frac_idx = get_frac_idx ( x , frac ) if weighting_locs is not None : dist_matrix = np . abs ( weighting_locs - x . reshape ( 1 , - 1 )) else : dist_matrix = create_dist_matrix ( x , reg_anchors = reg_anchors , num_fits = num_fits ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) return weights weights = get_weights_matrix ( x , frac = 0.5 , reg_anchors = reg_anchors ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') We'll check this still works when we want to carry out a LOWESS fit over all points weights = get_weights_matrix ( x , frac = frac ) print ( weights . shape ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) (26, 26) Text(69.58333333333334, 0.5, 'Regression Nodes') Regression \u00b6 Now that we've calculated the weightings necessary for local regression we need to create the regression functions. We'll start by calculating the intercept and gradient of a linear regression fit with optional weighting. N.b. This section of the code was heavily inspired by this gist created by Alexandere Gramfort #exports def calc_lin_reg_betas ( x , y , weights = None ): \"\"\"Calculates the intercept and gradient for the specified local regressions\"\"\" if weights is None : weights = np . ones ( len ( x )) b = np . array ([ np . sum ( weights * y ), np . sum ( weights * y * x )]) A = np . array ([[ np . sum ( weights ), np . sum ( weights * x )], [ np . sum ( weights * x ), np . sum ( weights * x * x )]]) betas = np . linalg . lstsq ( A , b , rcond = None )[ 0 ] return betas intercept , gradient = calc_lin_reg_betas ( x , y ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , label = 'Original' ) ax . plot ([ x . min (), x . max ()], [ intercept + gradient * x . min (), intercept + gradient * x . max ()], label = 'Linear Regression' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We'll now repeat the regression calculation but will also specify a weighting for each data-point row_weights = weights [ 14 , :] intercept , gradient = calc_lin_reg_betas ( x , y , row_weights ) x_used = x [ x * row_weights > 0 ] y_used = y [ x * row_weights > 0 ] x_weights = row_weights [ x * row_weights > 0 ] ## Plotting fig , ax = plt . subplots ( dpi = 250 , figsize = ( 7 , 4 )) weighted_points = ax . scatter ( x , y , s = ( row_weights ) * 300 , c = row_weights , edgecolor = 'k' , cmap = 'gray_r' , label = 'Points Used' , zorder = 3 , vmin = 0 ) cbar = fig . colorbar ( weighted_points , label = 'Point Weighting' ) ax . plot ( x , y , label = 'Original' ) ax . plot ([ x . min (), x . max ()], [ intercept + gradient * x . min (), intercept + gradient * x . max ()], label = 'Linear Regression' ) ax . set_ylim ( - 1.2 , 1.2 ) eda . hide_spines ( ax ) leg = ax . legend ( frameon = False ) fig . savefig ( '../img/LOWESS_single_regression_example.png' , dpi = 250 ) We can repeat this for all data-points, the error being minimized across these regressions is shown in the equation below \\[ \\begin{equation} \\label{eqn:lowess_err} n^{-1} \\sum_{i=1}^{n} W_{k i}(x)\\left(y_{i}-\\sum_{j=0}^{p} \\beta_{j} x^{j}\\right)^{2} \\end{equation} \\] n = len ( x ) y_pred = np . zeros ( n ) for i in range ( n ): row_weights = weights [ i , :] betas = calc_lin_reg_betas ( x , y , weights [ i , :]) y_pred [ i ] = betas [ 0 ] + betas [ 1 ] * x [ i ] plt . plot ( x , y , label = 'Original' ) plt . plot ( x , y_pred , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a23310> Whilst this fit doesn't look great remember we can reduce the fraction of data used in each fit to get a more localised regression, in this example we'll also make use of the num_fits parameter to reduce the number of computations that are run. num_fits = 10 weights = get_weights_matrix ( x , frac = 0.2 , num_fits = num_fits ) x_pred = num_fits_2_reg_anchors ( x , num_fits ) . reshape ( - 1 , 1 ) y_pred = np . zeros ( len ( x_pred )) for i in range ( len ( x_pred )): row_weights = weights [ i , :] betas = calc_lin_reg_betas ( x , y , row_weights ) y_pred [ i ] = betas [ 0 ] + betas [ 1 ] * x_pred [ i ] plt . plot ( x , y , label = 'Original' ) plt . plot ( x_pred , y_pred , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a85760> Rather than carrying out the regression fitting and prediction together we'll seperate them to add some flexibility, for example we wouldnt be able to make predictions with a specified number of polynomial fits using the code we just wrote. For this fitting function we'll introduce a design matrix which will hold the coefficients for all of our regressions. We'll also make the function used for regression a parameter, this will allow us to replace it with other regression functions (e.g. polynomials) later on. #exports check_array = lambda array , x : np . ones ( len ( x )) if array is None else array def fit_regressions ( x , y , weights = None , reg_func = calc_lin_reg_betas , num_coef = 2 , ** reg_params ): \"\"\"Calculates the design matrix for the specified local regressions\"\"\" if weights is None : weights = np . ones ( len ( x )) n = weights . shape [ 0 ] y_pred = np . zeros ( n ) design_matrix = np . zeros (( n , num_coef )) for i in range ( n ): design_matrix [ i , :] = reg_func ( x , y , weights = weights [ i , :], ** reg_params ) return design_matrix weights = get_weights_matrix ( x , frac = 0.4 , num_fits = 10 ) design_matrix = fit_regressions ( x , y , weights ) design_matrix array([[ 0.05905438, 0.73999059], [ 0.08020775, 0.6991559 ], [ 0.35324949, 0.41467757], [ 1.09720376, -0.10762239], [ 2.03581046, -0.58348682], [ 2.77888971, -0.88141726], [ 2.91594092, -0.92733431], [ 2.05244287, -0.68647646], [ 0.84017439, -0.38519789], [ 0.6033549 , -0.33232683]]) We can use this design matrix to create predictions for every local regression, then combine them based on their weightings #exports def lowess_fit_and_predict ( x , y , frac = 0.4 , reg_anchors = None , num_fits = None , x_pred = None ): \"\"\"Fits and predicts smoothed local regressions at the specified locations\"\"\" weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) weights = get_weights_matrix ( x , frac = frac , weighting_locs = weighting_locs ) design_matrix = fit_regressions ( x , y , weights ) if x_pred is None : x_pred = x point_evals = design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = frac , reg_anchors = weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) return y_pred x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) y_pred = lowess_fit_and_predict ( x , y ) plt . plot ( x , y , label = 'Original' ) plt . plot ( x , y_pred , '--' , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3aefd30> If we pass an array to the x_pred parameter then those values will be used as the locations of the output predictions x_pred = np . array ([ 0 , 1 , 1.5 , 2 , 3 , 4 , 5 ]) y_pred = lowess_fit_and_predict ( x , y , num_fits = 10 , x_pred = x_pred ) plt . plot ( x , y , label = 'Original' ) plt . plot ( x_pred , y_pred , '--' , label = 'LOESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3b48e80> Lets do some time tests, starting with a small dataset x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) %% timeit y_pred = lowess_fit_and_predict ( x , y ) 2.02 ms \u00b1 57.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Even with larger datasets it remains performant x = np . linspace ( 0 , 5 , num = 3000 ) y = np . sin ( x ) frac = 0.1 x_pred = np . linspace ( 0 , 5 , 100 )[ 1 :] # to avoid divide by zero in MAPE calc %% timeit y_pred = lowess_fit_and_predict ( x , y , frac = frac , x_pred = x_pred ) 989 ms \u00b1 21.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) When making predictions with large datasets and low fractions we can achieve low error for the simple sin curve fitting example y_pred = lowess_fit_and_predict ( x , y , frac = frac , x_pred = x_pred ) y_true = np . sin ( x_pred ) print ( f 'MAPE: { round ( 100 * np . abs (( y_true - y_pred ) / y_true ) . mean (), 3 ) } %' ) MAPE: 0.084% LOWESS on Real-World Data \u00b6 We'll now evaluate the LOWESS fit on some real data, we'll start by loading the electric insights dataset %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.86 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll start by extracting the X and y data y_col = 'day_ahead_price' X_cols = [ 'demand' ] yX = df_EI [ '2020' :][[ y_col ] + X_cols ] . dropna () . values y = yX [:, 0 ] X = yX [:, 1 :] print ( y . shape , X . shape ) (17435,) (17435, 1) We'll then fit the model and make the prediction %% time x = X [:, 0 ] x_pred = np . linspace ( 15 , 55 , 41 ) y_pred = lowess_fit_and_predict ( x , y , frac = 0.4 , num_fits = 25 , x_pred = x_pred ) Wall time: 63 ms And now we can plot the results! fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , c = 'r' ) ax . scatter ( x , y , s = 0.1 , c = 'k' , alpha = 0.25 ) ax . set_ylim ( 0 , 100 ) ax . set_xlim ( 15 , 55 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) Text(0, 0.5, 'Day-Ahead Price (\u00a3/MWh)') Robust Regression \u00b6 What we've done so far is ultimately nothing more than an extension of linear regression, which has been achieved by fitting multiple regressions and manipulating the weighting of the data points used in each one. x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) x_pred = np . linspace ( 0 , 5 , 100 ) y_pred = lowess_fit_and_predict ( x , y , num_fits = 5 , x_pred = x_pred ) plt . plot ( x , y ) plt . plot ( x_pred , y_pred , '--' ) [<matplotlib.lines.Line2D at 0x1a4b2f9e160>] But we can also do much more with the weights, including adjusting them to make our regression more robust against outliers. First we'll create a new dataset which has some noise. x = np . linspace ( 0 , 5 , num = 150 ) weights = get_weights_matrix ( x , frac = 0.2 ) y = np . sin ( x ) y_noisy = y + ( np . random . normal ( size = len ( y ))) / 10 y_noisy [ 25 ] = 2 y_noisy [ 50 ] = - 2 y_noisy [ 75 ] = 2 plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b39dc310> We can use our lowess_fit_and_predict function to make a lowess model of this data y_pred = lowess_fit_and_predict ( x , y_noisy , frac = 0.2 , num_fits = 25 ) plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x , y_pred , '--' , label = 'LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a630d0> The issue though is that our model is being highly skewed by the outliers present in the data, robust regression provides a method to handle this (some improvements can also be made by increasing the frac value). To carry it out we need to repeat the lowess fit, but when we do we must further weight the data-points to minimize the influence of outliers. Robust regression is an iterative procedure that seeks to identify outliers and minimize their impact on the coefficient estimates - NCSS Cleveland, W. S. (1979) in his paper 'Robust Locally Weighted Regression and Smoothing Scatterplots' outlines a procedure for carrying out robust lowess regression that we will use here. We'll start by calculating the standard deviation of the residuals. residuals = y_noisy - y_pred std_dev = np . quantile ( np . abs ( residuals ), 0.682 ) std_dev 0.13459575203567298 We can then define a threshold, say 6 standard deviations, and clip any values outside of that. We're left with a cleaned version of the residuals. max_std_dev = 6 cleaned_residuals = np . clip ( residuals / ( max_std_dev * std_dev ), - 1 , 1 ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 10 , 4 )) ax = axs [ 0 ] sns . histplot ( residuals , ax = ax ) ax . set_title ( 'Residuals' ) ax = axs [ 1 ] sns . histplot ( cleaned_residuals , ax = ax ) ax . set_xlim ( - 1 , 1 ) ax . set_title ( 'Cleaned Residuals' ) for ax in axs : eda . hide_spines ( ax ) In the last step we clipped all of our values from -1 to +1, that means if we square our values and deduct them from 1 any outliers will go to 0. The returned vector we'll call the robust_weights . robust_weights = ( 1 - cleaned_residuals ** 2 ) ** 2 # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . histplot ( robust_weights , ax = ax ) ax . set_xlim ( 0 , 1 ) eda . hide_spines ( ax ) Before we move on we'll combine these into a single step for calculating the robust_weights #exports def calc_robust_weights ( y , y_pred , max_std_dev = 6 ): \"\"\"Calculates robustifying weightings that penalise outliers\"\"\" residuals = y - y_pred std_dev = np . quantile ( np . abs ( residuals ), 0.682 ) cleaned_residuals = np . clip ( residuals / ( max_std_dev * std_dev ), - 1 , 1 ) robust_weights = ( 1 - cleaned_residuals ** 2 ) ** 2 return robust_weights We'll now refactor our previous lowess_fit_and_predict but this time will enable it to carry out robust regressions #exports def robust_lowess_fit_and_predict ( x , y , frac = 0.4 , reg_anchors = None , num_fits = None , x_pred = None , robust_weights = None , robust_iters = 3 ): \"\"\"Fits and predicts robust smoothed local regressions at the specified locations\"\"\" # Identifying the initial loading weights weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) loading_weights = get_weights_matrix ( x , frac = frac , weighting_locs = weighting_locs ) # Robustifying the weights (to reduce outlier influence) if robust_weights is None : robust_loading_weights = loading_weights else : robust_loading_weights = np . multiply ( robust_weights , loading_weights ) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): robust_loading_weights = robust_loading_weights / robust_loading_weights . sum ( axis = 0 ) robust_loading_weights = np . where ( ~ np . isfinite ( robust_loading_weights ), 0 , robust_loading_weights ) # Fitting the model and making predictions design_matrix = fit_regressions ( x , y , robust_loading_weights ) if x_pred is None : x_pred = x point_evals = design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = frac , reg_anchors = weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) # Recursive robust regression robust_weights = calc_robust_weights ( y , y_pred ) if robust_iters > 1 : robust_iters -= 1 y_pred = robust_lowess_fit_and_predict ( x , y , frac = frac , reg_anchors = reg_anchors , num_fits = num_fits , x_pred = x_pred , robust_weights = robust_weights , robust_iters = robust_iters ) return y_pred y_pred = robust_lowess_fit_and_predict ( x , y_noisy , frac = 0.2 , num_fits = 25 ) plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x , y_pred , '--' , label = 'Robust LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b329c7f0> We've got everything working nicely but the current way we make predictions doesn't make it easy to plug-and-play with other Python libraries, to reduce this friction we'll refactor the code again and create a Scikit-Learn wrapper for our process. #exports class Lowess ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a Scikit-Learn compatible model for Locally Weighted Scatterplot Smoothing, including robustifying procedures against outliers. For more information on the underlying algorithm please refer to * William S. Cleveland: \"Robust locally weighted regression and smoothing scatterplots\", Journal of the American Statistical Association, December 1979, volume 74, number 368, pp. 829-836. * William S. Cleveland and Susan J. Devlin: \"Locally weighted regression: An approach to regression analysis by local fitting\", Journal of the American Statistical Association, September 1988, volume 83, number 403, pp. 596-610. Example Usage: ``` x = np.linspace(0, 5, num=150) y = np.sin(x) y_noisy = y + (np.random.normal(size=len(y)))/10 lowess = Lowess() lowess.fit(x, y_noisy, frac=0.2) x_pred = np.linspace(0, 5, 26) y_pred = lowess.predict(x_pred) ``` Initialisation Parameters: reg_func: function that accepts the x and y values then returns the intercepts and gradients Attributes: reg_func: function that accepts the x and y values then returns the intercepts and gradients fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression weighting_locs: Locations of the local regression centers loading_weights: Weights of each data-point across the localalised models design_matrix: Regression coefficients for each of the localised models \"\"\" def __init__ ( self , reg_func = calc_lin_reg_betas ): self . reg_func = reg_func self . fitted = False return def calculate_loading_weights ( self , x , reg_anchors = None , num_fits = None , external_weights = None , robust_weights = None ): \"\"\" Calculates the loading weights for each data-point across the localised models Parameters: x: values for the independent variable reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers \"\"\" # Calculating the initial loading weights weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) loading_weights = get_weights_matrix ( x , frac = self . frac , weighting_locs = weighting_locs ) # Applying weight adjustments if external_weights is None : external_weights = np . ones ( x . shape [ 0 ]) if robust_weights is None : robust_weights = np . ones ( x . shape [ 0 ]) weight_adj = np . multiply ( external_weights , robust_weights ) loading_weights = np . multiply ( weight_adj , loading_weights ) # Post-processing weights with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): loading_weights = loading_weights / loading_weights . sum ( axis = 0 ) # normalising loading_weights = np . where ( ~ np . isfinite ( loading_weights ), 0 , loading_weights ) # removing non-finite values self . weighting_locs = weighting_locs self . loading_weights = loading_weights return def fit ( self , x , y , frac = 0.4 , reg_anchors = None , num_fits = None , external_weights = None , robust_weights = None , robust_iters = 3 , ** reg_params ): \"\"\" Calculation of the local regression coefficients for a LOWESS model across the dataset provided. This method will reassign the `frac`, `weighting_locs`, `loading_weights`, and `design_matrix` attributes of the `Lowess` object. Parameters: x: values for the independent variable y: values for the dependent variable frac: LOWESS bandwidth for local regression as a fraction reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" self . frac = frac # Solving for the design matrix self . calculate_loading_weights ( x , reg_anchors = reg_anchors , num_fits = num_fits , external_weights = external_weights , robust_weights = robust_weights ) self . design_matrix = fit_regressions ( x , y , weights = self . loading_weights , reg_func = self . reg_func , ** reg_params ) # Recursive robust regression if robust_iters > 1 : y_pred = self . predict ( x ) robust_weights = calc_robust_weights ( y , y_pred ) robust_iters -= 1 y_pred = self . fit ( x , y , frac = self . frac , reg_anchors = reg_anchors , num_fits = num_fits , external_weights = external_weights , robust_weights = robust_weights , robust_iters = robust_iters , ** reg_params ) return y_pred self . fitted = True return def predict ( self , x_pred ): \"\"\" Inference using the design matrix from the LOWESS fit Parameters: x_pred: Locations for the LOWESS inference Returns: y_pred: Estimated values using the LOWESS fit \"\"\" point_evals = self . design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), self . design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = self . frac , reg_anchors = self . weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) return y_pred lowess = Lowess () lowess . fit ( x , y_noisy , frac = 0.2 ) x_pred = np . linspace ( 0 , 5 , 26 ) y_pred = lowess . predict ( x_pred ) # Plotting plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x_pred , y_pred , '--' , label = 'Robust LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b45ad850> Confidence Intervals \u00b6 Create the ensemble_results then take the specified confidence range Put into an sklearn wrapper We're now able to take noisy data and find a robust estimate for the average value at any point, but what about uncertainty? In this work we'll look at two types of uncertainty quantification: confidence & prediction intervals. In this section we will discuss confidence intervals, summarised well in this statement: The selection of a confidence level for an interval determines the probability that the confidence interval produced will contain the true parameter value - stat.yale.edu We'll start by creating a slightly noiser and longer sin curve than used previously. x = np . linspace ( 0 , 10 , num = 500 ) y = np . sin ( x ) heteroskedasticity_factor = ( 1 + 2 * np . array ( range ( len ( x ))) / len ( x )) / 10 y_noisy = y + heteroskedasticity_factor * ( np . random . normal ( size = len ( y ))) plt . plot ( x , y , label = 'Original' , linewidth = 2 , zorder = 2 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b78d3ac0> To determine the confidence in our estimate we want to know how the model will perform against previously unseen data-points. We can randomly separate our data, calculate the error, then repeat many times to obtain a set of possible errors along the curve - this is a specific use-case of the more general bootstrapping approach. To generate bootstrapped statistics we first need to split our data set, we'll begin by creating a function that returns the indexes for our in- and out-of-bag samples. #exports def get_bootstrap_idxs ( x , bootstrap_bag_size = 0.5 ): \"\"\"Determines the indexes of an array to be used for the in- and out-of-bag bootstrap samples\"\"\" # Bag size handling assert bootstrap_bag_size > 0 , 'Bootstrap bag size must be greater than 0' if bootstrap_bag_size > 1 : assert int ( bootstrap_bag_size ) == bootstrap_bag_size , 'If the bootstrab bag size is not provided as a fraction then it must be an integer' else : bootstrap_bag_size = int ( np . ceil ( bootstrap_bag_size * len ( x ))) # Splitting in-bag and out-of-bag samlpes idxs = np . array ( range ( len ( x ))) ib_idxs = np . sort ( np . random . choice ( idxs , bootstrap_bag_size , replace = True )) oob_idxs = np . setdiff1d ( idxs , ib_idxs ) return ib_idxs , oob_idxs ib_idxs , oob_idxs = get_bootstrap_idxs ( x ) print ( f 'in-bag: { len ( ib_idxs ) } , out-of-bag: { len ( oob_idxs ) } ' ) in-bag: 250, out-of-bag: 299 We'll now calculate the standard deviation of the in- and out-of-bag errors #exports def get_bootstrap_resid_std_devs ( x , y , bag_size , model = Lowess (), ** model_kwargs ): \"\"\"Calculates the standard deviation of the in- and out-of-bag errors\"\"\" # Splitting the in- and out-of-bag samples ib_idxs , oob_idxs = get_bootstrap_idxs ( x , bag_size ) x_ib , x_oob = x [ ib_idxs ], x [ oob_idxs ] y_ib , y_oob = y [ ib_idxs ], y [ oob_idxs ] # Fitting and predicting with the model model . fit ( x_ib , y_ib , ** model_kwargs ) y_pred = model . predict ( x ) y_ib_pred = model . predict ( x_ib ) y_oob_pred = model . predict ( x_oob ) # Calculating the error y_ib_resids = y_ib - y_ib_pred ib_resid_std_dev = np . std ( np . abs ( y_ib_resids )) y_oob_resids = y_oob - y_oob_pred oob_resid_std_dev = np . std ( np . abs ( y_oob_resids )) return ib_resid_std_dev , oob_resid_std_dev get_bootstrap_resid_std_devs ( x , y , bag_size = 0.5 , frac = 0.2 , num_fits = 20 ) (0.020320708955639148, 0.024223597689702395) We'll quickly plot the distributions of the errors for each set bag_size = 0.5 num_runs = 1000 ib_resid_std_devs = [] oob_resid_std_devs = [] for model_run in track ( range ( num_runs )): ib_resid_std_dev , oob_resid_std_dev = get_bootstrap_resid_std_devs ( x , y_noisy , bag_size , frac = 0.2 , num_fits = 20 ) ib_resid_std_devs += [ ib_resid_std_dev ] oob_resid_std_devs += [ oob_resid_std_dev ] # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . histplot ( ib_resid_std_devs , ax = ax , alpha = 0.5 , label = 'In-Bag' ) sns . histplot ( oob_resid_std_devs , ax = ax , alpha = 0.5 , color = 'C1' , label = 'Out-of-Bag' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Residual \\' s Standard Deviation' ) 100% 1000/1000 [00:28 < 00:00, 0.03s/it] Text(0.5, 0, \"Residual's Standard Deviation\") We'll now create two wrapper functions, one for running models, the other for bootstrapping them. The returned df_bootstrap includes the predictions for each model run. N.b. the bootstrap_model is a generalisable function that will work with any Scikit-Learn compatible model. #exports def run_model ( x , y , bag_size , model = Lowess (), x_pred = None , ** model_kwargs ): \"\"\"Fits a model and then uses it to make a prediction\"\"\" if x_pred is None : x_pred = x # Splitting the in- and out-of-bag samples ib_idxs , oob_idxs = get_bootstrap_idxs ( x , bag_size ) x_ib , y_ib = x [ ib_idxs ], y [ ib_idxs ] # Fitting and predicting the model model . fit ( x_ib , y_ib , ** model_kwargs ) y_pred = model . predict ( x_pred ) return y_pred def bootstrap_model ( x , y , bag_size = 0.5 , model = Lowess (), x_pred = None , num_runs = 1000 , ** model_kwargs ): \"\"\"Repeatedly fits and predicts using the specified model, using different subsets of the data each time\"\"\" # Creating the ensemble predictions preds = [] for bootstrap_run in track ( range ( num_runs )): y_pred = run_model ( x , y , bag_size , model = model , x_pred = x_pred , ** model_kwargs ) preds += [ y_pred ] # Wrangling into a dataframe df_bootstrap = pd . DataFrame ( preds , columns = x ) . T df_bootstrap . index . name = 'x' df_bootstrap . columns . name = 'bootstrap_run' return df_bootstrap df_bootstrap = bootstrap_model ( x , y_noisy , num_runs = 1000 , frac = 0.2 , num_fits = 20 ) df_bootstrap . head () 100% 1000/1000 [00:25 < 00:00, 0.03s/it] x 0 1 2 3 4 5 6 7 8 9 ... 990 991 992 993 994 995 996 997 998 999 0 0.12324 0.100568 0.034607 0.062935 0.037283 0.024595 0.077896 0.071517 0.154899 0.104609 ... 0.106006 0.022008 0.117897 0.042924 0.094948 0.095224 0.109828 0.097625 0.096894 0.102314 0.02004 0.135523 0.114414 0.04894 0.077258 0.051836 0.039222 0.091694 0.086137 0.166246 0.116779 ... 0.119922 0.037429 0.131267 0.057019 0.109155 0.107799 0.12283 0.111439 0.109543 0.11558 0.04008 0.147791 0.128252 0.063258 0.091566 0.06638 0.053843 0.105484 0.100748 0.177581 0.128936 ... 0.133827 0.052831 0.144617 0.071105 0.12335 0.120364 0.135816 0.125241 0.122177 0.128832 0.06012 0.160044 0.142084 0.077562 0.10586 0.080914 0.068458 0.119268 0.115353 0.188904 0.141083 ... 0.147722 0.068217 0.157946 0.085199 0.137534 0.132918 0.148786 0.13903 0.134797 0.14207 0.08016 0.172286 0.155927 0.091852 0.120165 0.09544 0.083068 0.133063 0.129976 0.200216 0.153222 ... 0.161608 0.083587 0.171256 0.099382 0.151709 0.14548 0.161742 0.15281 0.147405 0.1553 Using df_bootstrap we can calculate the confidence interval of our predictions, the Pandas DataFrame quantile method makes this particularly simple. #exports def get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ): \"\"\"Estimates the confidence interval of a prediction based on the bootstrapped estimates\"\"\" conf_margin = ( 1 - conf_pct ) / 2 df_conf_intvl = pd . DataFrame ( columns = [ 'min' , 'max' ], index = df_bootstrap . index ) df_conf_intvl [ 'min' ] = df_bootstrap . quantile ( conf_margin , axis = 1 ) df_conf_intvl [ 'max' ] = df_bootstrap . quantile ( 1 - conf_margin , axis = 1 ) df_conf_intvl = df_conf_intvl . sort_index () return df_conf_intvl df_conf_intvl = get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , 'k--' , label = 'Original' , linewidth = 1 , zorder = 2 ) ax . fill_between ( df_conf_intvl . index , df_conf_intvl [ 'min' ], df_conf_intvl [ 'max' ], color = 'r' , edgecolor = 'k' , alpha = 0.25 , label = '95% Confidence' ) ax . scatter ( x , y_noisy , label = 'With Noise' , color = 'w' , edgecolor = 'k' , linewidth = 0.3 , s = 2.5 , zorder = 1 ) ax . legend ( frameon = False ) ax . set_xlim ( 0 , 10 ) eda . hide_spines ( ax ) Quantile Predictions \u00b6 Earlier when creating our Lowess class we enabled the function used in calculating the design matrix betas to be specified at initialisation. We can now use this to pass a custom function that will calculate the design matrix for a local quantile regression. #exports def pred_to_quantile_loss ( y , y_pred , q = 0.5 , weights = None ): \"\"\"Calculates the quantile error for a prediction\"\"\" residuals = y - y_pred if weights is not None : residuals = weights * residuals loss = np . array ([ q * residuals , ( q - 1 ) * residuals ]) . max ( axis = 0 ) . mean () return loss def calc_quant_reg_loss ( x0 , x , y , q , weights = None ): \"\"\"Makes a quantile prediction then calculates its error\"\"\" if weights is None : weights = np . ones ( len ( x )) quantile_pred = x0 [ 0 ] + x0 [ 1 ] * x loss = pred_to_quantile_loss ( y , quantile_pred , q , weights ) return loss calc_quant_reg_betas = lambda x , y , q = 0.5 , x0 = np . zeros ( 2 ), weights = None , method = 'nelder-mead' : minimize ( calc_quant_reg_loss , x0 , method = method , args = ( x , y , q , weights )) . x We'll then create a wrapper that will fit the model for several specified quantiles. N.b. this function should generalise to any Scikit-Learn compatible model that uses q as the kwarg for the quantile. #exports def quantile_model ( x , y , model = Lowess ( calc_quant_reg_betas ), x_pred = None , qs = np . linspace ( 0.1 , 0.9 , 9 ), ** model_kwargs ): \"\"\"Model wrapper that will repeatedly fit and predict for the specified quantiles\"\"\" if x_pred is None : x_pred = np . sort ( np . unique ( x )) q_to_preds = dict () for q in track ( qs ): model . fit ( x , y , q = q , ** model_kwargs ) q_to_preds [ q ] = model . predict ( x_pred ) df_quantiles = pd . DataFrame ( q_to_preds , index = x_pred ) df_quantiles . index . name = 'x' df_quantiles . columns . name = 'quantiles' return df_quantiles %% time df_quantiles = quantile_model ( x , y_noisy , frac = 0.2 , num_fits = 100 , robust_iters = 1 ) df_quantiles . head () 100% 9/9 [00:11 < 00:01, 1.18s/it] Wall time: 10.6 s x 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 -0.071164 -0.004352 0.011374 0.025166 0.080732 0.091657 0.144884 0.163482 0.192227 0.02004 -0.056877 0.009553 0.025934 0.040718 0.095134 0.106431 0.158797 0.178013 0.207308 0.04008 -0.042584 0.023455 0.040493 0.056241 0.109531 0.121194 0.172707 0.192542 0.222345 0.06012 -0.028267 0.037368 0.055069 0.07175 0.123931 0.135959 0.186627 0.207077 0.237343 0.08016 -0.013915 0.0513 0.069668 0.087256 0.138339 0.150735 0.200564 0.221624 0.252307 We can visualise the range of our predictions fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , 'k--' , label = 'Original' , linewidth = 1 , zorder = 2 ) ax . scatter ( x , y_noisy , label = 'With Noise' , color = 'w' , edgecolor = 'k' , linewidth = 0.3 , s = 2.5 , zorder = 1 ) ax . fill_between ( df_quantiles . index , df_quantiles [ 0.1 ], df_quantiles [ 0.9 ], color = 'r' , edgecolor = 'k' , alpha = 0.25 , label = '10-90% Prediction Interval' ) ax . legend ( frameon = False , loc = 3 ) ax . set_xlim ( 0 , 10 ) eda . hide_spines ( ax ) External Weights \u00b6 When we made our Lowess class we included the option to specify external_weights , the reason for this is that it allows us to carry out further model smoothing using variables outside of the regression. This makes particular sense for variables such as time. Lets first plot two subsets of the data to see why we need to do this in the first place. df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-09' : '2011-03' ], s_price [ '2010-09' : '2011-03' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Lets start by adding some boolean filters that we'll then cast as integers to act as weights, for one we'll choose an early winter period from the dataset, for the other we'll take the latest summer model_to_dt_weights = { 'Winter 10-11' : (( s_dispatchable . index < '2011-03' ) & ( s_dispatchable . index > '2010-09' )) . astype ( int ), 'Summer 20' : (( s_dispatchable . index < '2020-09' ) & ( s_dispatchable . index > '2020-03' )) . astype ( int ) } df_preds = pd . DataFrame () for model_name , dt_weights in model_to_dt_weights . items (): lowess = Lowess () lowess . fit ( s_dispatchable . values , s_price . values , frac = 0.3 , num_fits = 31 , external_weights = dt_weights ) x_pred = np . linspace ( 8 , 60 , 53 ) y_pred = lowess . predict ( x_pred ) df_preds [ model_name ] = pd . Series ( y_pred , index = x_pred ) We'll plot our estimates alongside the true values fig , ax = plt . subplots ( dpi = 250 ) for i , ( model_name , dt_weights ) in enumerate ( model_to_dt_weights . items ()): color = f 'C { i } ' ax . scatter ( s_dispatchable [ dt_weights . astype ( bool )], s_price [ dt_weights . astype ( bool )], color = color , s = 0.5 , label = model_name ) min_ , max_ = s_dispatchable [ dt_weights . astype ( bool )] . quantile ([ 0.001 , 0.99 ]) df_preds . loc [ df_preds . index > min_ , model_name ] . plot ( ax = ax , color = 'k' , linewidth = 2 , label = '_nolegend_' ) df_preds . loc [ df_preds . index > min_ , model_name ] . plot ( ax = ax , color = color , linestyle = '--' , label = '_nolegend_' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Instead of just using a boolean value to indicate whether an observation belongs to a specific date period, we could instead assign weightings based on the distance from specific dates. This has the benefit that we can reuse existing functions that we wrote earlier. #exports def calc_timedelta_dists ( dates , central_date , threshold_value = 24 , threshold_units = 'W' ): \"\"\"Maps datetimes to weights using the central date and threshold information provided\"\"\" timedeltas = pd . to_datetime ( dates , utc = True ) - pd . to_datetime ( central_date , utc = True ) timedelta_dists = timedeltas / pd . Timedelta ( value = threshold_value , unit = threshold_units ) return timedelta_dists central_date = '2017-01-01' timedelta_dists = calc_timedelta_dists ( df_EI . index , central_date ) weights = dist_to_weights ( timedelta_dists . values ) plt . plot ( df_EI . index , weights ) [<matplotlib.lines.Line2D at 0x1a4b3acd190>] We'll create a wrapper that does this for all of the dates at which we wish to create a localised Lowess model #exports def construct_dt_weights ( dt_idx , reg_dates , threshold_value = 52 , threshold_units = 'W' ): \"\"\"Constructs a set of distance weightings based on the regression dates provided\"\"\" dt_to_weights = dict () for reg_date in reg_dates : dt_to_weights [ reg_date ] = pd . Series ( calc_timedelta_dists ( dt_idx , reg_date , threshold_value = threshold_value , threshold_units = threshold_units )) . pipe ( dist_to_weights ) . values return dt_to_weights %% time reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = 'Ys' ) dt_to_weights = construct_dt_weights ( df_EI_model . index , reg_dates ) sns . heatmap ( pd . DataFrame ( dt_to_weights , index = df_EI_model . index )) Wall time: 2.86 s <AxesSubplot:ylabel='local_datetime'> We'll create two wrapper functions for fitting the models and estimating using them as an ensemble. We'll also create a function that sanitises the inputs to the SmoothDates fitting method. #exports def fit_external_weighted_ensemble ( x , y , ensemble_member_to_weights , lowess_kwargs = {}, ** fit_kwargs ): \"\"\"Fits an ensemble of LOWESS models which have varying relevance for each subset of data over time\"\"\" ensemble_member_to_models = dict () for ensemble_member , ensemble_weights in track ( ensemble_member_to_weights . items ()): ensemble_member_to_models [ ensemble_member ] = Lowess ( ** lowess_kwargs ) ensemble_member_to_models [ ensemble_member ] . fit ( x , y , external_weights = ensemble_weights , ** fit_kwargs ) return ensemble_member_to_models def get_ensemble_preds ( ensemble_member_to_model , x_pred = np . linspace ( 8 , 60 , 53 )): \"\"\"Using the fitted ensemble of LOWESS models to generate the predictions for each of them\"\"\" ensemble_member_to_preds = dict () for ensemble_member in ensemble_member_to_model . keys (): ensemble_member_to_preds [ ensemble_member ] = ensemble_member_to_model [ ensemble_member ] . predict ( x_pred ) return ensemble_member_to_preds def process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ): \"\"\"Sanitises the inputs to the SmoothDates fitting method\"\"\" if hasattr ( x , 'index' ) and hasattr ( y , 'index' ): assert x . index . equals ( y . index ), 'If `x` and `y` have indexes then they must be the same' if dt_idx is None : dt_idx = x . index x = x . values y = y . values assert dt_idx is not None , '`dt_idx` must either be passed directly or `x` and `y` must include indexes' if reg_dates is None : reg_dates = dt_idx return x , y , dt_idx , reg_dates We now have everything we need to create our SmoothDates class that will enable us to create estimates of the surface fit of a LOWESS model over time #exports class SmoothDates ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a time-adaptive extension of the classical Locally Weighted Scatterplot Smoothing regression technique, including robustifying procedures against outliers. This model predicts the surface rather than individual point estimates. Initialisation Parameters: frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function Attributes: fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function ensemble_member_to_weights: Mapping from the regression dates to their respective weightings for each data-point ensemble_member_to_models: Mapping from the regression dates to their localised models reg_dates: Dates at which the local time-adaptive models will be centered around pred_weights: Weightings to map from the local models to the values to be inferenced pred_values: Raw prediction values as generated by each of the individual local models \"\"\" def __init__ ( self , frac = 0.3 , threshold_value = 52 , threshold_units = 'W' ): self . fitted = False self . frac = frac self . threshold_value = threshold_value self . threshold_units = threshold_units def fit ( self , x , y , dt_idx = None , reg_dates = None , lowess_kwargs = {}, ** fit_kwargs ): \"\"\" Calculation of the local regression coefficients for each of the LOWESS models across the dataset provided. This is a time-adaptive ensembled version of the `Lowess` model. Parameters: x: Values for the independent variable y: Values for the dependent variable dt_idx: Datetime index, if not provided the index of the x and y series will be used reg_dates: Dates at which the local time-adaptive models will be centered around lowess_kwargs: Additional arguments to be passed at model initialisation reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" x , y , dt_idx , reg_dates = process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ) self . ensemble_member_to_weights = construct_dt_weights ( dt_idx , reg_dates , threshold_value = self . threshold_value , threshold_units = self . threshold_units ) self . ensemble_member_to_models = fit_external_weighted_ensemble ( x , y , self . ensemble_member_to_weights , lowess_kwargs = lowess_kwargs , frac = self . frac , ** fit_kwargs ) self . reg_dates = reg_dates self . fitted = True return def predict ( self , x_pred = np . linspace ( 8 , 60 , 53 ), dt_pred = None , return_df = True ): \"\"\" Inference using the design matrix from the time-adaptive LOWESS fits Parameters: x_pred: Independent variable locations for the time-adaptive LOWESS inference dt_pred: Date locations for the time-adaptive LOWESS inference return_df: Flag specifying whether to return a dataframe or numpy matrix Returns: df_pred/y_pred: Estimated surface of the time-adaptive the LOWESS fit \"\"\" if dt_pred is None : dt_pred = self . reg_dates if isinstance ( x_pred , pd . Series ): x_pred = x_pred . values self . ensemble_member_to_preds = get_ensemble_preds ( self . ensemble_member_to_models , x_pred = x_pred ) self . pred_weights = np . array ( list ( construct_dt_weights ( dt_pred , self . reg_dates ) . values ())) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): self . pred_weights = self . pred_weights / self . pred_weights . sum ( axis = 0 ) self . pred_values = np . array ( list ( self . ensemble_member_to_preds . values ())) y_pred = np . dot ( self . pred_weights . T , self . pred_values ) if return_df == True : df_pred = pd . DataFrame ( y_pred , index = dt_pred , columns = x_pred ) . T return df_pred else : return y_pred %% time # Fitting reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) # Prediction x_pred = np . linspace ( 8 , 60 , 53 ) df_pred = smooth_dates . predict ( x_pred = x_pred ) df_pred . head () Wall time: 206 ms Unnamed: 0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 ... 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0 60.0 2009-01-04 -25.1979 -19.6978 -14.1985 -8.70185 -3.20799 2.27958 7.7668 13.2571 18.7712 24.3397 ... 191.52 199.994 208.405 216.829 225.204 233.6 241.959 250.322 258.639 267.002 2009-04-05 -35.8976 -28.4311 -20.9662 -13.5091 -6.05717 1.38257 8.82444 16.2744 23.7693 31.3579 ... 241.977 251.959 261.86 271.777 281.632 291.514 301.353 311.198 320.989 330.837 2009-07-05 -36.7886 -28.2248 -19.6637 -11.1177 -2.58043 5.93604 14.4576 22.9934 31.6014 40.3522 ... 267.246 277.279 287.219 297.171 307.058 316.974 326.848 336.73 346.558 356.447 2009-10-04 -18.8748 -10.7551 -2.63899 5.45781 13.5435 21.6034 29.6715 37.7625 45.9473 54.3142 ... 258.998 267.471 275.842 284.219 292.535 300.88 309.191 317.513 325.788 334.121 2010-01-03 10.9547 17.854 24.7496 31.627 38.4928 45.3325 52.1798 59.0516 66.0185 73.172 ... 244.872 251.738 258.495 265.25 271.951 278.678 285.379 292.095 298.771 305.502 We'll visualise our surface estimate as a wire-plot, where the darker colours denote price curve estimates from longer ago. fig , ax = plt . subplots ( dpi = 150 ) df_pred . T . plot ( legend = False , cmap = 'viridis' , linewidth = 1 , ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) ax . set_xlim ( df_pred . columns [ 0 ]) ax . set_ylim ( 0 , 400 ) (0.0, 400.0) Whilst SmoothDates accepts time-series of dispatchable generation and price as inputs to the fit method, predict doesn't accept a time-series of dispatchable generation or return a time-series of price estimates. Instead, predict returns a dataframe of the smoothed surface - unfortunately this is not what we need if we want to interface our work with the wider Python eco-system for sklearn based models. We'll create a further wrapper on top of SmoothDates that will accept a time-series of dispatchable generation and return the price estimate when the predict method is used. This will later be used for hyper-parameter tuning in, but could also be interfaced with tools such as TPOT for automated pipeline generation (perhaps the MOE estimate could be ensembled as an input to an ML model?). #exports def construct_pred_ts ( s , df_pred , rounding_dec = 1 ): \"\"\"Uses the time-adaptive LOWESS surface to generate time-series prediction\"\"\" vals = [] for dt_idx , val in track ( s . iteritems (), total = s . size ): vals += [ df_pred . loc [ round ( val , rounding_dec ), dt_idx . strftime ( '%Y-%m- %d ' )]] s_pred_ts = pd . Series ( vals , index = s . index ) return s_pred_ts class LowessDates ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a time-adaptive extension of the classical Locally Weighted Scatterplot Smoothing regression technique, including robustifying procedures against outliers. Initialisation Parameters: frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function Attributes: fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function ensemble_member_to_weights: Mapping from the regression dates to their respective weightings for each data-point ensemble_member_to_models: Mapping from the regression dates to their localised models reg_dates: Dates at which the local time-adaptive models will be centered around ensemble_member_to_preds: Mapping from the regression dates to their predictions reg_weights: Mapping from the prediction values to the weighting of each time-adaptive model reg_values: Predictions from each regression df_reg: A DataFrame of the time-adaptive surfce regression \"\"\" def __init__ ( self , frac = 0.3 , threshold_value = 52 , threshold_units = 'W' , pred_reg_dates = None ): self . fitted = False self . frac = frac self . threshold_value = threshold_value self . threshold_units = threshold_units self . pred_reg_dates = pred_reg_dates def fit ( self , x , y , dt_idx = None , reg_dates = None , lowess_kwargs = {}, ** fit_kwargs ): \"\"\" Calculation of the local regression coefficients for each of the LOWESS models across the dataset provided. This is a time-adaptive ensembled version of the `Lowess` model. Parameters: x: Values for the independent variable y: Values for the dependent variable dt_idx: Datetime index, if not provided the index of the x and y series will be used reg_dates: Dates at which the local time-adaptive models will be centered around lowess_kwargs: Additional arguments to be passed at model initialisation reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" x , y , dt_idx , reg_dates = process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ) self . ensemble_member_to_weights = construct_dt_weights ( dt_idx , reg_dates , threshold_value = self . threshold_value , threshold_units = self . threshold_units ) self . ensemble_member_to_models = fit_external_weighted_ensemble ( x , y , self . ensemble_member_to_weights , lowess_kwargs = lowess_kwargs , frac = self . frac , ** fit_kwargs ) self . reg_dates = reg_dates self . fitted = True return def predict ( self , x_pred , reg_x = None , reg_dates = None , return_df = True , rounding_dec = 1 ): \"\"\" Inference using the design matrix from the time-adaptive LOWESS fits Parameters: x_pred: Locations for the time-adaptive LOWESS inference Returns: y_pred: Estimated values using the time-adaptive LOWESS fit \"\"\" reg_dates = self . pred_reg_dates if reg_x is None : reg_x = np . round ( np . arange ( np . floor ( x_pred . min ()) - 5 , np . ceil ( x_pred . max ()) + 5 , 1 / ( 10 ** rounding_dec )), rounding_dec ) x_pred = x_pred . round ( rounding_dec ) if isinstance ( reg_x , pd . Series ): reg_x = reg_x . values # Fitting the smoothed regression self . ensemble_member_to_preds = get_ensemble_preds ( self . ensemble_member_to_models , x_pred = reg_x ) self . reg_weights = np . array ( list ( construct_dt_weights ( reg_dates , self . reg_dates ) . values ())) self . reg_weights = self . reg_weights / self . reg_weights . sum ( axis = 0 ) self . reg_values = np . array ( list ( self . ensemble_member_to_preds . values ())) y_reg = np . dot ( self . reg_weights . T , self . reg_values ) self . df_reg = pd . DataFrame ( y_reg , index = reg_dates . strftime ( '%Y-%m- %d ' ), columns = reg_x ) . T # Making the prediction s_pred_ts = construct_pred_ts ( x_pred , self . df_reg , rounding_dec = rounding_dec ) return s_pred_ts","title":"LOWESS"},{"location":"dev-03-lowess/#lowess","text":"Outlines the development of the Scikit-Learn compatible Lowess model, as well as its extension LowessDates used for time-adaptive LOWESS regression. Included are functions for extending both models to generate prediction and confidence intervals.","title":"Lowess"},{"location":"dev-03-lowess/#imports","text":"#exports import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from collections.abc import Iterable from sklearn import linear_model from sklearn.base import BaseEstimator , RegressorMixin from scipy.optimize import minimize from scipy import linalg from timeit import timeit from ipypb import track from moepy import eda","title":"Imports"},{"location":"dev-03-lowess/#lowess-development","text":"Before we go ahead any further we'll create some sample data for fitting and also define the fraction of the data over which we'll do the localised regression. x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) frac = 0.5 We can see that we've just created a simple sin curve plt . plot ( x , y ) [<matplotlib.lines.Line2D at 0x1fd650f86d0>]","title":"LOWESS Development"},{"location":"dev-03-lowess/#weights","text":"In order to do localised regression we need to know what points are local, for this reason we'll start by creating a function for calculating the distance between one point and all of the other points #exports get_dist = lambda X , x : np . abs ( X - x ) dist = get_dist ( x , x [ 0 ]) dist array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. , 2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.6, 3.8, 4. , 4.2, 4.4, 4.6, 4.8, 5. ]) We've defined our selection of local based on the fraction of surrounding data, this means we need to calculate the distance around any given point that contains the fraction of data specified #exports def get_dist_threshold ( dist , frac = 0.4 ): \"\"\"Identifies the minimum distance that contains the desired data fraction\"\"\" frac_idx = int ( np . ceil ( len ( dist ) * frac )) dist_threshold = sorted ( dist )[ frac_idx ] return dist_threshold dist_threshold = get_dist_threshold ( dist , frac = frac ) dist_threshold 2.6 We'll now define a function that will map from the distances to their relative weights according to a tricube kernel \\[ \\begin{equation} \\label{eqn:tricube_kernel} w(x) = \\left\\{ \\begin{array}{ll} (1 - |x|^3)^3 & \\mbox{for $|x| < 1$} \\\\ 0 & \\mbox{for $|x| \\geq 1$} \\end{array} \\right. \\end{equation} \\] #exports dist_to_weights = lambda dist , dist_threshold = 1 : ( 1 - (( np . abs ( dist ) / dist_threshold ) . clip ( 0 , 1 ) ** 3 )) ** 3 dist_threshold = 5 dist_sample = np . linspace ( 0 , 10 ) weights_sample = dist_to_weights ( dist_sample , dist_threshold ) # Plotting plt . plot ( dist_sample , weights_sample ) plt . xlabel ( 'Distance' ) plt . ylabel ( 'Weight' ) plt . xlim ( 0 , max ( dist_sample )) plt . ylim ( 0 , 1.1 ) (0.0, 1.1) We can now use the distance matrix and threshold to create a vector of the relative weights of all data points for the local regression at a specified location weights = dist_to_weights ( dist , dist_threshold ) weights array([1. , 0.99980801, 0.99846479, 0.99482495, 0.98776226, 0.97619149, 0.95909867, 0.93557909, 0.90488204, 0.86646079, 0.82002586, 0.76559882, 0.70356317, 0.63470792, 0.56025877, 0.4818903 , 0.40171203, 0.32221935, 0.24619951, 0.17658114, 0.11621427, 0.06756635, 0.03231788, 0.01083964, 0.00153137, 0. ]) We'll wrap these steps into a single function and see how long it takes to compute def get_weights ( x , loc , frac = 0.4 ): \"\"\"Calculates the weightings at each data point for a single localised regression\"\"\" dist = get_dist ( x , loc ) dist_threshold = get_dist_threshold ( dist , frac = frac ) weights = dist_to_weights ( dist , dist_threshold ) return weights timeit ( lambda : get_weights ( x , x [ 5 ]), number = 10000 ) 0.6126709999999989 We've successfully calculated the weights with respect to a single point but we need to repeat this across each of value locations in our dataset. #exports def get_all_weights ( x , frac = 0.4 ): \"\"\"Calculates the weightings at each data point for a LOWESS regression\"\"\" all_weights = [] for i in range ( len ( x )): weights = get_weights ( x , x [ i ], frac = frac ) all_weights += [ weights ] all_weights = np . array ( all_weights ) return all_weights all_weights = get_all_weights ( x , frac = frac ) all_weights [: 5 , : 5 ] array([[1. , 0.99863512, 0.98911574, 0.96358278, 0.91512916], [0.99826489, 1. , 0.99826489, 0.98617531, 0.95385361], [0.98207661, 0.99774775, 1. , 0.99774775, 0.98207661], [0.92116732, 0.97619149, 0.997003 , 1. , 0.997003 ], [0.75907091, 0.89295331, 0.96743815, 0.99589042, 1. ]]) Not too bad at all, we could now use this to weight the fitting of the polynomials in the LOWESS. However, we've carried out most of these operations as part of for loops over vectors, what if we could store our data in matrices and do single operations over them? Thankfully Numpy has lots of helpful functions to aid us in this. We'll start by creating a matrix with the distances, to do this we can reshape the vectors into matrices of shape (25, 1) and (1, 25), then deduct the matrix with only one row from the matrix with only one column. #exports vector_to_dist_matrix = lambda x : np . abs ( x . reshape ( - 1 , 1 ) - x . reshape ( 1 , - 1 )) dist_matrix = vector_to_dist_matrix ( x ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( dist_matrix , cmap = 'cool' , cbar_kws = { 'label' : 'Distance' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') This approach brings an order of magnitude speed-up to the operation %% timeit timeit ( lambda : [ get_dist ( x , x [ x_idx ]) for x_idx in range ( len ( x ))], number = 10000 ) 825 ms \u00b1 163 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) %% timeit timeit ( lambda : vector_to_dist_matrix ( x ), number = 10000 ) 55.5 ms \u00b1 3.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each) Now we need to calculate the distance threshold (max distance away from point of interest that is within the data fraction specified). Alexandre Gramfort lays out one approach to determine the distances using a list comprehension. #exports get_frac_idx = lambda x , frac : int ( np . ceil ( len ( x ) * frac )) - 1 gramfort_get_dist_thresholds = lambda x , frac_idx : [ np . sort ( np . abs ( x - x [ i ]))[ frac_idx ] for i in range ( len ( x ))] frac_idx = get_frac_idx ( x , frac ) timeit ( lambda : gramfort_get_dist_thresholds ( x , frac_idx ), number = 10000 ) 1.7785535999999986 Pretty quick, lets see if we could do better though. We'll try keeping the distance matrix intact rather than breaking it up in each iteration. This enables us to do the absolute conversion, sorting and indexing over a matrix rather than looping the operations over vectors. These changes gave us an order of magnitude speed-up. #exports get_dist_thresholds = lambda x , frac_idx , dist_matrix : np . sort ( dist_matrix )[:, frac_idx ] dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) timeit ( lambda : get_dist_thresholds ( x , frac_idx , dist_matrix ), number = 10000 ) 0.10906620000000089 Now we have both the distance matrix and thresholds we can start to calculate the weightings, the first step to this is scale and clip the distances based on their threshold values. inv_linear_weights = np . clip ( dist_matrix / dist_thresholds . reshape ( - 1 , 1 ), 0 , 1 ) inv_linear_weights [: 5 , : 5 ] array([[0. , 0.08333333, 0.16666667, 0.25 , 0.33333333], [0.09090909, 0. , 0.09090909, 0.18181818, 0.27272727], [0.2 , 0.1 , 0. , 0.1 , 0.2 ], [0.33333333, 0.22222222, 0.11111111, 0. , 0.11111111], [0.5 , 0.375 , 0.25 , 0.125 , 0. ]]) We can now calculate the tri-cubic weighting. We repeat this using both base Python and Numpy to see which is faster, surprisingly base Python comes out on top. timeit ( lambda : ( 1 - inv_linear_weights ** 3 ) ** 3 , number = 10000 ) 0.4310475999999994 timeit ( lambda : np . power ( 1 - np . power ( inv_linear_weights , 3 ), 3 ), number = 10000 ) 0.44873660000000015 After a little more exploration it appears that Numpy's power function only offers improvements over base Python when the exponent is very high. For a good discussion on why this is the case you can read more here . timeit ( lambda : ( 1 - inv_linear_weights ** 50000 ) ** 50000 , number = 10000 ) 0.5387637000000005 timeit ( lambda : np . power ( 1 - np . power ( inv_linear_weights , 50000 ), 50000 ), number = 10000 ) 0.526861499999999 We'll now wrap these two steps up into a single function #exports def clean_weights ( weights ): \"\"\"Normalises each models weightings and removes non-finite values\"\"\" with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): weights = weights / weights . sum ( axis = 0 ) # We'll then normalise the weights so that for each model they sum to 1 for a single data point weights = np . where ( ~ np . isfinite ( weights ), 0 , weights ) # And remove any non-finite values return weights def dist_2_weights_matrix ( dist_matrix , dist_thresholds ): \"\"\"Converts distance matrix and thresholds to weightings\"\"\" weights = dist_to_weights ( dist_matrix , dist_thresholds . reshape ( - 1 , 1 )) weights = clean_weights ( weights ) return weights weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) weights [: 5 , : 5 ] array([[0.20861363, 0.18398963, 0.16064453, 0.13720751, 0.11473407], [0.20814378, 0.18430943, 0.16252964, 0.14126726, 0.12082652], [0.20364685, 0.18375705, 0.16289653, 0.14341436, 0.1254292 ], [0.18628223, 0.17830797, 0.16222709, 0.14384546, 0.12796029], [0.13975483, 0.15666172, 0.15537944, 0.14300426, 0.12848832]]) And then combine them with the creation of the distance matrix and threshold #exports def get_full_dataset_weights_matrix ( x , frac = 0.4 ): \"\"\"Wrapper for calculating weights from the raw data and LOWESS fraction\"\"\" frac_idx = get_frac_idx ( x , frac ) dist_matrix = vector_to_dist_matrix ( x ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) return weights weights = get_full_dataset_weights_matrix ( x , frac ) We'll do a quick visual check to see if they look reasonable weights [: 5 , : 5 ] array([[0.20861363, 0.18398963, 0.16064453, 0.13720751, 0.11473407], [0.20814378, 0.18430943, 0.16252964, 0.14126726, 0.12082652], [0.20364685, 0.18375705, 0.16289653, 0.14341436, 0.1254292 ], [0.18628223, 0.17830797, 0.16222709, 0.14384546, 0.12796029], [0.13975483, 0.15666172, 0.15537944, 0.14300426, 0.12848832]]) Looks good, we'll also time it timeit ( lambda : get_full_dataset_weights_matrix ( x , frac = frac ), number = 10000 ) 1.4529582999999988 Currently we have created a weights matrix that scales in size as the square of the dataset length, this could quickly become prohibitively computationally expensive for large datasets. Instead we'll create a new function that lets us either specify a vector of locations ( reg_anchors ) where the regressions will be centered, or alternatively if num_fits is passed that will be the number of local regressions. #exports num_fits_2_reg_anchors = lambda x , num_fits : np . linspace ( x . min (), x . max (), num = num_fits ) def get_weighting_locs ( x , reg_anchors = None , num_fits = None ): \"\"\"Identifies the weighting locations for the provided dataset\"\"\" num_type_2_dist_rows = { type ( None ) : lambda x , num_fits : x . reshape ( - 1 , 1 ), int : lambda x , num_fits : num_fits_2_reg_anchors ( x , num_fits ) . reshape ( - 1 , 1 ), } if reg_anchors is None : weighting_locs = num_type_2_dist_rows [ type ( num_fits )]( x , num_fits ) else : weighting_locs = reg_anchors . reshape ( - 1 , 1 ) return weighting_locs def create_dist_matrix ( x , reg_anchors = None , num_fits = None ): \"\"\"Constructs the distance matrix for the desired weighting locations\"\"\" weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) dist_matrix = np . abs ( weighting_locs - x . reshape ( 1 , - 1 )) return dist_matrix dist_matrix = create_dist_matrix ( x ) dist_matrix . shape (26, 26) When neither reg_anchors nor num_fits are passed it defaults to using all data-points in the matrix which can be seen below dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') However if we specify num_fits as 10 we can see that only 10 regression nodes are defined in the weights matrix num_fits = 10 dist_matrix = create_dist_matrix ( x , num_fits = num_fits ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') But what about if you were only really interested in getting highly localised regressions for a specific part of your variable space? Using the reg_anchors variable we can now specify our own grid over which to carry out the regressions. reg_anchors = np . array ([ 0 , 0.25 , 0.5 , 0.75 , 1 , 1.25 , 1.5 , 1.75 , 2 , 2.5 , 3 , 3.5 , 4 , 4.5 , 5 ]) dist_matrix = create_dist_matrix ( x , reg_anchors = reg_anchors ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') We'll wrap these steps up in a couple of functions and add in some syntactic sugar to allow the user to more flexibily specify the distance matrix kwargs #exports def get_weights_matrix ( x , frac = 0.4 , weighting_locs = None , reg_anchors = None , num_fits = None ): \"\"\"Wrapper for calculating weights from the raw data and LOWESS fraction\"\"\" frac_idx = get_frac_idx ( x , frac ) if weighting_locs is not None : dist_matrix = np . abs ( weighting_locs - x . reshape ( 1 , - 1 )) else : dist_matrix = create_dist_matrix ( x , reg_anchors = reg_anchors , num_fits = num_fits ) dist_thresholds = get_dist_thresholds ( x , frac_idx , dist_matrix ) weights = dist_2_weights_matrix ( dist_matrix , dist_thresholds ) return weights weights = get_weights_matrix ( x , frac = 0.5 , reg_anchors = reg_anchors ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) Text(69.58333333333334, 0.5, 'Regression Nodes') We'll check this still works when we want to carry out a LOWESS fit over all points weights = get_weights_matrix ( x , frac = frac ) print ( weights . shape ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . heatmap ( weights , cmap = 'cool' , cbar_kws = { 'label' : 'Weights' }, ax = ax ) ax . set_xlabel ( 'Data Point' ) ax . set_ylabel ( 'Regression Nodes' ) (26, 26) Text(69.58333333333334, 0.5, 'Regression Nodes')","title":"Weights"},{"location":"dev-03-lowess/#regression","text":"Now that we've calculated the weightings necessary for local regression we need to create the regression functions. We'll start by calculating the intercept and gradient of a linear regression fit with optional weighting. N.b. This section of the code was heavily inspired by this gist created by Alexandere Gramfort #exports def calc_lin_reg_betas ( x , y , weights = None ): \"\"\"Calculates the intercept and gradient for the specified local regressions\"\"\" if weights is None : weights = np . ones ( len ( x )) b = np . array ([ np . sum ( weights * y ), np . sum ( weights * y * x )]) A = np . array ([[ np . sum ( weights ), np . sum ( weights * x )], [ np . sum ( weights * x ), np . sum ( weights * x * x )]]) betas = np . linalg . lstsq ( A , b , rcond = None )[ 0 ] return betas intercept , gradient = calc_lin_reg_betas ( x , y ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , label = 'Original' ) ax . plot ([ x . min (), x . max ()], [ intercept + gradient * x . min (), intercept + gradient * x . max ()], label = 'Linear Regression' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We'll now repeat the regression calculation but will also specify a weighting for each data-point row_weights = weights [ 14 , :] intercept , gradient = calc_lin_reg_betas ( x , y , row_weights ) x_used = x [ x * row_weights > 0 ] y_used = y [ x * row_weights > 0 ] x_weights = row_weights [ x * row_weights > 0 ] ## Plotting fig , ax = plt . subplots ( dpi = 250 , figsize = ( 7 , 4 )) weighted_points = ax . scatter ( x , y , s = ( row_weights ) * 300 , c = row_weights , edgecolor = 'k' , cmap = 'gray_r' , label = 'Points Used' , zorder = 3 , vmin = 0 ) cbar = fig . colorbar ( weighted_points , label = 'Point Weighting' ) ax . plot ( x , y , label = 'Original' ) ax . plot ([ x . min (), x . max ()], [ intercept + gradient * x . min (), intercept + gradient * x . max ()], label = 'Linear Regression' ) ax . set_ylim ( - 1.2 , 1.2 ) eda . hide_spines ( ax ) leg = ax . legend ( frameon = False ) fig . savefig ( '../img/LOWESS_single_regression_example.png' , dpi = 250 ) We can repeat this for all data-points, the error being minimized across these regressions is shown in the equation below \\[ \\begin{equation} \\label{eqn:lowess_err} n^{-1} \\sum_{i=1}^{n} W_{k i}(x)\\left(y_{i}-\\sum_{j=0}^{p} \\beta_{j} x^{j}\\right)^{2} \\end{equation} \\] n = len ( x ) y_pred = np . zeros ( n ) for i in range ( n ): row_weights = weights [ i , :] betas = calc_lin_reg_betas ( x , y , weights [ i , :]) y_pred [ i ] = betas [ 0 ] + betas [ 1 ] * x [ i ] plt . plot ( x , y , label = 'Original' ) plt . plot ( x , y_pred , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a23310> Whilst this fit doesn't look great remember we can reduce the fraction of data used in each fit to get a more localised regression, in this example we'll also make use of the num_fits parameter to reduce the number of computations that are run. num_fits = 10 weights = get_weights_matrix ( x , frac = 0.2 , num_fits = num_fits ) x_pred = num_fits_2_reg_anchors ( x , num_fits ) . reshape ( - 1 , 1 ) y_pred = np . zeros ( len ( x_pred )) for i in range ( len ( x_pred )): row_weights = weights [ i , :] betas = calc_lin_reg_betas ( x , y , row_weights ) y_pred [ i ] = betas [ 0 ] + betas [ 1 ] * x_pred [ i ] plt . plot ( x , y , label = 'Original' ) plt . plot ( x_pred , y_pred , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a85760> Rather than carrying out the regression fitting and prediction together we'll seperate them to add some flexibility, for example we wouldnt be able to make predictions with a specified number of polynomial fits using the code we just wrote. For this fitting function we'll introduce a design matrix which will hold the coefficients for all of our regressions. We'll also make the function used for regression a parameter, this will allow us to replace it with other regression functions (e.g. polynomials) later on. #exports check_array = lambda array , x : np . ones ( len ( x )) if array is None else array def fit_regressions ( x , y , weights = None , reg_func = calc_lin_reg_betas , num_coef = 2 , ** reg_params ): \"\"\"Calculates the design matrix for the specified local regressions\"\"\" if weights is None : weights = np . ones ( len ( x )) n = weights . shape [ 0 ] y_pred = np . zeros ( n ) design_matrix = np . zeros (( n , num_coef )) for i in range ( n ): design_matrix [ i , :] = reg_func ( x , y , weights = weights [ i , :], ** reg_params ) return design_matrix weights = get_weights_matrix ( x , frac = 0.4 , num_fits = 10 ) design_matrix = fit_regressions ( x , y , weights ) design_matrix array([[ 0.05905438, 0.73999059], [ 0.08020775, 0.6991559 ], [ 0.35324949, 0.41467757], [ 1.09720376, -0.10762239], [ 2.03581046, -0.58348682], [ 2.77888971, -0.88141726], [ 2.91594092, -0.92733431], [ 2.05244287, -0.68647646], [ 0.84017439, -0.38519789], [ 0.6033549 , -0.33232683]]) We can use this design matrix to create predictions for every local regression, then combine them based on their weightings #exports def lowess_fit_and_predict ( x , y , frac = 0.4 , reg_anchors = None , num_fits = None , x_pred = None ): \"\"\"Fits and predicts smoothed local regressions at the specified locations\"\"\" weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) weights = get_weights_matrix ( x , frac = frac , weighting_locs = weighting_locs ) design_matrix = fit_regressions ( x , y , weights ) if x_pred is None : x_pred = x point_evals = design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = frac , reg_anchors = weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) return y_pred x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) y_pred = lowess_fit_and_predict ( x , y ) plt . plot ( x , y , label = 'Original' ) plt . plot ( x , y_pred , '--' , label = 'LOWESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3aefd30> If we pass an array to the x_pred parameter then those values will be used as the locations of the output predictions x_pred = np . array ([ 0 , 1 , 1.5 , 2 , 3 , 4 , 5 ]) y_pred = lowess_fit_and_predict ( x , y , num_fits = 10 , x_pred = x_pred ) plt . plot ( x , y , label = 'Original' ) plt . plot ( x_pred , y_pred , '--' , label = 'LOESS' ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3b48e80> Lets do some time tests, starting with a small dataset x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) %% timeit y_pred = lowess_fit_and_predict ( x , y ) 2.02 ms \u00b1 57.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each) Even with larger datasets it remains performant x = np . linspace ( 0 , 5 , num = 3000 ) y = np . sin ( x ) frac = 0.1 x_pred = np . linspace ( 0 , 5 , 100 )[ 1 :] # to avoid divide by zero in MAPE calc %% timeit y_pred = lowess_fit_and_predict ( x , y , frac = frac , x_pred = x_pred ) 989 ms \u00b1 21.6 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each) When making predictions with large datasets and low fractions we can achieve low error for the simple sin curve fitting example y_pred = lowess_fit_and_predict ( x , y , frac = frac , x_pred = x_pred ) y_true = np . sin ( x_pred ) print ( f 'MAPE: { round ( 100 * np . abs (( y_true - y_pred ) / y_true ) . mean (), 3 ) } %' ) MAPE: 0.084%","title":"Regression"},{"location":"dev-03-lowess/#lowess-on-real-world-data","text":"We'll now evaluate the LOWESS fit on some real data, we'll start by loading the electric insights dataset %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.86 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll start by extracting the X and y data y_col = 'day_ahead_price' X_cols = [ 'demand' ] yX = df_EI [ '2020' :][[ y_col ] + X_cols ] . dropna () . values y = yX [:, 0 ] X = yX [:, 1 :] print ( y . shape , X . shape ) (17435,) (17435, 1) We'll then fit the model and make the prediction %% time x = X [:, 0 ] x_pred = np . linspace ( 15 , 55 , 41 ) y_pred = lowess_fit_and_predict ( x , y , frac = 0.4 , num_fits = 25 , x_pred = x_pred ) Wall time: 63 ms And now we can plot the results! fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , c = 'r' ) ax . scatter ( x , y , s = 0.1 , c = 'k' , alpha = 0.25 ) ax . set_ylim ( 0 , 100 ) ax . set_xlim ( 15 , 55 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) Text(0, 0.5, 'Day-Ahead Price (\u00a3/MWh)')","title":"LOWESS on Real-World Data"},{"location":"dev-03-lowess/#robust-regression","text":"What we've done so far is ultimately nothing more than an extension of linear regression, which has been achieved by fitting multiple regressions and manipulating the weighting of the data points used in each one. x = np . linspace ( 0 , 5 , num = 26 ) y = np . sin ( x ) x_pred = np . linspace ( 0 , 5 , 100 ) y_pred = lowess_fit_and_predict ( x , y , num_fits = 5 , x_pred = x_pred ) plt . plot ( x , y ) plt . plot ( x_pred , y_pred , '--' ) [<matplotlib.lines.Line2D at 0x1a4b2f9e160>] But we can also do much more with the weights, including adjusting them to make our regression more robust against outliers. First we'll create a new dataset which has some noise. x = np . linspace ( 0 , 5 , num = 150 ) weights = get_weights_matrix ( x , frac = 0.2 ) y = np . sin ( x ) y_noisy = y + ( np . random . normal ( size = len ( y ))) / 10 y_noisy [ 25 ] = 2 y_noisy [ 50 ] = - 2 y_noisy [ 75 ] = 2 plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b39dc310> We can use our lowess_fit_and_predict function to make a lowess model of this data y_pred = lowess_fit_and_predict ( x , y_noisy , frac = 0.2 , num_fits = 25 ) plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x , y_pred , '--' , label = 'LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b3a630d0> The issue though is that our model is being highly skewed by the outliers present in the data, robust regression provides a method to handle this (some improvements can also be made by increasing the frac value). To carry it out we need to repeat the lowess fit, but when we do we must further weight the data-points to minimize the influence of outliers. Robust regression is an iterative procedure that seeks to identify outliers and minimize their impact on the coefficient estimates - NCSS Cleveland, W. S. (1979) in his paper 'Robust Locally Weighted Regression and Smoothing Scatterplots' outlines a procedure for carrying out robust lowess regression that we will use here. We'll start by calculating the standard deviation of the residuals. residuals = y_noisy - y_pred std_dev = np . quantile ( np . abs ( residuals ), 0.682 ) std_dev 0.13459575203567298 We can then define a threshold, say 6 standard deviations, and clip any values outside of that. We're left with a cleaned version of the residuals. max_std_dev = 6 cleaned_residuals = np . clip ( residuals / ( max_std_dev * std_dev ), - 1 , 1 ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 10 , 4 )) ax = axs [ 0 ] sns . histplot ( residuals , ax = ax ) ax . set_title ( 'Residuals' ) ax = axs [ 1 ] sns . histplot ( cleaned_residuals , ax = ax ) ax . set_xlim ( - 1 , 1 ) ax . set_title ( 'Cleaned Residuals' ) for ax in axs : eda . hide_spines ( ax ) In the last step we clipped all of our values from -1 to +1, that means if we square our values and deduct them from 1 any outliers will go to 0. The returned vector we'll call the robust_weights . robust_weights = ( 1 - cleaned_residuals ** 2 ) ** 2 # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . histplot ( robust_weights , ax = ax ) ax . set_xlim ( 0 , 1 ) eda . hide_spines ( ax ) Before we move on we'll combine these into a single step for calculating the robust_weights #exports def calc_robust_weights ( y , y_pred , max_std_dev = 6 ): \"\"\"Calculates robustifying weightings that penalise outliers\"\"\" residuals = y - y_pred std_dev = np . quantile ( np . abs ( residuals ), 0.682 ) cleaned_residuals = np . clip ( residuals / ( max_std_dev * std_dev ), - 1 , 1 ) robust_weights = ( 1 - cleaned_residuals ** 2 ) ** 2 return robust_weights We'll now refactor our previous lowess_fit_and_predict but this time will enable it to carry out robust regressions #exports def robust_lowess_fit_and_predict ( x , y , frac = 0.4 , reg_anchors = None , num_fits = None , x_pred = None , robust_weights = None , robust_iters = 3 ): \"\"\"Fits and predicts robust smoothed local regressions at the specified locations\"\"\" # Identifying the initial loading weights weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) loading_weights = get_weights_matrix ( x , frac = frac , weighting_locs = weighting_locs ) # Robustifying the weights (to reduce outlier influence) if robust_weights is None : robust_loading_weights = loading_weights else : robust_loading_weights = np . multiply ( robust_weights , loading_weights ) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): robust_loading_weights = robust_loading_weights / robust_loading_weights . sum ( axis = 0 ) robust_loading_weights = np . where ( ~ np . isfinite ( robust_loading_weights ), 0 , robust_loading_weights ) # Fitting the model and making predictions design_matrix = fit_regressions ( x , y , robust_loading_weights ) if x_pred is None : x_pred = x point_evals = design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = frac , reg_anchors = weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) # Recursive robust regression robust_weights = calc_robust_weights ( y , y_pred ) if robust_iters > 1 : robust_iters -= 1 y_pred = robust_lowess_fit_and_predict ( x , y , frac = frac , reg_anchors = reg_anchors , num_fits = num_fits , x_pred = x_pred , robust_weights = robust_weights , robust_iters = robust_iters ) return y_pred y_pred = robust_lowess_fit_and_predict ( x , y_noisy , frac = 0.2 , num_fits = 25 ) plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x , y_pred , '--' , label = 'Robust LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b329c7f0> We've got everything working nicely but the current way we make predictions doesn't make it easy to plug-and-play with other Python libraries, to reduce this friction we'll refactor the code again and create a Scikit-Learn wrapper for our process. #exports class Lowess ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a Scikit-Learn compatible model for Locally Weighted Scatterplot Smoothing, including robustifying procedures against outliers. For more information on the underlying algorithm please refer to * William S. Cleveland: \"Robust locally weighted regression and smoothing scatterplots\", Journal of the American Statistical Association, December 1979, volume 74, number 368, pp. 829-836. * William S. Cleveland and Susan J. Devlin: \"Locally weighted regression: An approach to regression analysis by local fitting\", Journal of the American Statistical Association, September 1988, volume 83, number 403, pp. 596-610. Example Usage: ``` x = np.linspace(0, 5, num=150) y = np.sin(x) y_noisy = y + (np.random.normal(size=len(y)))/10 lowess = Lowess() lowess.fit(x, y_noisy, frac=0.2) x_pred = np.linspace(0, 5, 26) y_pred = lowess.predict(x_pred) ``` Initialisation Parameters: reg_func: function that accepts the x and y values then returns the intercepts and gradients Attributes: reg_func: function that accepts the x and y values then returns the intercepts and gradients fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression weighting_locs: Locations of the local regression centers loading_weights: Weights of each data-point across the localalised models design_matrix: Regression coefficients for each of the localised models \"\"\" def __init__ ( self , reg_func = calc_lin_reg_betas ): self . reg_func = reg_func self . fitted = False return def calculate_loading_weights ( self , x , reg_anchors = None , num_fits = None , external_weights = None , robust_weights = None ): \"\"\" Calculates the loading weights for each data-point across the localised models Parameters: x: values for the independent variable reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers \"\"\" # Calculating the initial loading weights weighting_locs = get_weighting_locs ( x , reg_anchors = reg_anchors , num_fits = num_fits ) loading_weights = get_weights_matrix ( x , frac = self . frac , weighting_locs = weighting_locs ) # Applying weight adjustments if external_weights is None : external_weights = np . ones ( x . shape [ 0 ]) if robust_weights is None : robust_weights = np . ones ( x . shape [ 0 ]) weight_adj = np . multiply ( external_weights , robust_weights ) loading_weights = np . multiply ( weight_adj , loading_weights ) # Post-processing weights with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): loading_weights = loading_weights / loading_weights . sum ( axis = 0 ) # normalising loading_weights = np . where ( ~ np . isfinite ( loading_weights ), 0 , loading_weights ) # removing non-finite values self . weighting_locs = weighting_locs self . loading_weights = loading_weights return def fit ( self , x , y , frac = 0.4 , reg_anchors = None , num_fits = None , external_weights = None , robust_weights = None , robust_iters = 3 , ** reg_params ): \"\"\" Calculation of the local regression coefficients for a LOWESS model across the dataset provided. This method will reassign the `frac`, `weighting_locs`, `loading_weights`, and `design_matrix` attributes of the `Lowess` object. Parameters: x: values for the independent variable y: values for the dependent variable frac: LOWESS bandwidth for local regression as a fraction reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" self . frac = frac # Solving for the design matrix self . calculate_loading_weights ( x , reg_anchors = reg_anchors , num_fits = num_fits , external_weights = external_weights , robust_weights = robust_weights ) self . design_matrix = fit_regressions ( x , y , weights = self . loading_weights , reg_func = self . reg_func , ** reg_params ) # Recursive robust regression if robust_iters > 1 : y_pred = self . predict ( x ) robust_weights = calc_robust_weights ( y , y_pred ) robust_iters -= 1 y_pred = self . fit ( x , y , frac = self . frac , reg_anchors = reg_anchors , num_fits = num_fits , external_weights = external_weights , robust_weights = robust_weights , robust_iters = robust_iters , ** reg_params ) return y_pred self . fitted = True return def predict ( self , x_pred ): \"\"\" Inference using the design matrix from the LOWESS fit Parameters: x_pred: Locations for the LOWESS inference Returns: y_pred: Estimated values using the LOWESS fit \"\"\" point_evals = self . design_matrix [:, 0 ] + np . dot ( x_pred . reshape ( - 1 , 1 ), self . design_matrix [:, 1 ] . reshape ( 1 , - 1 )) pred_weights = get_weights_matrix ( x_pred , frac = self . frac , reg_anchors = self . weighting_locs ) y_pred = np . multiply ( pred_weights , point_evals . T ) . sum ( axis = 0 ) return y_pred lowess = Lowess () lowess . fit ( x , y_noisy , frac = 0.2 ) x_pred = np . linspace ( 0 , 5 , 26 ) y_pred = lowess . predict ( x_pred ) # Plotting plt . plot ( x , y , label = 'Original' , zorder = 2 ) plt . plot ( x_pred , y_pred , '--' , label = 'Robust LOWESS' , color = 'k' , zorder = 3 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b45ad850>","title":"Robust Regression"},{"location":"dev-03-lowess/#confidence-intervals","text":"Create the ensemble_results then take the specified confidence range Put into an sklearn wrapper We're now able to take noisy data and find a robust estimate for the average value at any point, but what about uncertainty? In this work we'll look at two types of uncertainty quantification: confidence & prediction intervals. In this section we will discuss confidence intervals, summarised well in this statement: The selection of a confidence level for an interval determines the probability that the confidence interval produced will contain the true parameter value - stat.yale.edu We'll start by creating a slightly noiser and longer sin curve than used previously. x = np . linspace ( 0 , 10 , num = 500 ) y = np . sin ( x ) heteroskedasticity_factor = ( 1 + 2 * np . array ( range ( len ( x ))) / len ( x )) / 10 y_noisy = y + heteroskedasticity_factor * ( np . random . normal ( size = len ( y ))) plt . plot ( x , y , label = 'Original' , linewidth = 2 , zorder = 2 ) plt . scatter ( x , y_noisy , label = 'With Noise' , color = 'C1' , s = 5 , zorder = 1 ) plt . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1a4b78d3ac0> To determine the confidence in our estimate we want to know how the model will perform against previously unseen data-points. We can randomly separate our data, calculate the error, then repeat many times to obtain a set of possible errors along the curve - this is a specific use-case of the more general bootstrapping approach. To generate bootstrapped statistics we first need to split our data set, we'll begin by creating a function that returns the indexes for our in- and out-of-bag samples. #exports def get_bootstrap_idxs ( x , bootstrap_bag_size = 0.5 ): \"\"\"Determines the indexes of an array to be used for the in- and out-of-bag bootstrap samples\"\"\" # Bag size handling assert bootstrap_bag_size > 0 , 'Bootstrap bag size must be greater than 0' if bootstrap_bag_size > 1 : assert int ( bootstrap_bag_size ) == bootstrap_bag_size , 'If the bootstrab bag size is not provided as a fraction then it must be an integer' else : bootstrap_bag_size = int ( np . ceil ( bootstrap_bag_size * len ( x ))) # Splitting in-bag and out-of-bag samlpes idxs = np . array ( range ( len ( x ))) ib_idxs = np . sort ( np . random . choice ( idxs , bootstrap_bag_size , replace = True )) oob_idxs = np . setdiff1d ( idxs , ib_idxs ) return ib_idxs , oob_idxs ib_idxs , oob_idxs = get_bootstrap_idxs ( x ) print ( f 'in-bag: { len ( ib_idxs ) } , out-of-bag: { len ( oob_idxs ) } ' ) in-bag: 250, out-of-bag: 299 We'll now calculate the standard deviation of the in- and out-of-bag errors #exports def get_bootstrap_resid_std_devs ( x , y , bag_size , model = Lowess (), ** model_kwargs ): \"\"\"Calculates the standard deviation of the in- and out-of-bag errors\"\"\" # Splitting the in- and out-of-bag samples ib_idxs , oob_idxs = get_bootstrap_idxs ( x , bag_size ) x_ib , x_oob = x [ ib_idxs ], x [ oob_idxs ] y_ib , y_oob = y [ ib_idxs ], y [ oob_idxs ] # Fitting and predicting with the model model . fit ( x_ib , y_ib , ** model_kwargs ) y_pred = model . predict ( x ) y_ib_pred = model . predict ( x_ib ) y_oob_pred = model . predict ( x_oob ) # Calculating the error y_ib_resids = y_ib - y_ib_pred ib_resid_std_dev = np . std ( np . abs ( y_ib_resids )) y_oob_resids = y_oob - y_oob_pred oob_resid_std_dev = np . std ( np . abs ( y_oob_resids )) return ib_resid_std_dev , oob_resid_std_dev get_bootstrap_resid_std_devs ( x , y , bag_size = 0.5 , frac = 0.2 , num_fits = 20 ) (0.020320708955639148, 0.024223597689702395) We'll quickly plot the distributions of the errors for each set bag_size = 0.5 num_runs = 1000 ib_resid_std_devs = [] oob_resid_std_devs = [] for model_run in track ( range ( num_runs )): ib_resid_std_dev , oob_resid_std_dev = get_bootstrap_resid_std_devs ( x , y_noisy , bag_size , frac = 0.2 , num_fits = 20 ) ib_resid_std_devs += [ ib_resid_std_dev ] oob_resid_std_devs += [ oob_resid_std_dev ] # Plotting fig , ax = plt . subplots ( dpi = 150 ) sns . histplot ( ib_resid_std_devs , ax = ax , alpha = 0.5 , label = 'In-Bag' ) sns . histplot ( oob_resid_std_devs , ax = ax , alpha = 0.5 , color = 'C1' , label = 'Out-of-Bag' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Residual \\' s Standard Deviation' ) 100% 1000/1000 [00:28 < 00:00, 0.03s/it] Text(0.5, 0, \"Residual's Standard Deviation\") We'll now create two wrapper functions, one for running models, the other for bootstrapping them. The returned df_bootstrap includes the predictions for each model run. N.b. the bootstrap_model is a generalisable function that will work with any Scikit-Learn compatible model. #exports def run_model ( x , y , bag_size , model = Lowess (), x_pred = None , ** model_kwargs ): \"\"\"Fits a model and then uses it to make a prediction\"\"\" if x_pred is None : x_pred = x # Splitting the in- and out-of-bag samples ib_idxs , oob_idxs = get_bootstrap_idxs ( x , bag_size ) x_ib , y_ib = x [ ib_idxs ], y [ ib_idxs ] # Fitting and predicting the model model . fit ( x_ib , y_ib , ** model_kwargs ) y_pred = model . predict ( x_pred ) return y_pred def bootstrap_model ( x , y , bag_size = 0.5 , model = Lowess (), x_pred = None , num_runs = 1000 , ** model_kwargs ): \"\"\"Repeatedly fits and predicts using the specified model, using different subsets of the data each time\"\"\" # Creating the ensemble predictions preds = [] for bootstrap_run in track ( range ( num_runs )): y_pred = run_model ( x , y , bag_size , model = model , x_pred = x_pred , ** model_kwargs ) preds += [ y_pred ] # Wrangling into a dataframe df_bootstrap = pd . DataFrame ( preds , columns = x ) . T df_bootstrap . index . name = 'x' df_bootstrap . columns . name = 'bootstrap_run' return df_bootstrap df_bootstrap = bootstrap_model ( x , y_noisy , num_runs = 1000 , frac = 0.2 , num_fits = 20 ) df_bootstrap . head () 100% 1000/1000 [00:25 < 00:00, 0.03s/it] x 0 1 2 3 4 5 6 7 8 9 ... 990 991 992 993 994 995 996 997 998 999 0 0.12324 0.100568 0.034607 0.062935 0.037283 0.024595 0.077896 0.071517 0.154899 0.104609 ... 0.106006 0.022008 0.117897 0.042924 0.094948 0.095224 0.109828 0.097625 0.096894 0.102314 0.02004 0.135523 0.114414 0.04894 0.077258 0.051836 0.039222 0.091694 0.086137 0.166246 0.116779 ... 0.119922 0.037429 0.131267 0.057019 0.109155 0.107799 0.12283 0.111439 0.109543 0.11558 0.04008 0.147791 0.128252 0.063258 0.091566 0.06638 0.053843 0.105484 0.100748 0.177581 0.128936 ... 0.133827 0.052831 0.144617 0.071105 0.12335 0.120364 0.135816 0.125241 0.122177 0.128832 0.06012 0.160044 0.142084 0.077562 0.10586 0.080914 0.068458 0.119268 0.115353 0.188904 0.141083 ... 0.147722 0.068217 0.157946 0.085199 0.137534 0.132918 0.148786 0.13903 0.134797 0.14207 0.08016 0.172286 0.155927 0.091852 0.120165 0.09544 0.083068 0.133063 0.129976 0.200216 0.153222 ... 0.161608 0.083587 0.171256 0.099382 0.151709 0.14548 0.161742 0.15281 0.147405 0.1553 Using df_bootstrap we can calculate the confidence interval of our predictions, the Pandas DataFrame quantile method makes this particularly simple. #exports def get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ): \"\"\"Estimates the confidence interval of a prediction based on the bootstrapped estimates\"\"\" conf_margin = ( 1 - conf_pct ) / 2 df_conf_intvl = pd . DataFrame ( columns = [ 'min' , 'max' ], index = df_bootstrap . index ) df_conf_intvl [ 'min' ] = df_bootstrap . quantile ( conf_margin , axis = 1 ) df_conf_intvl [ 'max' ] = df_bootstrap . quantile ( 1 - conf_margin , axis = 1 ) df_conf_intvl = df_conf_intvl . sort_index () return df_conf_intvl df_conf_intvl = get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , 'k--' , label = 'Original' , linewidth = 1 , zorder = 2 ) ax . fill_between ( df_conf_intvl . index , df_conf_intvl [ 'min' ], df_conf_intvl [ 'max' ], color = 'r' , edgecolor = 'k' , alpha = 0.25 , label = '95% Confidence' ) ax . scatter ( x , y_noisy , label = 'With Noise' , color = 'w' , edgecolor = 'k' , linewidth = 0.3 , s = 2.5 , zorder = 1 ) ax . legend ( frameon = False ) ax . set_xlim ( 0 , 10 ) eda . hide_spines ( ax )","title":"Confidence Intervals"},{"location":"dev-03-lowess/#quantile-predictions","text":"Earlier when creating our Lowess class we enabled the function used in calculating the design matrix betas to be specified at initialisation. We can now use this to pass a custom function that will calculate the design matrix for a local quantile regression. #exports def pred_to_quantile_loss ( y , y_pred , q = 0.5 , weights = None ): \"\"\"Calculates the quantile error for a prediction\"\"\" residuals = y - y_pred if weights is not None : residuals = weights * residuals loss = np . array ([ q * residuals , ( q - 1 ) * residuals ]) . max ( axis = 0 ) . mean () return loss def calc_quant_reg_loss ( x0 , x , y , q , weights = None ): \"\"\"Makes a quantile prediction then calculates its error\"\"\" if weights is None : weights = np . ones ( len ( x )) quantile_pred = x0 [ 0 ] + x0 [ 1 ] * x loss = pred_to_quantile_loss ( y , quantile_pred , q , weights ) return loss calc_quant_reg_betas = lambda x , y , q = 0.5 , x0 = np . zeros ( 2 ), weights = None , method = 'nelder-mead' : minimize ( calc_quant_reg_loss , x0 , method = method , args = ( x , y , q , weights )) . x We'll then create a wrapper that will fit the model for several specified quantiles. N.b. this function should generalise to any Scikit-Learn compatible model that uses q as the kwarg for the quantile. #exports def quantile_model ( x , y , model = Lowess ( calc_quant_reg_betas ), x_pred = None , qs = np . linspace ( 0.1 , 0.9 , 9 ), ** model_kwargs ): \"\"\"Model wrapper that will repeatedly fit and predict for the specified quantiles\"\"\" if x_pred is None : x_pred = np . sort ( np . unique ( x )) q_to_preds = dict () for q in track ( qs ): model . fit ( x , y , q = q , ** model_kwargs ) q_to_preds [ q ] = model . predict ( x_pred ) df_quantiles = pd . DataFrame ( q_to_preds , index = x_pred ) df_quantiles . index . name = 'x' df_quantiles . columns . name = 'quantiles' return df_quantiles %% time df_quantiles = quantile_model ( x , y_noisy , frac = 0.2 , num_fits = 100 , robust_iters = 1 ) df_quantiles . head () 100% 9/9 [00:11 < 00:01, 1.18s/it] Wall time: 10.6 s x 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 -0.071164 -0.004352 0.011374 0.025166 0.080732 0.091657 0.144884 0.163482 0.192227 0.02004 -0.056877 0.009553 0.025934 0.040718 0.095134 0.106431 0.158797 0.178013 0.207308 0.04008 -0.042584 0.023455 0.040493 0.056241 0.109531 0.121194 0.172707 0.192542 0.222345 0.06012 -0.028267 0.037368 0.055069 0.07175 0.123931 0.135959 0.186627 0.207077 0.237343 0.08016 -0.013915 0.0513 0.069668 0.087256 0.138339 0.150735 0.200564 0.221624 0.252307 We can visualise the range of our predictions fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x , y , 'k--' , label = 'Original' , linewidth = 1 , zorder = 2 ) ax . scatter ( x , y_noisy , label = 'With Noise' , color = 'w' , edgecolor = 'k' , linewidth = 0.3 , s = 2.5 , zorder = 1 ) ax . fill_between ( df_quantiles . index , df_quantiles [ 0.1 ], df_quantiles [ 0.9 ], color = 'r' , edgecolor = 'k' , alpha = 0.25 , label = '10-90% Prediction Interval' ) ax . legend ( frameon = False , loc = 3 ) ax . set_xlim ( 0 , 10 ) eda . hide_spines ( ax )","title":"Quantile Predictions"},{"location":"dev-03-lowess/#external-weights","text":"When we made our Lowess class we included the option to specify external_weights , the reason for this is that it allows us to carry out further model smoothing using variables outside of the regression. This makes particular sense for variables such as time. Lets first plot two subsets of the data to see why we need to do this in the first place. df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-09' : '2011-03' ], s_price [ '2010-09' : '2011-03' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Lets start by adding some boolean filters that we'll then cast as integers to act as weights, for one we'll choose an early winter period from the dataset, for the other we'll take the latest summer model_to_dt_weights = { 'Winter 10-11' : (( s_dispatchable . index < '2011-03' ) & ( s_dispatchable . index > '2010-09' )) . astype ( int ), 'Summer 20' : (( s_dispatchable . index < '2020-09' ) & ( s_dispatchable . index > '2020-03' )) . astype ( int ) } df_preds = pd . DataFrame () for model_name , dt_weights in model_to_dt_weights . items (): lowess = Lowess () lowess . fit ( s_dispatchable . values , s_price . values , frac = 0.3 , num_fits = 31 , external_weights = dt_weights ) x_pred = np . linspace ( 8 , 60 , 53 ) y_pred = lowess . predict ( x_pred ) df_preds [ model_name ] = pd . Series ( y_pred , index = x_pred ) We'll plot our estimates alongside the true values fig , ax = plt . subplots ( dpi = 250 ) for i , ( model_name , dt_weights ) in enumerate ( model_to_dt_weights . items ()): color = f 'C { i } ' ax . scatter ( s_dispatchable [ dt_weights . astype ( bool )], s_price [ dt_weights . astype ( bool )], color = color , s = 0.5 , label = model_name ) min_ , max_ = s_dispatchable [ dt_weights . astype ( bool )] . quantile ([ 0.001 , 0.99 ]) df_preds . loc [ df_preds . index > min_ , model_name ] . plot ( ax = ax , color = 'k' , linewidth = 2 , label = '_nolegend_' ) df_preds . loc [ df_preds . index > min_ , model_name ] . plot ( ax = ax , color = color , linestyle = '--' , label = '_nolegend_' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Instead of just using a boolean value to indicate whether an observation belongs to a specific date period, we could instead assign weightings based on the distance from specific dates. This has the benefit that we can reuse existing functions that we wrote earlier. #exports def calc_timedelta_dists ( dates , central_date , threshold_value = 24 , threshold_units = 'W' ): \"\"\"Maps datetimes to weights using the central date and threshold information provided\"\"\" timedeltas = pd . to_datetime ( dates , utc = True ) - pd . to_datetime ( central_date , utc = True ) timedelta_dists = timedeltas / pd . Timedelta ( value = threshold_value , unit = threshold_units ) return timedelta_dists central_date = '2017-01-01' timedelta_dists = calc_timedelta_dists ( df_EI . index , central_date ) weights = dist_to_weights ( timedelta_dists . values ) plt . plot ( df_EI . index , weights ) [<matplotlib.lines.Line2D at 0x1a4b3acd190>] We'll create a wrapper that does this for all of the dates at which we wish to create a localised Lowess model #exports def construct_dt_weights ( dt_idx , reg_dates , threshold_value = 52 , threshold_units = 'W' ): \"\"\"Constructs a set of distance weightings based on the regression dates provided\"\"\" dt_to_weights = dict () for reg_date in reg_dates : dt_to_weights [ reg_date ] = pd . Series ( calc_timedelta_dists ( dt_idx , reg_date , threshold_value = threshold_value , threshold_units = threshold_units )) . pipe ( dist_to_weights ) . values return dt_to_weights %% time reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = 'Ys' ) dt_to_weights = construct_dt_weights ( df_EI_model . index , reg_dates ) sns . heatmap ( pd . DataFrame ( dt_to_weights , index = df_EI_model . index )) Wall time: 2.86 s <AxesSubplot:ylabel='local_datetime'> We'll create two wrapper functions for fitting the models and estimating using them as an ensemble. We'll also create a function that sanitises the inputs to the SmoothDates fitting method. #exports def fit_external_weighted_ensemble ( x , y , ensemble_member_to_weights , lowess_kwargs = {}, ** fit_kwargs ): \"\"\"Fits an ensemble of LOWESS models which have varying relevance for each subset of data over time\"\"\" ensemble_member_to_models = dict () for ensemble_member , ensemble_weights in track ( ensemble_member_to_weights . items ()): ensemble_member_to_models [ ensemble_member ] = Lowess ( ** lowess_kwargs ) ensemble_member_to_models [ ensemble_member ] . fit ( x , y , external_weights = ensemble_weights , ** fit_kwargs ) return ensemble_member_to_models def get_ensemble_preds ( ensemble_member_to_model , x_pred = np . linspace ( 8 , 60 , 53 )): \"\"\"Using the fitted ensemble of LOWESS models to generate the predictions for each of them\"\"\" ensemble_member_to_preds = dict () for ensemble_member in ensemble_member_to_model . keys (): ensemble_member_to_preds [ ensemble_member ] = ensemble_member_to_model [ ensemble_member ] . predict ( x_pred ) return ensemble_member_to_preds def process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ): \"\"\"Sanitises the inputs to the SmoothDates fitting method\"\"\" if hasattr ( x , 'index' ) and hasattr ( y , 'index' ): assert x . index . equals ( y . index ), 'If `x` and `y` have indexes then they must be the same' if dt_idx is None : dt_idx = x . index x = x . values y = y . values assert dt_idx is not None , '`dt_idx` must either be passed directly or `x` and `y` must include indexes' if reg_dates is None : reg_dates = dt_idx return x , y , dt_idx , reg_dates We now have everything we need to create our SmoothDates class that will enable us to create estimates of the surface fit of a LOWESS model over time #exports class SmoothDates ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a time-adaptive extension of the classical Locally Weighted Scatterplot Smoothing regression technique, including robustifying procedures against outliers. This model predicts the surface rather than individual point estimates. Initialisation Parameters: frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function Attributes: fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function ensemble_member_to_weights: Mapping from the regression dates to their respective weightings for each data-point ensemble_member_to_models: Mapping from the regression dates to their localised models reg_dates: Dates at which the local time-adaptive models will be centered around pred_weights: Weightings to map from the local models to the values to be inferenced pred_values: Raw prediction values as generated by each of the individual local models \"\"\" def __init__ ( self , frac = 0.3 , threshold_value = 52 , threshold_units = 'W' ): self . fitted = False self . frac = frac self . threshold_value = threshold_value self . threshold_units = threshold_units def fit ( self , x , y , dt_idx = None , reg_dates = None , lowess_kwargs = {}, ** fit_kwargs ): \"\"\" Calculation of the local regression coefficients for each of the LOWESS models across the dataset provided. This is a time-adaptive ensembled version of the `Lowess` model. Parameters: x: Values for the independent variable y: Values for the dependent variable dt_idx: Datetime index, if not provided the index of the x and y series will be used reg_dates: Dates at which the local time-adaptive models will be centered around lowess_kwargs: Additional arguments to be passed at model initialisation reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" x , y , dt_idx , reg_dates = process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ) self . ensemble_member_to_weights = construct_dt_weights ( dt_idx , reg_dates , threshold_value = self . threshold_value , threshold_units = self . threshold_units ) self . ensemble_member_to_models = fit_external_weighted_ensemble ( x , y , self . ensemble_member_to_weights , lowess_kwargs = lowess_kwargs , frac = self . frac , ** fit_kwargs ) self . reg_dates = reg_dates self . fitted = True return def predict ( self , x_pred = np . linspace ( 8 , 60 , 53 ), dt_pred = None , return_df = True ): \"\"\" Inference using the design matrix from the time-adaptive LOWESS fits Parameters: x_pred: Independent variable locations for the time-adaptive LOWESS inference dt_pred: Date locations for the time-adaptive LOWESS inference return_df: Flag specifying whether to return a dataframe or numpy matrix Returns: df_pred/y_pred: Estimated surface of the time-adaptive the LOWESS fit \"\"\" if dt_pred is None : dt_pred = self . reg_dates if isinstance ( x_pred , pd . Series ): x_pred = x_pred . values self . ensemble_member_to_preds = get_ensemble_preds ( self . ensemble_member_to_models , x_pred = x_pred ) self . pred_weights = np . array ( list ( construct_dt_weights ( dt_pred , self . reg_dates ) . values ())) with np . errstate ( divide = 'ignore' , invalid = 'ignore' ): self . pred_weights = self . pred_weights / self . pred_weights . sum ( axis = 0 ) self . pred_values = np . array ( list ( self . ensemble_member_to_preds . values ())) y_pred = np . dot ( self . pred_weights . T , self . pred_values ) if return_df == True : df_pred = pd . DataFrame ( y_pred , index = dt_pred , columns = x_pred ) . T return df_pred else : return y_pred %% time # Fitting reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) # Prediction x_pred = np . linspace ( 8 , 60 , 53 ) df_pred = smooth_dates . predict ( x_pred = x_pred ) df_pred . head () Wall time: 206 ms Unnamed: 0 8.0 9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0 ... 51.0 52.0 53.0 54.0 55.0 56.0 57.0 58.0 59.0 60.0 2009-01-04 -25.1979 -19.6978 -14.1985 -8.70185 -3.20799 2.27958 7.7668 13.2571 18.7712 24.3397 ... 191.52 199.994 208.405 216.829 225.204 233.6 241.959 250.322 258.639 267.002 2009-04-05 -35.8976 -28.4311 -20.9662 -13.5091 -6.05717 1.38257 8.82444 16.2744 23.7693 31.3579 ... 241.977 251.959 261.86 271.777 281.632 291.514 301.353 311.198 320.989 330.837 2009-07-05 -36.7886 -28.2248 -19.6637 -11.1177 -2.58043 5.93604 14.4576 22.9934 31.6014 40.3522 ... 267.246 277.279 287.219 297.171 307.058 316.974 326.848 336.73 346.558 356.447 2009-10-04 -18.8748 -10.7551 -2.63899 5.45781 13.5435 21.6034 29.6715 37.7625 45.9473 54.3142 ... 258.998 267.471 275.842 284.219 292.535 300.88 309.191 317.513 325.788 334.121 2010-01-03 10.9547 17.854 24.7496 31.627 38.4928 45.3325 52.1798 59.0516 66.0185 73.172 ... 244.872 251.738 258.495 265.25 271.951 278.678 285.379 292.095 298.771 305.502 We'll visualise our surface estimate as a wire-plot, where the darker colours denote price curve estimates from longer ago. fig , ax = plt . subplots ( dpi = 150 ) df_pred . T . plot ( legend = False , cmap = 'viridis' , linewidth = 1 , ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) ax . set_xlim ( df_pred . columns [ 0 ]) ax . set_ylim ( 0 , 400 ) (0.0, 400.0) Whilst SmoothDates accepts time-series of dispatchable generation and price as inputs to the fit method, predict doesn't accept a time-series of dispatchable generation or return a time-series of price estimates. Instead, predict returns a dataframe of the smoothed surface - unfortunately this is not what we need if we want to interface our work with the wider Python eco-system for sklearn based models. We'll create a further wrapper on top of SmoothDates that will accept a time-series of dispatchable generation and return the price estimate when the predict method is used. This will later be used for hyper-parameter tuning in, but could also be interfaced with tools such as TPOT for automated pipeline generation (perhaps the MOE estimate could be ensembled as an input to an ML model?). #exports def construct_pred_ts ( s , df_pred , rounding_dec = 1 ): \"\"\"Uses the time-adaptive LOWESS surface to generate time-series prediction\"\"\" vals = [] for dt_idx , val in track ( s . iteritems (), total = s . size ): vals += [ df_pred . loc [ round ( val , rounding_dec ), dt_idx . strftime ( '%Y-%m- %d ' )]] s_pred_ts = pd . Series ( vals , index = s . index ) return s_pred_ts class LowessDates ( BaseEstimator , RegressorMixin ): \"\"\" This class provides a time-adaptive extension of the classical Locally Weighted Scatterplot Smoothing regression technique, including robustifying procedures against outliers. Initialisation Parameters: frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function Attributes: fitted: Boolean flag indicating whether the model has been fitted frac: Fraction of the dataset to use in each local regression threshold_value: Number of datetime units to use in each regression threshold_units: Datetime unit which should be compatible with pandas `date_range` function ensemble_member_to_weights: Mapping from the regression dates to their respective weightings for each data-point ensemble_member_to_models: Mapping from the regression dates to their localised models reg_dates: Dates at which the local time-adaptive models will be centered around ensemble_member_to_preds: Mapping from the regression dates to their predictions reg_weights: Mapping from the prediction values to the weighting of each time-adaptive model reg_values: Predictions from each regression df_reg: A DataFrame of the time-adaptive surfce regression \"\"\" def __init__ ( self , frac = 0.3 , threshold_value = 52 , threshold_units = 'W' , pred_reg_dates = None ): self . fitted = False self . frac = frac self . threshold_value = threshold_value self . threshold_units = threshold_units self . pred_reg_dates = pred_reg_dates def fit ( self , x , y , dt_idx = None , reg_dates = None , lowess_kwargs = {}, ** fit_kwargs ): \"\"\" Calculation of the local regression coefficients for each of the LOWESS models across the dataset provided. This is a time-adaptive ensembled version of the `Lowess` model. Parameters: x: Values for the independent variable y: Values for the dependent variable dt_idx: Datetime index, if not provided the index of the x and y series will be used reg_dates: Dates at which the local time-adaptive models will be centered around lowess_kwargs: Additional arguments to be passed at model initialisation reg_anchors: Locations at which to center the local regressions num_fits: Number of locations at which to carry out a local regression external_weights: Further weighting for the specific regression robust_weights: Robustifying weights to remove the influence of outliers robust_iters: Number of robustifying iterations to carry out \"\"\" x , y , dt_idx , reg_dates = process_smooth_dates_fit_inputs ( x , y , dt_idx , reg_dates ) self . ensemble_member_to_weights = construct_dt_weights ( dt_idx , reg_dates , threshold_value = self . threshold_value , threshold_units = self . threshold_units ) self . ensemble_member_to_models = fit_external_weighted_ensemble ( x , y , self . ensemble_member_to_weights , lowess_kwargs = lowess_kwargs , frac = self . frac , ** fit_kwargs ) self . reg_dates = reg_dates self . fitted = True return def predict ( self , x_pred , reg_x = None , reg_dates = None , return_df = True , rounding_dec = 1 ): \"\"\" Inference using the design matrix from the time-adaptive LOWESS fits Parameters: x_pred: Locations for the time-adaptive LOWESS inference Returns: y_pred: Estimated values using the time-adaptive LOWESS fit \"\"\" reg_dates = self . pred_reg_dates if reg_x is None : reg_x = np . round ( np . arange ( np . floor ( x_pred . min ()) - 5 , np . ceil ( x_pred . max ()) + 5 , 1 / ( 10 ** rounding_dec )), rounding_dec ) x_pred = x_pred . round ( rounding_dec ) if isinstance ( reg_x , pd . Series ): reg_x = reg_x . values # Fitting the smoothed regression self . ensemble_member_to_preds = get_ensemble_preds ( self . ensemble_member_to_models , x_pred = reg_x ) self . reg_weights = np . array ( list ( construct_dt_weights ( reg_dates , self . reg_dates ) . values ())) self . reg_weights = self . reg_weights / self . reg_weights . sum ( axis = 0 ) self . reg_values = np . array ( list ( self . ensemble_member_to_preds . values ())) y_reg = np . dot ( self . reg_weights . T , self . reg_values ) self . df_reg = pd . DataFrame ( y_reg , index = reg_dates . strftime ( '%Y-%m- %d ' ), columns = reg_x ) . T # Making the prediction s_pred_ts = construct_pred_ts ( x_pred , self . df_reg , rounding_dec = rounding_dec ) return s_pred_ts","title":"External Weights"},{"location":"dev-04-price-surface-estimation/","text":"Estimation of Price Surfaces \u00b6 This notebook outlines how to specify different variants the model, then proceeds to fit them. Imports \u00b6 #exports import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import os import pickle import FEAutils as hlp from ipypb import track from moepy import lowess , eda User Inputs \u00b6 models_dir = '../data/models' load_existing_model = True Loading & Cleaning Data \u00b6 We'll start by loading in ... %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.69 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 ... and cleaning the GB data df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_demand = df_EI_model [ 'demand' ] s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-09' : '2011-03' ], s_price [ '2010-09' : '2011-03' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) hlp . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') As well as the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_demand = df_DE_model [ 'demand' ] s_DE_price = df_DE_model [ 'price' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_dispatchable [ '2015-09' : '2016-03' ], s_DE_price [ '2015-09' : '2016-03' ], s = 1 ) ax . scatter ( s_DE_dispatchable [ '2020-03' : '2020-09' ], s_DE_price [ '2020-03' : '2020-09' ], s = 1 ) hlp . hide_spines ( ax ) ax . set_xlim ( 8 , 75 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Results Wrapper \u00b6 We'll start defining each of the price models that we'll fit, using the PicklableFunction class to ensure that all of our models can be saved for later use. #exports import copy import types import marshal class PicklableFunction : \"\"\"Provides a wrapper to ensure functions can be pickled\"\"\" def __init__ ( self , fun ): self . _fun = fun def __call__ ( self , * args , ** kwargs ): return self . _fun ( * args , ** kwargs ) def __getstate__ ( self ): try : return pickle . dumps ( self . _fun ) except Exception : return marshal . dumps (( self . _fun . __code__ , self . _fun . __name__ )) def __setstate__ ( self , state ): try : self . _fun = pickle . loads ( state ) except Exception : code , name = marshal . loads ( state ) self . _fun = types . FunctionType ( code , {}, name ) return def get_fit_kwarg_sets ( qs = np . linspace ( 0.1 , 0.9 , 9 )): \"\"\"Helper to generate kwargs for the `fit` method of `Lowess`\"\"\" fit_kwarg_sets = [ # quantile lowess { 'name' : f 'p { int ( q * 100 ) } ' , 'lowess_kwargs' : { 'reg_func' : PicklableFunction ( lowess . calc_quant_reg_betas )}, 'q' : q , } for q in qs # standard lowess ] + [{ 'name' : 'average' }] return fit_kwarg_sets model_definitions = { 'DAM_price_GB' : { 'dt_idx' : s_dispatchable . index , 'x' : s_dispatchable . values , 'y' : s_price . values , 'reg_dates_start' : '2009-01-01' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'DAM_price_demand_GB' : { 'dt_idx' : s_demand . index , 'x' : s_demand . values , 'y' : s_price . values , 'reg_dates_start' : '2009-01-01' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.5 ]) }, 'DAM_price_DE' : { 'dt_idx' : s_DE_dispatchable . index , 'x' : s_DE_dispatchable . values , 'y' : s_DE_price . values , 'reg_dates_start' : '2015-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'DAM_price_demand_DE' : { 'dt_idx' : s_DE_dispatchable . index , 'x' : s_DE_demand . values , 'y' : s_DE_price . values , 'reg_dates_start' : '2015-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.5 ]) } } We'll now take these model definitions to fit and save them #exports def fit_models ( model_definitions , models_dir ): \"\"\"Fits LOWESS variants using the specified model definitions\"\"\" for model_parent_name , model_spec in model_definitions . items (): for fit_kwarg_set in track ( model_spec [ 'fit_kwarg_sets' ], label = model_parent_name ): run_name = fit_kwarg_set . pop ( 'name' ) model_name = f ' { model_parent_name } _ { run_name } ' if f ' { model_name } .pkl' not in os . listdir ( models_dir ): smooth_dates = lowess . SmoothDates () reg_dates = pd . date_range ( model_spec [ 'reg_dates_start' ], model_spec [ 'reg_dates_end' ], freq = model_spec [ 'reg_dates_freq' ] ) smooth_dates . fit ( model_spec [ 'x' ], model_spec [ 'y' ], dt_idx = model_spec [ 'dt_idx' ], reg_dates = reg_dates , frac = model_spec [ 'frac' ], threshold_value = model_spec [ 'dates_smoothing_value' ], threshold_units = model_spec [ 'dates_smoothing_units' ], num_fits = model_spec [ 'num_fits' ], ** fit_kwarg_set ) model_fp = f ' { models_dir } / { model_name } .pkl' pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) del smooth_dates fit_models ( model_definitions , models_dir ) DAM_price_GB 100% 4/4 [00:00 < 00:00, 0.00s/it] DAM_price_demand_GB 100% 2/2 [00:00 < 00:00, 0.00s/it] DAM_price_DE 100% 4/4 [00:00 < 00:00, 0.00s/it] DAM_price_demand_DE 100% 2/2 [00:00 < 00:00, 0.00s/it] We'll load one of the models in %% time if load_existing_model == True : smooth_dates = pickle . load ( open ( f ' { models_dir } /DAM_price_GB_p50.pkl' , 'rb' )) else : lowess_kwargs = {} reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 , lowess_kwargs = lowess_kwargs ) Wall time: 2.7 s And create a prediction surface using it %% time x_pred = np . linspace ( 8 , 60 , 521 ) dt_pred = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '1W' ) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . head () Wall time: 346 ms Unnamed: 0 2009-01-04 2009-01-11 2009-01-18 2009-01-25 2009-02-01 2009-02-08 2009-02-15 2009-02-22 2009-03-01 2009-03-08 ... 2020-10-25 2020-11-01 2020-11-08 2020-11-15 2020-11-22 2020-11-29 2020-12-06 2020-12-13 2020-12-20 2020-12-27 8 -7.66001 -7.78927 -7.91081 -8.02572 -8.13481 -8.23875 -8.33813 -8.43345 -8.52519 -8.61382 ... 10.2354 10.292 10.3476 10.4021 10.4557 10.5085 10.5611 10.6143 10.6691 10.7271 8.1 -7.46772 -7.59637 -7.71734 -7.83171 -7.94028 -8.04374 -8.14266 -8.23754 -8.32887 -8.41709 ... 10.4429 10.4994 10.5548 10.6092 10.6627 10.7154 10.7679 10.821 10.8758 10.9336 8.2 -7.27561 -7.40364 -7.52404 -7.63785 -7.74592 -7.84889 -7.94734 -8.04178 -8.13268 -8.22049 ... 10.6503 10.7066 10.7619 10.8162 10.8695 10.9222 10.9746 11.0276 11.0823 11.1401 8.3 -7.08366 -7.21108 -7.33089 -7.44416 -7.5517 -7.65418 -7.75217 -7.84616 -7.93663 -8.02403 ... 10.8576 10.9138 10.9689 11.023 11.0763 11.1288 11.1812 11.2341 11.2888 11.3464 8.4 -6.89188 -7.01867 -7.13789 -7.25061 -7.35763 -7.45962 -7.55713 -7.65067 -7.74071 -7.82769 ... 11.0648 11.1208 11.1757 11.2298 11.2829 11.3353 11.3876 11.4405 11.4951 11.5527","title":"Price Curve Estimation"},{"location":"dev-04-price-surface-estimation/#estimation-of-price-surfaces","text":"This notebook outlines how to specify different variants the model, then proceeds to fit them.","title":"Estimation of Price Surfaces"},{"location":"dev-04-price-surface-estimation/#imports","text":"#exports import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import os import pickle import FEAutils as hlp from ipypb import track from moepy import lowess , eda","title":"Imports"},{"location":"dev-04-price-surface-estimation/#user-inputs","text":"models_dir = '../data/models' load_existing_model = True","title":"User Inputs"},{"location":"dev-04-price-surface-estimation/#loading-cleaning-data","text":"We'll start by loading in ... %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.69 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 ... and cleaning the GB data df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_demand = df_EI_model [ 'demand' ] s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-09' : '2011-03' ], s_price [ '2010-09' : '2011-03' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) hlp . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') As well as the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_demand = df_DE_model [ 'demand' ] s_DE_price = df_DE_model [ 'price' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_dispatchable [ '2015-09' : '2016-03' ], s_DE_price [ '2015-09' : '2016-03' ], s = 1 ) ax . scatter ( s_DE_dispatchable [ '2020-03' : '2020-09' ], s_DE_price [ '2020-03' : '2020-09' ], s = 1 ) hlp . hide_spines ( ax ) ax . set_xlim ( 8 , 75 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (MW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)')","title":"Loading &amp; Cleaning Data"},{"location":"dev-04-price-surface-estimation/#results-wrapper","text":"We'll start defining each of the price models that we'll fit, using the PicklableFunction class to ensure that all of our models can be saved for later use. #exports import copy import types import marshal class PicklableFunction : \"\"\"Provides a wrapper to ensure functions can be pickled\"\"\" def __init__ ( self , fun ): self . _fun = fun def __call__ ( self , * args , ** kwargs ): return self . _fun ( * args , ** kwargs ) def __getstate__ ( self ): try : return pickle . dumps ( self . _fun ) except Exception : return marshal . dumps (( self . _fun . __code__ , self . _fun . __name__ )) def __setstate__ ( self , state ): try : self . _fun = pickle . loads ( state ) except Exception : code , name = marshal . loads ( state ) self . _fun = types . FunctionType ( code , {}, name ) return def get_fit_kwarg_sets ( qs = np . linspace ( 0.1 , 0.9 , 9 )): \"\"\"Helper to generate kwargs for the `fit` method of `Lowess`\"\"\" fit_kwarg_sets = [ # quantile lowess { 'name' : f 'p { int ( q * 100 ) } ' , 'lowess_kwargs' : { 'reg_func' : PicklableFunction ( lowess . calc_quant_reg_betas )}, 'q' : q , } for q in qs # standard lowess ] + [{ 'name' : 'average' }] return fit_kwarg_sets model_definitions = { 'DAM_price_GB' : { 'dt_idx' : s_dispatchable . index , 'x' : s_dispatchable . values , 'y' : s_price . values , 'reg_dates_start' : '2009-01-01' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'DAM_price_demand_GB' : { 'dt_idx' : s_demand . index , 'x' : s_demand . values , 'y' : s_price . values , 'reg_dates_start' : '2009-01-01' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.5 ]) }, 'DAM_price_DE' : { 'dt_idx' : s_DE_dispatchable . index , 'x' : s_DE_dispatchable . values , 'y' : s_DE_price . values , 'reg_dates_start' : '2015-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'DAM_price_demand_DE' : { 'dt_idx' : s_DE_dispatchable . index , 'x' : s_DE_demand . values , 'y' : s_DE_price . values , 'reg_dates_start' : '2015-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , # 13 'frac' : 0.3 , 'num_fits' : 31 , # 31 'dates_smoothing_value' : 26 , # 26 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : get_fit_kwarg_sets ( qs = [ 0.5 ]) } } We'll now take these model definitions to fit and save them #exports def fit_models ( model_definitions , models_dir ): \"\"\"Fits LOWESS variants using the specified model definitions\"\"\" for model_parent_name , model_spec in model_definitions . items (): for fit_kwarg_set in track ( model_spec [ 'fit_kwarg_sets' ], label = model_parent_name ): run_name = fit_kwarg_set . pop ( 'name' ) model_name = f ' { model_parent_name } _ { run_name } ' if f ' { model_name } .pkl' not in os . listdir ( models_dir ): smooth_dates = lowess . SmoothDates () reg_dates = pd . date_range ( model_spec [ 'reg_dates_start' ], model_spec [ 'reg_dates_end' ], freq = model_spec [ 'reg_dates_freq' ] ) smooth_dates . fit ( model_spec [ 'x' ], model_spec [ 'y' ], dt_idx = model_spec [ 'dt_idx' ], reg_dates = reg_dates , frac = model_spec [ 'frac' ], threshold_value = model_spec [ 'dates_smoothing_value' ], threshold_units = model_spec [ 'dates_smoothing_units' ], num_fits = model_spec [ 'num_fits' ], ** fit_kwarg_set ) model_fp = f ' { models_dir } / { model_name } .pkl' pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) del smooth_dates fit_models ( model_definitions , models_dir ) DAM_price_GB 100% 4/4 [00:00 < 00:00, 0.00s/it] DAM_price_demand_GB 100% 2/2 [00:00 < 00:00, 0.00s/it] DAM_price_DE 100% 4/4 [00:00 < 00:00, 0.00s/it] DAM_price_demand_DE 100% 2/2 [00:00 < 00:00, 0.00s/it] We'll load one of the models in %% time if load_existing_model == True : smooth_dates = pickle . load ( open ( f ' { models_dir } /DAM_price_GB_p50.pkl' , 'rb' )) else : lowess_kwargs = {} reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 , lowess_kwargs = lowess_kwargs ) Wall time: 2.7 s And create a prediction surface using it %% time x_pred = np . linspace ( 8 , 60 , 521 ) dt_pred = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '1W' ) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . head () Wall time: 346 ms Unnamed: 0 2009-01-04 2009-01-11 2009-01-18 2009-01-25 2009-02-01 2009-02-08 2009-02-15 2009-02-22 2009-03-01 2009-03-08 ... 2020-10-25 2020-11-01 2020-11-08 2020-11-15 2020-11-22 2020-11-29 2020-12-06 2020-12-13 2020-12-20 2020-12-27 8 -7.66001 -7.78927 -7.91081 -8.02572 -8.13481 -8.23875 -8.33813 -8.43345 -8.52519 -8.61382 ... 10.2354 10.292 10.3476 10.4021 10.4557 10.5085 10.5611 10.6143 10.6691 10.7271 8.1 -7.46772 -7.59637 -7.71734 -7.83171 -7.94028 -8.04374 -8.14266 -8.23754 -8.32887 -8.41709 ... 10.4429 10.4994 10.5548 10.6092 10.6627 10.7154 10.7679 10.821 10.8758 10.9336 8.2 -7.27561 -7.40364 -7.52404 -7.63785 -7.74592 -7.84889 -7.94734 -8.04178 -8.13268 -8.22049 ... 10.6503 10.7066 10.7619 10.8162 10.8695 10.9222 10.9746 11.0276 11.0823 11.1401 8.3 -7.08366 -7.21108 -7.33089 -7.44416 -7.5517 -7.65418 -7.75217 -7.84616 -7.93663 -8.02403 ... 10.8576 10.9138 10.9689 11.023 11.0763 11.1288 11.1812 11.2341 11.2888 11.3464 8.4 -6.89188 -7.01867 -7.13789 -7.25061 -7.35763 -7.45962 -7.55713 -7.65067 -7.74071 -7.82769 ... 11.0648 11.1208 11.1757 11.2298 11.2829 11.3353 11.3876 11.4405 11.4951 11.5527","title":"Results Wrapper"},{"location":"dev-05-price-moe/","text":"Price Merit Order Effect Analysis \u00b6 This notebook outlines the analysis required to determine the price merit-order-effect of variable renewable generation in the GB and DE power markets. Imports \u00b6 #exports import json import pandas as pd import numpy as np import pickle import scipy from sklearn import linear_model from sklearn.metrics import r2_score from collections.abc import Iterable import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.dates as mdates from ipypb import track from IPython.display import JSON from moepy import lowess , eda from moepy.surface import PicklableFunction User Inputs \u00b6 GB_model_fp = '../data/models/DAM_price_GB_p50.pkl' DE_model_fp = '../data/models/DAM_price_DE_p50.pkl' load_existing_GB_model = True load_existing_DE_model = True Estimating the Price Surface \u00b6 We'll start by loading in the data %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.76 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll do a quick plot of the average price fig , ax = plt . subplots ( dpi = 150 ) df_EI [ 'day_ahead_price' ] . resample ( '4W' ) . mean () . plot ( ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Day-Ahead Price \\n Monthly Average (\u00a3/MWh)' ) Text(0, 0.5, 'Day-Ahead Price\\nMonthly Average (\u00a3/MWh)') We'll also visualise individual half-hour periods for two different date ranges in the dataset df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-03' : '2010-09' ], s_price [ '2010-03' : '2010-09' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Next we'll load in (or fit) one of the models trained in the previous notebook %% time if load_existing_GB_model == True : smooth_dates = pickle . load ( open ( GB_model_fp , 'rb' )) else : reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) Wall time: 2.42 s We're now ready to make our price curve predictions, we'll make one for each day that the model used in training %% time x_pred = np . linspace ( 3 , 61 , 581 ) dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' ) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . index = np . round ( df_pred . index , 1 ) df_pred . head () Wall time: 469 ms Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 3 -17.4948 -17.5178 -17.5406 -17.5632 -17.5856 -17.6077 -17.6297 -17.6514 -17.673 -17.6943 ... 0.194247 0.203242 0.212319 0.221479 0.230719 0.240041 0.249444 0.258926 0.268489 0.278132 3.1 -17.2964 -17.3193 -17.3421 -17.3646 -17.3869 -17.409 -17.4309 -17.4525 -17.474 -17.4953 ... 0.404498 0.413478 0.42254 0.431683 0.440907 0.450212 0.459598 0.469064 0.478609 0.488234 3.2 -17.0983 -17.1212 -17.1439 -17.1663 -17.1886 -17.2106 -17.2324 -17.254 -17.2754 -17.2966 ... 0.614692 0.623656 0.632702 0.641829 0.651037 0.660326 0.669695 0.679143 0.688671 0.698278 3.3 -16.9006 -16.9234 -16.946 -16.9684 -16.9905 -17.0125 -17.0342 -17.0558 -17.0771 -17.0983 ... 0.824815 0.833764 0.842794 0.851906 0.861097 0.87037 0.879722 0.889153 0.898664 0.908253 3.4 -16.7031 -16.7259 -16.7484 -16.7707 -16.7928 -16.8147 -16.8364 -16.8579 -16.8791 -16.9002 ... 1.03486 1.04379 1.0528 1.0619 1.07108 1.08033 1.08967 1.09908 1.10858 1.11815 Visualising the Price Surface \u00b6 We now want to actually visualise the results of our model, in particular the price surface it's fitted. We'll start by plotting all of the price curves overlapping each other. cmap = plt . get_cmap ( 'viridis' ) cbar_ticks = [ 0 , 0.25 , 0.5 , 0.75 , 1 ] # Plotting fig , ax = plt . subplots ( dpi = 250 ) lp = df_pred . loc [:, :: 7 ] . plot ( legend = False , cmap = cmap , linewidth = 0.25 , ax = ax ) cax = fig . add_axes ([ 0.9 , 0.2 , 0.03 , 0.55 ]) cbar = mpl . colorbar . ColorbarBase ( cax , orientation = 'vertical' , cmap = cmap , ticks = cbar_ticks ) cbar . ax . set_yticklabels ([ dt_pred [ min ( int ( len ( dt_pred ) * tick_loc ), len ( dt_pred ) - 1 )] . strftime ( '%b %Y' ) for tick_loc in cbar_ticks ]) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) ax . set_xlim ( df_pred . index [ 0 ]) ax . set_ylim ( 0 , 75 ) ax . set_title ( 'Day-Ahead Market Average Price Curve' ) Text(0.5, 1.0, 'Day-Ahead Market Average Price Curve') Whilst the previous plot might look quite nice it's rather difficult to interpret, an alternative way to visualise how the price curve evolves over time is using a heatmap #exports def construct_dispatchable_lims_df ( s_dispatchable , rolling_w = 3 , daily_quantiles = [ 0.001 , 0.999 ]): \"\"\"Identifies the rolling limits to be used in masking\"\"\" df_dispatchable_lims = ( s_dispatchable . resample ( '1d' ) . quantile ( daily_quantiles ) . unstack () . rolling ( rolling_w * 7 ) . mean () . bfill () . ffill () . iloc [: - 1 , :] ) df_dispatchable_lims . index = pd . to_datetime ( df_dispatchable_lims . index . strftime ( '%Y-%m- %d ' )) return df_dispatchable_lims def construct_pred_mask_df ( df_pred , df_dispatchable_lims ): \"\"\"Constructs a DataFrame mask for the prediction\"\"\" df_pred = df_pred [ df_dispatchable_lims . index ] df_pred_mask = pd . DataFrame ( dict ( zip ( df_pred . columns , [ df_pred . index ] * df_pred . shape [ 1 ])), index = df_pred . index ) df_pred_mask = ( df_pred_mask > df_dispatchable_lims . iloc [:, 0 ] . values ) & ( df_pred_mask < df_dispatchable_lims . iloc [:, 1 ] . values ) df_pred . columns = pd . to_datetime ( df_pred . columns ) df_pred_mask . columns = pd . to_datetime ( df_pred_mask . columns ) return df_pred_mask df_dispatchable_lims = construct_dispatchable_lims_df ( s_dispatchable ) df_pred_mask = construct_pred_mask_df ( df_pred , df_dispatchable_lims ) sns . heatmap ( df_pred . where ( df_pred_mask , np . nan ) . iloc [:: - 1 ]) <AxesSubplot:> The default output for seaborn heatmaps never looks great when one of the axis is a datetime, we'll write a custom class and wrapper to handle this #exports class AxTransformer : \"\"\"Helper class for cleaning axis tick locations and labels\"\"\" def __init__ ( self , datetime_vals = False ): self . datetime_vals = datetime_vals self . lr = linear_model . LinearRegression () return def process_tick_vals ( self , tick_vals ): if not isinstance ( tick_vals , Iterable ) or isinstance ( tick_vals , str ): tick_vals = [ tick_vals ] if self . datetime_vals == True : tick_vals = pd . to_datetime ( tick_vals ) . astype ( int ) . values tick_vals = np . array ( tick_vals ) return tick_vals def fit ( self , ax , axis = 'x' ): axis = getattr ( ax , f 'get_ { axis } axis' )() tick_locs = axis . get_ticklocs () tick_vals = self . process_tick_vals ([ label . _text for label in axis . get_ticklabels ()]) self . lr . fit ( tick_vals . reshape ( - 1 , 1 ), tick_locs ) return def transform ( self , tick_vals ): tick_vals = self . process_tick_vals ( tick_vals ) tick_locs = self . lr . predict ( np . array ( tick_vals ) . reshape ( - 1 , 1 )) return tick_locs def set_ticks ( ax , tick_locs , tick_labels = None , axis = 'y' ): \"\"\"Sets ticks at standard numerical locations\"\"\" if tick_labels is None : tick_labels = tick_locs ax_transformer = AxTransformer () ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( tick_locs )) getattr ( ax , f 'set_ { axis } ticklabels' )( tick_labels ) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax def set_date_ticks ( ax , start_date , end_date , axis = 'y' , date_format = '%Y-%m- %d ' , ** date_range_kwargs ): \"\"\"Sets ticks at datetime locations\"\"\" dt_rng = pd . date_range ( start_date , end_date , ** date_range_kwargs ) ax_transformer = AxTransformer ( datetime_vals = True ) ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( dt_rng )) getattr ( ax , f 'set_ { axis } ticklabels' )( dt_rng . strftime ( date_format )) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax %% time fig , ax = plt . subplots ( dpi = 150 , figsize = ( 10 , 6 )) htmp = sns . heatmap ( df_pred [ 10 : 60 ] . where ( df_pred_mask [ 10 : 60 ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Price (\u00a3/MWh)' }) set_ticks ( ax , np . arange ( 0 , 70 , 10 ), axis = 'y' ) set_date_ticks ( ax , '2009-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) Wall time: 1.79 s C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(144.58333333333331, 0.5, 'Demand - [Solar + Wind] (GW)') We also want to visualise specific date ranges, here we'll look towards the end of 2020 %% time center_date = '2020-12-01' dt_min = pd . to_datetime ( center_date ) - pd . Timedelta ( weeks = 4 ) dt_max = pd . to_datetime ( center_date ) + pd . Timedelta ( weeks = 4 ) x = s_dispatchable [ dt_min : dt_max ] . values y = s_price . loc [ s_dispatchable . index ][ dt_min : dt_max ] . values x_pred = np . linspace ( 11 , 40 , 41 ) y_pred = lowess . lowess_fit_and_predict ( x , y , frac = 0.6 , num_fits = 25 , x_pred = x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , linewidth = 1.5 , color = 'r' ) ax . scatter ( x , y , color = 'k' , s = 1 , alpha = 0.2 ) ax . set_title ( f 'November & December 2020' ) # remove in the LaTeX plot and just state in the caption ax . set_xlim ( 11 , 40 ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) Wall time: 58.1 ms C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) Text(0, 0.5, 'Day-Ahead Price (\u00a3/MWh)') Evaluating the Price Curve Predictions \u00b6 #exports def construct_df_pred ( model_fp , x_pred = np . linspace ( - 2 , 61 , 631 ), dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' )): \"\"\"Constructs the prediction surface for the specified pre-fitted model\"\"\" smooth_dates = pickle . load ( open ( model_fp , 'rb' )) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . index = np . round ( df_pred . index , 1 ) return df_pred model_fp = '../data/models/DAM_price_average.pkl' df_pred = construct_df_pred ( model_fp ) df_pred . head () Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -2 -28.4122 -28.4306 -28.4488 -28.4668 -28.4846 -28.5022 -28.5197 -28.5369 -28.554 -28.5708 ... -17.5465 -17.5399 -17.5331 -17.5263 -17.5194 -17.5125 -17.5054 -17.4982 -17.4909 -17.4836 -1.9 -28.2171 -28.2354 -28.2536 -28.2715 -28.2893 -28.3068 -28.3242 -28.3414 -28.3584 -28.3752 ... -17.2979 -17.2913 -17.2846 -17.2778 -17.2709 -17.2639 -17.2568 -17.2496 -17.2424 -17.235 -1.8 -28.022 -28.0403 -28.0583 -28.0762 -28.0939 -28.1115 -28.1288 -28.1459 -28.1629 -28.1796 ... -17.0494 -17.0427 -17.036 -17.0292 -17.0223 -17.0153 -17.0082 -17.001 -16.9938 -16.9864 -1.7 -27.8269 -27.8451 -27.8631 -27.881 -27.8986 -27.9161 -27.9334 -27.9504 -27.9673 -27.984 ... -16.8008 -16.7941 -16.7874 -16.7806 -16.7737 -16.7667 -16.7596 -16.7524 -16.7451 -16.7378 -1.6 -27.6318 -27.65 -27.6679 -27.6857 -27.7033 -27.7207 -27.7379 -27.7549 -27.7718 -27.7884 ... -16.5522 -16.5455 -16.5388 -16.532 -16.5251 -16.5181 -16.511 -16.5038 -16.4965 -16.4891 Now we've created our prediction dataframe we can calculate a time-series for our price prediction N.b. to speed things up every 100th half-hour has been sampled rather than using the full dataset #exports def construct_pred_ts ( s , df_pred ): \"\"\"Uses the time-adaptive LOWESS surface to generate time-series prediction\"\"\" s_pred_ts = pd . Series ( index = s . index , dtype = 'float64' ) for dt_idx , val in track ( s . iteritems (), total = s . size ): s_pred_ts . loc [ dt_idx ] = df_pred . loc [ round ( val , 1 ), dt_idx . strftime ( '%Y-%m- %d ' )] return s_pred_ts s_dispatchable = ( df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 )) . dropna () . loc [: df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts = construct_pred_ts ( s_dispatchable . iloc [:: 100 ] . iloc [: - 1 ], df_pred ) s_pred_ts . head () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 1600/2095 [00:02 < 00:00, 0.00s/it] local_datetime 2009-01-01 00:00:00+00:00 39.026220 2009-01-03 02:00:00+00:00 34.314689 2009-01-05 04:00:00+00:00 32.502689 2009-01-07 06:00:00+00:00 42.705269 2009-01-09 08:00:00+00:00 64.025113 dtype: float64 We'll quickly inspect the error distribution s_price = df_EI [ 'day_ahead_price' ] s_err = s_pred_ts - s_price . loc [ s_pred_ts . index ] print ( s_err . abs () . mean ()) sns . histplot ( s_err ) _ = plt . xlim ( - 75 , 75 ) 6.8625170828105615 We'll quickly calculate the r2 score for this model, for context the UK day-ahead price multiple linear regression model from the Staffell/Green group achieved an adjusted r2 of 0.451 r2_score ( s_price . loc [ s_pred_ts . index ], s_pred_ts ) 0.46527623686663677 Now we'll calculate some error metrics, including the option to remove extreme error outliers #exports def calc_error_metrics ( s_err , max_err_quantile = 1 ): \"\"\"Calculates several error metrics using the passed error series\"\"\" if s_err . isnull () . sum () > 0 : s_err = s_err . dropna () max_err_cutoff = s_err . abs () . quantile ( max_err_quantile ) s_err = s_err [ s_err . abs () <= max_err_cutoff ] metrics = { 'median_abs_err' : s_err . abs () . median (), 'mean_abs_err' : s_err . abs () . mean (), 'root_mean_square_error' : np . sqrt (( s_err ** 2 ) . mean ()) } return metrics metrics = calc_error_metrics ( s_err ) metrics {'median_abs_err': 4.843738852384298, 'mean_abs_err': 6.8625170828105615, 'root_mean_square_error': 12.301327313943641} We'll now create a wrapper for the last few steps and repeat the analysis for four variants of the model #exports def get_model_pred_ts ( s , model_fp , s_demand = None , x_pred = np . linspace ( - 2 , 61 , 631 ), dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' )): \"\"\"Constructs the time-series prediction for the specified pre-fitted model\"\"\" df_pred = construct_df_pred ( model_fp , x_pred = x_pred , dt_pred = dt_pred ) s_cleaned = s . dropna () . loc [ df_pred . columns . min (): df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts = construct_pred_ts ( s_cleaned , df_pred ) if s_demand is None : return s_pred_ts else : s_cleaned = s_demand . dropna () . loc [ df_pred . columns . min (): df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts_demand = construct_pred_ts ( s_cleaned , df_pred ) return s_pred_ts , s_pred_ts_demand s_demand = df_EI [ 'demand' ] s_price = df_EI [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) model_runs = { 'demand_p50' : { 'model_fp' : '../data/models/DAM_price_demand_p50.pkl' , 's' : s_demand }, 'demand_avg' : { 'model_fp' : '../data/models/DAM_price_demand_average.pkl' , 's' : s_demand }, 'dispatch_p50' : { 'model_fp' : '../data/models/DAM_price_p50.pkl' , 's' : s_dispatchable }, 'dispatch_avg' : { 'model_fp' : '../data/models/DAM_price_average.pkl' , 's' : s_dispatchable }, } model_outputs = dict () for model_name , model_kwargs in track ( model_runs . items ()): s_pred_ts = get_model_pred_ts ( ** model_kwargs ) s_err = s_pred_ts - s_price . loc [ s_pred_ts . index ] metrics = calc_error_metrics ( s_err ) model_outputs [ model_name ] = { 's_pred_ts' : s_pred_ts , 's_err' : s_err , 'metrics' : metrics } 100% 4/4 [11:27 < 02:57, 171.67s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 192924/209736 [02:23 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/209736 [02:26 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 165505/209533 [03:30 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/209533 [02:53 < 00:00, 0.00s/it] Overall, the dispatch model fitted to the median price appears to be the most accurate df_metrics = pd . DataFrame ({ name : outputs [ 'metrics' ] for name , outputs in model_outputs . items () }) df_metrics Unnamed: 0 demand_p50 demand_avg dispatch_p50 dispatch_avg median_abs_err 4.70534 5.04971 4.47311 4.77768 mean_abs_err 6.97489 7.09014 6.55688 6.6518 root_mean_square_error 12.6323 12.41 12.0539 11.8075 We'll quantify the difference against the standard LOWESS approach old_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_avg' ] new_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_p50' ] print ( f ' { - 100 * ( new_err - old_err ) / old_err : .2f } % mean absolute error reduction using the p50 model rather than the average model' ) 1.43% mean absolute error reduction using the p50 model rather than the average model And also look at the improvement in accuracy when regressing against dispatchable generation instead of total generation old_err = df_metrics . loc [ 'mean_abs_err' , 'demand_avg' ] new_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_avg' ] print ( f ' { - 100 * ( new_err - old_err ) / old_err : .2f } % mean absolute error reduction using the dispatchable demand model rather than just demand' ) 6.18% mean absolute error reduction using the dispatchable demand model rather than just demand Quantifying & Visualising the Merit Order Effect \u00b6 To begin we'll load up our dispatchable supply model and make an inference for each half-hour in the dataset, based on the results in the previous section we'll use the p50 model. %% time s_GB_demand = df_EI_model [ 'demand' ] s_GB_price = df_EI_model [ 'day_ahead_price' ] s_GB_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) s_GB_pred_ts_dispatch , s_GB_pred_ts_demand = get_model_pred_ts ( s_GB_dispatchable , GB_model_fp , s_demand = s_GB_demand ) s_dispatch_GB_err = s_GB_pred_ts_dispatch - s_GB_price . loc [ s_GB_pred_ts_dispatch . index ] GB_dispatch_metrics = calc_error_metrics ( s_dispatch_GB_err ) s_demand_GB_err = s_GB_pred_ts_demand - s_GB_price . loc [ s_GB_pred_ts_demand . index ] GB_demand_metrics = calc_error_metrics ( s_demand_GB_err ) s_GB_pred_ts_dispatch Wall time: 92.1 ms local_datetime 2009-01-01 00:00:00+00:00 37.203441 2009-01-01 00:30:00+00:00 37.313379 2009-01-01 01:00:00+00:00 36.768513 2009-01-01 01:30:00+00:00 35.595162 2009-01-01 02:00:00+00:00 34.849422 ... 2020-12-30 21:30:00+00:00 48.066638 2020-12-30 22:00:00+00:00 45.268069 2020-12-30 22:30:00+00:00 42.647597 2020-12-30 23:00:00+00:00 39.520118 2020-12-30 23:30:00+00:00 37.948852 Length: 209533, dtype: float64 The difference between the price forecast when using the dispatchable generation and total generation is due to the merit order effect s_GB_MOE = s_GB_pred_ts_demand - s_GB_pred_ts_dispatch s_GB_MOE = s_GB_MOE . dropna () s_GB_MOE . plot () <AxesSubplot:xlabel='local_datetime'> We'll quickly calculate the averages for 2010 and 2020 s_GB_MOE [ '2010' ] . mean (), s_GB_MOE [ '2020' ] . mean () (0.8813580281178922, 13.888630517942211) We'll also visualise the predictions for a sample day date = '2017-07-01' # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_GB_pred_ts_dispatch [ date ] . plot ( label = 'Prediction with RES' , ax = ax ) s_GB_pred_ts_demand [ date ] . plot ( label = 'Prediction without RES' , ax = ax ) s_GB_price [ date ] . plot ( label = 'Observed' , ax = ax ) ax . legend ( frameon = False , ncol = 3 , bbox_to_anchor = ( 1.075 , - 0.15 )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) eda . hide_spines ( ax ) We can see that there is a clear pattern in the bias over the course of the day s_GB_err . groupby ( s_GB_err . index . hour + s_GB_err . index . minute / 60 ) . mean () . plot . bar () <AxesSubplot:xlabel='local_datetime'> We'll now calculate the half-hourly savings due to the merit order effect. The demand is expressed in terms of GW which we'll convert into MWh to be compatible with the units of the merit order effect. s_GB_saving = s_GB_MOE * s_GB_demand . loc [ s_GB_MOE . index ] * 1000 * 0.5 s_GB_saving . resample ( '2W' ) . mean () . plot () <AxesSubplot:xlabel='local_datetime'> The distribution of the price savings appears to follow a rough Pareto distribution sns . histplot ( s_GB_saving ) plt . xlim ( 0 , 750000 ) eda . hide_spines ( plt . gca ()) We'll calculate the total savings over the dataset period start_date = '2010' end_date = '2020' total_saving = s_GB_saving [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was \u00a3 { total_saving : ,.0f } \" ) The total saving between 2010 and 2020 was \u00a317,454,468,143 To add some context we'll calculate the average half-hourly market volume (note that the DAM volume will be smaller than demand but it acts as a good proxy to cover all markets) s_mkt_cost = df_EI [ 'day_ahead_price' ] * df_EI [ 'demand' ] * 1000 * 0.5 avg_HH_mkt_cost = s_mkt_cost . mean () avg_HH_mkt_cost 784440.8559554707 We'll also calculate the long-term price reduction due to the merit order effect s_GB_MOE [ start_date : end_date ] . mean () / ( df_EI [ 'day_ahead_price' ] + s_GB_MOE )[ start_date : end_date ] . mean () 0.15034496268274428 This only tells part of the story though, lets look at how this value evolves over time s_GB_DAM = s_GB_price . loc [ s_GB_MOE . index ] s_GB_MOE_rolling = s_GB_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_GB_DAM_rolling = s_GB_DAM . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_pct_reduction = s_GB_MOE_rolling / s_GB_DAM_rolling s_GB_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> We're half-way to creating the metrics required for comparing between markets of different sizes, prices and RES penetration. We'll calculate the renewables penetration percentage and then aggregate it on an annual basis alongside the pct price reduction due to the MOE. %% time s_GB_MOE_pct_annual_avg = ( s_GB_MOE / ( df_EI . loc [ s_GB_MOE . index ][ 'day_ahead_price' ] + s_GB_MOE )) . replace ( np . inf , np . nan ) . dropna () . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) s_GB_RES_pct_annual_avg = ( df_EI [[ 'wind' , 'solar' ]] . sum ( axis = 1 ) / df_EI [ 'demand' ]) . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) # Plotting fig , ax = plt . subplots ( dpi = 250 ) sns . regplot ( x = 100 * s_GB_RES_pct_annual_avg , y = 100 * s_GB_MOE_pct_annual_avg , ax = ax , label = 'Incl. 2020 (m=0.86)' ) sns . regplot ( x = 100 * s_GB_RES_pct_annual_avg . loc [: 2019 ], y = 100 * s_GB_MOE_pct_annual_avg . loc [: 2019 ], truncate = False , ax = ax , label = 'Excl. 2020 (m=0.67)' ) ax . scatter ( x = 100 * s_GB_RES_pct_annual_avg , y = 100 * s_GB_MOE_pct_annual_avg , color = 'k' ) eda . hide_spines ( ax ) ax . legend ( frameon = False , loc = 'upper left' ) ax . set_ylim ( 0 ) ax . set_xlabel ( 'Average RES Penetration (%)' ) ax . set_ylabel ( 'Average MOE Price Reduction (%)' ) Wall time: 753 ms Text(0, 0.5, 'Average MOE Price Reduction (%)') We'll fit a linear regression that includes 2020 linreg_results = scipy . stats . linregress ( x = s_GB_RES_pct_annual_avg , y = s_GB_MOE_pct_annual_avg ) linreg_results LinregressResult(slope=0.8635390661954275, intercept=-0.01260770809733558, rvalue=0.9407770535093302, pvalue=5.192107447861512e-06, stderr=0.0984074783808127, intercept_stderr=0.01545140550162944) And one that excludes 2020 max_year = 2019 linreg_results = scipy . stats . linregress ( x = s_GB_RES_pct_annual_avg . loc [: max_year ], y = s_GB_MOE_pct_annual_avg . loc [: max_year ]) linreg_results LinregressResult(slope=0.6675133238057994, intercept=0.004850800918389769, rvalue=0.9588281998847612, pvalue=3.215977627236812e-06, stderr=0.06590160431497857, intercept_stderr=0.009037033037260158) We'll also visualise the how the MOE time-series and trend fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.1 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22ccd2776d0> We can also attribute specific savings from the MOE to the different RES technologies wind_weight = df_EI . loc [ s_GB_MOE . index , 'wind' ] / df_EI . loc [ s_GB_MOE . index ][[ 'wind' , 'solar' ]] . mean ( axis = 1 ) solar_weight = df_EI . loc [ s_GB_MOE . index , 'solar' ] / df_EI . loc [ s_GB_MOE . index ][[ 'wind' , 'solar' ]] . mean ( axis = 1 ) s_wind_MOE = s_GB_MOE * wind_weight s_solar_MOE = s_GB_MOE * solar_weight # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_wind_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Wind' ) s_solar_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Solar' ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) To add some context we'll plot the MOE alongside the actual price, we'll also do this for both years at the extremes of the dataset time range year_1 = '2010' year_2 = '2020' month_num_to_name = dict ( zip ( np . arange ( 1 , 13 ), pd . date_range ( '2010-01-01' , '2010-12-31' , freq = 'M' ) . strftime ( '%b' ))) s_hourly_dam_10 = s_GB_DAM [ year_1 ] . groupby ( s_GB_DAM [ year_1 ] . index . month ) . mean () s_hourly_MOE_10 = s_GB_MOE [ year_1 ] . groupby ( s_GB_MOE [ year_1 ] . index . month ) . mean () s_hourly_dam_20 = s_GB_DAM [ year_2 ] . groupby ( s_GB_DAM [ year_2 ] . index . month ) . mean () s_hourly_MOE_20 = s_GB_MOE [ year_2 ] . groupby ( s_GB_MOE [ year_2 ] . index . month ) . mean () for s in [ s_hourly_dam_10 , s_hourly_MOE_10 , s_hourly_dam_20 , s_hourly_MOE_20 ]: s . index = s . index . map ( month_num_to_name ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 9 , 3 )) ( s_hourly_dam_10 + s_hourly_MOE_10 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 0 ]) s_hourly_dam_10 . plot . bar ( label = 'Price' , ax = axs [ 0 ]) ( s_hourly_dam_20 + s_hourly_MOE_20 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 1 ]) s_hourly_dam_20 . plot . bar ( label = 'Day-Ahead' , ax = axs [ 1 ]) axs [ 0 ] . set_title ( year_1 , y = 0.9 ) axs [ 1 ] . set_title ( year_2 , y = 0.9 ) for ax in axs : eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlabel ( '' ) axs [ 1 ] . legend ( frameon = False , bbox_to_anchor = ( 0.125 , 1.05 )) axs [ 1 ] . set_yticks ([]) eda . hide_spines ( axs [ 1 ], positions = [ 'left' ]) axs [ 0 ] . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') We'll create a combined dataframe of the predicted and observed time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : s_GB_price , 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 Which we'll then save as a csv df_GB_results_ts . to_csv ( '../data/results/GB_price.csv' ) Wind Capture-Value Ratio \u00b6 Now we'll turn our focus to calculating the capture-value ratio of wind over time s_wind = df_EI [[ 'day_ahead_price' , 'wind' ]] . dropna ()[ 'wind' ] s_dam = df_EI [[ 'day_ahead_price' , 'wind' ]] . dropna ()[ 'day_ahead_price' ] We'll do this by weighting the price time-series using the wind generation data #exports def weighted_mean_s ( s , s_weight = None , dt_rng = pd . date_range ( '2009-12-01' , '2021-01-01' , freq = 'W' ), end_dt_delta_days = 7 ): \"\"\"Calculates the weighted average of a series\"\"\" capture_prices = dict () for start_dt in dt_rng : end_dt = start_dt + pd . Timedelta ( days = end_dt_delta_days ) if s_weight is not None : weights = s_weight [ start_dt : end_dt ] else : weights = None capture_prices [ start_dt ] = np . average ( s [ start_dt : end_dt ], weights = weights ) s_capture_prices = pd . Series ( capture_prices ) s_capture_prices . index = pd . to_datetime ( s_capture_prices . index ) return s_capture_prices s_wind_capture_prices = weighted_mean_s ( s_dam , s_wind ) s_dam_prices = weighted_mean_s ( s_dam ) s_wind_capture_prices . plot () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) <AxesSubplot:> We'll look at the distribution of the wind capture value ratio s_wind_capture_value_ratio = ( s_wind_capture_prices - s_dam_prices ) / s_dam_prices print ( round ( s_wind_capture_value_ratio . mean (), 4 )) sns . histplot ( s_wind_capture_value_ratio ) -0.028 <AxesSubplot:ylabel='Count'> We'll look at how this has changed on an annual basis fig , ax = plt . subplots ( dpi = 150 ) ( - 100 * s_wind_capture_value_ratio . groupby ( s_wind_capture_value_ratio . index . year ) . mean ()) . plot . bar ( ax = ax ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Wind Capture Price Suppression (%)' ) Text(0, 0.5, 'Wind Capture Price Suppression (%)') It could be interesting to look at the effect of price suppression on specific wind farms that have CfDs, and then estimate the increased burden on the tax-payer. German Model \u00b6 We'll now repeat the price MOE calculations for Germany, starting by loading in the relevant data %% time df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () Wall time: 2.29 s local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan We'll clean up the data and do a quick plot df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_price = df_DE_model [ 'price' ] s_DE_demand = df_DE_model [ 'demand' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_dispatchable [ '2015-03' : '2015-09' ], s_DE_price [ '2015-03' : '2015-09' ], s = 1 ) ax . scatter ( s_DE_dispatchable [ '2020-03' : '2020-09' ], s_DE_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (EUR/MWh)' ) Text(0, 0.5, 'Price (EUR/MWh)') Let's create a visualisation that highlights the importance of regressing against dispatchable instead of total load %% time dt_min = '2020-10' dt_max = '2020-12' # Dispatchable x_dispatch = s_DE_dispatchable [ dt_min : dt_max ] . values y_dispatch = s_DE_price . loc [ s_DE_dispatchable . index ][ dt_min : dt_max ] . values x_pred_dispatch = np . linspace ( 18 , 67 , 41 ) y_pred_dispatch = lowess . lowess_fit_and_predict ( x_dispatch , y_dispatch , frac = 0.25 , num_fits = 50 , x_pred = x_pred_dispatch ) # Demand x_demand = s_DE_demand [ dt_min : dt_max ] . values y_demand = s_DE_price . loc [ s_DE_dispatchable . index ][ dt_min : dt_max ] . values x_pred_demand = np . linspace ( 35 , 85 , 51 ) y_pred_demand = lowess . lowess_fit_and_predict ( x_demand , y_demand , frac = 0.25 , num_fits = 50 , x_pred = x_pred_demand ) # Plotting fig , axs = plt . subplots ( dpi = 250 , ncols = 2 , figsize = ( 12 , 5 )) ax = axs [ 0 ] ax . plot ( x_pred_demand , y_pred_demand , linewidth = 1.5 , color = 'r' ) ax . scatter ( x_demand , y_demand , color = 'k' , s = 0.5 , alpha = 0.5 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) ax = axs [ 1 ] ax . plot ( x_pred_dispatch , y_pred_dispatch , linewidth = 1.5 , color = 'r' ) ax . scatter ( x_dispatch , y_dispatch , color = 'k' , s = 0.5 , alpha = 0.5 ) ax . set_xlim ( 10 , 80 ) eda . hide_spines ( ax , positions = [ 'top' , 'left' , 'right' ]) ax . set_yticks ([]) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) for ax in axs : ax . set_ylim ( - 20 , 120 ) fig . tight_layout () Wall time: 236 ms We'll load in our model %% time if load_existing_DE_model == True : smooth_dates = pickle . load ( open ( DE_model_fp , 'rb' )) else : reg_dates = pd . date_range ( '2015-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_DE_dispatchable . values , s_DE_price . values , dt_idx = s_DE_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) Wall time: 364 ms Generate the regression surface prediction %% time x_pred = np . linspace ( - 5 , 90 , 951 ) dt_pred = pd . date_range ( '2015-01-01' , '2020-12-31' , freq = '1D' ) df_DE_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_DE_pred . index = np . round ( df_DE_pred . index , 1 ) df_DE_pred . head () Wall time: 276 ms Unnamed: 0 2015-01-01 2015-01-02 2015-01-03 2015-01-04 2015-01-05 2015-01-06 2015-01-07 2015-01-08 2015-01-09 2015-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 -38.4761 -38.4242 -38.3727 -38.3215 -38.2708 -38.2205 -38.1705 -38.1208 -38.0715 -38.0225 ... -48.2642 -48.2766 -48.289 -48.3016 -48.3144 -48.3272 -48.3402 -48.3534 -48.3667 -48.3801 -4.9 -38.3167 -38.2649 -38.2136 -38.1626 -38.112 -38.0618 -38.012 -37.9625 -37.9133 -37.8644 ... -48.0285 -48.0407 -48.0531 -48.0657 -48.0783 -48.0911 -48.104 -48.1171 -48.1304 -48.1437 -4.8 -38.1572 -38.1056 -38.0544 -38.0036 -37.9532 -37.9031 -37.8534 -37.8041 -37.7551 -37.7064 ... -47.7926 -47.8048 -47.8172 -47.8296 -47.8422 -47.8549 -47.8678 -47.8808 -47.894 -47.9073 -4.7 -37.9977 -37.9463 -37.8952 -37.8446 -37.7943 -37.7444 -37.6949 -37.6457 -37.5968 -37.5482 ... -47.5568 -47.5689 -47.5812 -47.5936 -47.6061 -47.6188 -47.6316 -47.6445 -47.6576 -47.6709 -4.6 -37.8382 -37.7869 -37.736 -37.6855 -37.6354 -37.5857 -37.5363 -37.4872 -37.4385 -37.3901 ... -47.3209 -47.333 -47.3452 -47.3575 -47.3699 -47.3825 -47.3953 -47.4082 -47.4212 -47.4344 As with the GB market you can see what is likely the effect of higher gas prices in 2018 df_DE_dispatchable_lims = construct_dispatchable_lims_df ( s_DE_dispatchable , rolling_w = 6 ) df_DE_pred_mask = construct_pred_mask_df ( df_DE_pred , df_DE_dispatchable_lims ) # Plotting min_y = 10 max_y = 70 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Price (EUR/MWh)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2015-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate some metrics for our model s_DE_pred_ts_dispatch , s_DE_pred_ts_demand = get_model_pred_ts ( s_DE_dispatchable , DE_model_fp , s_demand = s_DE_demand , x_pred = x_pred , dt_pred = dt_pred ) s_DE_dispatch_err = s_DE_pred_ts_dispatch - s_DE_price . loc [ s_DE_pred_ts_dispatch . index ] DE_dispatch_metrics = calc_error_metrics ( s_DE_dispatch_err ) s_DE_demand_err = s_DE_pred_ts_demand - s_DE_price . loc [ s_DE_pred_ts_demand . index ] DE_demand_metrics = calc_error_metrics ( s_DE_demand_err ) DE_dispatch_metrics {'median_abs_err': 4.257075090332123, 'mean_abs_err': 5.852023979176648, 'root_mean_square_error': 8.705711313706535} As well as the \\(r^{2}\\) score # 0.733 for Halttunen2021 r2_score ( s_DE_price . loc [ s_DE_pred_ts_dispatch . index ], s_DE_pred_ts_dispatch ) 0.7244797152665161 We'll now calculate the total savings start_date = '2015' end_date = '2020' s_DE_MOE = s_DE_pred_ts_demand - s_DE_pred_ts_dispatch s_DE_MOE = s_DE_MOE . dropna () s_DE_saving = s_DE_MOE * df_DE [ 'demand' ] . loc [ s_DE_MOE . index ] * 1000 total_saving = s_DE_saving [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was \u00a3 { total_saving : ,.0f } \" ) The total saving between 2015 and 2020 was \u00a355,856,316,374 And get some context for the market average and total volumes over the same period s_DE_mkt_cost = df_DE [ 'price' ] * df_DE [ 'demand' ] * 1000 avg_DE_HH_mkt_cost = s_DE_mkt_cost . mean () total_DE_mkt_cost = s_DE_mkt_cost [ start_date : end_date ] . sum () avg_DE_HH_mkt_cost , total_DE_mkt_cost (2076614.8441243432, 109047198694.6575) When we plot the percentage MOE over time we can see the large influence of lowered demand in 2020 s_DE_MOE_rolling = s_DE_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_DE_DAM_rolling = df_DE . loc [ s_DE_MOE . index ][ 'price' ] . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_pct_reduction = s_DE_MOE_rolling / s_DE_DAM_rolling s_DE_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> We'll quickly calculate the average percentage price suppresion s_DE_MOE [ start_date : end_date ] . mean () / ( df_DE [ 'price' ] + s_DE_MOE )[ start_date : end_date ] . mean () 0.3316044469891089 Of note there appeared to be relatively little impact on the MOE from the effects of the covid-19 response fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.3 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22ccaf82700> Interestingly, the forecast made in 2011 by Traber et al as to the German MOE in 2020 was quite close. \"In the absence of expanded deployment of renewable energy, a higher price increase of 20% can be expected\" in 2020 - source s_DE_MOE [ '2015' ] . mean (), s_DE_MOE [ '2020' ] . mean () (11.657604746082448, 22.174503965250132) We'll now disaggregate the MOE based on the wind and solar drivers wind_weight = df_DE . loc [ s_DE_MOE . index , 'Wind' ] / df_DE . loc [ s_DE_MOE . index ][[ 'Wind' , 'Solar' ]] . mean ( axis = 1 ) solar_weight = df_DE . loc [ s_DE_MOE . index , 'Solar' ] / df_DE . loc [ s_DE_MOE . index ][[ 'Wind' , 'Solar' ]] . mean ( axis = 1 ) s_wind_MOE = ( s_DE_MOE * wind_weight ) . dropna () s_solar_MOE = ( s_DE_MOE * solar_weight ) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_wind_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Wind' ) s_solar_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Solar' ) ax . set_ylim ( 0 , 55 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We'll look at how the MOE has changed over time in terms of the seasonal effect year_1 = '2015' year_2 = '2020' s_DE_DAM = df_DE . loc [ s_DE_MOE . index , 'price' ] month_num_to_name = dict ( zip ( np . arange ( 1 , 13 ), pd . date_range ( '2010-01-01' , '2010-12-31' , freq = 'M' ) . strftime ( '%b' ))) s_DE_hourly_dam_10 = s_DE_DAM [ year_1 ] . groupby ( s_DE_DAM [ year_1 ] . index . month ) . mean () s_DE_hourly_MOE_10 = s_DE_MOE [ year_1 ] . groupby ( s_DE_MOE [ year_1 ] . index . month ) . mean () s_DE_hourly_dam_20 = s_DE_DAM [ year_2 ] . groupby ( s_DE_DAM [ year_2 ] . index . month ) . mean () s_DE_hourly_MOE_20 = s_DE_MOE [ year_2 ] . groupby ( s_DE_MOE [ year_2 ] . index . month ) . mean () for s in [ s_DE_hourly_dam_10 , s_DE_hourly_MOE_10 , s_DE_hourly_dam_20 , s_DE_hourly_MOE_20 ]: s . index = s . index . map ( month_num_to_name ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 9 , 3 )) ( s_DE_hourly_dam_10 + s_DE_hourly_MOE_10 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 0 ]) s_DE_hourly_dam_10 . plot . bar ( label = 'Price' , ax = axs [ 0 ]) ( s_DE_hourly_dam_20 + s_DE_hourly_MOE_20 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 1 ]) s_DE_hourly_dam_20 . plot . bar ( label = 'Day-Ahead' , ax = axs [ 1 ]) axs [ 0 ] . set_title ( year_1 , y = 0.9 ) axs [ 1 ] . set_title ( year_2 , y = 0.9 ) for ax in axs : eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlabel ( '' ) axs [ 1 ] . legend ( frameon = False , bbox_to_anchor = ( 0.125 , 1.05 )) axs [ 1 ] . set_yticks ([]) eda . hide_spines ( axs [ 1 ], positions = [ 'left' ]) axs [ 0 ] . set_ylabel ( 'Price (EUR/MWh)' ) Text(0, 0.5, 'Price (EUR/MWh)') We'll also inspect the predictions on a sample day date = '2020-04-11' # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_DE_pred_ts_dispatch [ date ] . plot ( label = 'Prediction with RES' , ax = ax ) s_DE_pred_ts_demand [ date ] . plot ( label = 'Prediction without RES' , ax = ax ) s_DE_price [ date ] . plot ( label = 'Observed' , ax = ax ) ax . legend ( frameon = False , ncol = 3 , bbox_to_anchor = ( 1.075 , - 0.15 )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (EUR/MWh)' ) eda . hide_spines ( ax ) There's clearly a correlation between the annual MOE average and the RES percentage penetration %% time s_DE_MOE_annual_avg = s_DE_MOE . replace ( np . inf , np . nan ) . dropna () . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) s_DE_RES_pct_annual_avg = 100 * ( df_DE [[ 'Wind' , 'Solar' ]] . sum ( axis = 1 ) / df_DE [ 'demand' ]) . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) . loc [ 2015 :] . dropna () plt . scatter ( s_DE_MOE_annual_avg , s_DE_RES_pct_annual_avg ) Wall time: 51 ms <matplotlib.collections.PathCollection at 0x22c405d2dc0> We'll quickly calculate the gradient linreg_results = scipy . stats . linregress ( x = s_DE_RES_pct_annual_avg , y = s_DE_MOE_annual_avg ) linreg_results LinregressResult(slope=0.7365449501368756, intercept=-3.832555184231513, rvalue=0.9793888213890273, pvalue=0.0006328529981050964, stderr=0.07595057880689415, intercept_stderr=2.196641338586976) We'll get some context from the literature imperial_paper_slope = 0.63 pct_diff = round ( 100 * ( linreg_results . slope - imperial_paper_slope ) / imperial_paper_slope ) print ( f 'In this work the MOE increase per percentage penetration of RES was { pct_diff } % higher (for Germany) than the Imperial study' ) In this work the MOE increase per percentage penetration of RES was 17% higher (for Germany) than the Imperial study Plots \u00b6 In this section we'll generate some of the plots needed for the paper, starting with the heatmap of the price surfaces fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] min_y = 0 max_y = 60 htmp = sns . heatmap ( df_pred [ min_y : max_y ] . where ( df_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) # DE ax = axs [ 1 ] min_y = 10 max_y = 80 htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2015-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) fig . tight_layout () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) We'll also plot the MOE time-series fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.1 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) # DE ax = axs [ 1 ] ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.3 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22c22be1040> Saving Results \u00b6 Additionaly we'll save the time-series predictions and model metrics, starting with the GB time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : s_GB_price , 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 Which we'll save to csv df_GB_results_ts . to_csv ( '../data/results/GB_price.csv' ) Then the DE time-series df_DE_results_ts = pd . DataFrame ({ 'prediction' : s_DE_pred_ts_dispatch , 'counterfactual' : s_DE_pred_ts_demand , 'observed' : s_DE_price , 'moe' : s_DE_MOE }) df_DE_results_ts . to_csv ( '../data/results/DE_price.csv' ) df_DE_results_ts . head () local_datetime prediction counterfactual observed moe 2015-01-04 23:00:00+00:00 24.4963 34.5304 22.34 10.0341 2015-01-05 00:00:00+00:00 23.7584 33.626 17.93 9.86759 2015-01-05 01:00:00+00:00 23.6626 33.9245 15.17 10.2619 2015-01-05 02:00:00+00:00 24.1342 34.5315 16.38 10.3973 2015-01-05 03:00:00+00:00 24.9343 36.2104 17.38 11.276 And finally the model metrics for both model_accuracy_metrics = { 'DE_dispatch' : DE_dispatch_metrics , 'DE_demand' : DE_demand_metrics , 'GB_dispatch' : GB_dispatch_metrics , 'GB_demand' : GB_demand_metrics } with open ( '../data/results/price_model_accuracy_metrics.json' , 'w' ) as fp : json . dump ( model_accuracy_metrics , fp ) JSON ( model_accuracy_metrics ) <IPython.core.display.JSON object>","title":"Price MOE Calculation"},{"location":"dev-05-price-moe/#price-merit-order-effect-analysis","text":"This notebook outlines the analysis required to determine the price merit-order-effect of variable renewable generation in the GB and DE power markets.","title":"Price Merit Order Effect Analysis"},{"location":"dev-05-price-moe/#imports","text":"#exports import json import pandas as pd import numpy as np import pickle import scipy from sklearn import linear_model from sklearn.metrics import r2_score from collections.abc import Iterable import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt import matplotlib.dates as mdates from ipypb import track from IPython.display import JSON from moepy import lowess , eda from moepy.surface import PicklableFunction","title":"Imports"},{"location":"dev-05-price-moe/#user-inputs","text":"GB_model_fp = '../data/models/DAM_price_GB_p50.pkl' DE_model_fp = '../data/models/DAM_price_DE_p50.pkl' load_existing_GB_model = True load_existing_DE_model = True","title":"User Inputs"},{"location":"dev-05-price-moe/#estimating-the-price-surface","text":"We'll start by loading in the data %% time df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI . head () Wall time: 1.76 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 00:00:00+00:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 00:30:00+00:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:00:00+00:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 01:30:00+00:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 02:00:00+00:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll do a quick plot of the average price fig , ax = plt . subplots ( dpi = 150 ) df_EI [ 'day_ahead_price' ] . resample ( '4W' ) . mean () . plot ( ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Day-Ahead Price \\n Monthly Average (\u00a3/MWh)' ) Text(0, 0.5, 'Day-Ahead Price\\nMonthly Average (\u00a3/MWh)') We'll also visualise individual half-hour periods for two different date ranges in the dataset df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable [ '2010-03' : '2010-09' ], s_price [ '2010-03' : '2010-09' ], s = 1 ) ax . scatter ( s_dispatchable [ '2020-03' : '2020-09' ], s_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlim ( 8 , 60 ) ax . set_ylim ( - 25 , 100 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Next we'll load in (or fit) one of the models trained in the previous notebook %% time if load_existing_GB_model == True : smooth_dates = pickle . load ( open ( GB_model_fp , 'rb' )) else : reg_dates = pd . date_range ( '2009-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_dispatchable . values , s_price . values , dt_idx = s_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) Wall time: 2.42 s We're now ready to make our price curve predictions, we'll make one for each day that the model used in training %% time x_pred = np . linspace ( 3 , 61 , 581 ) dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' ) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . index = np . round ( df_pred . index , 1 ) df_pred . head () Wall time: 469 ms Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 3 -17.4948 -17.5178 -17.5406 -17.5632 -17.5856 -17.6077 -17.6297 -17.6514 -17.673 -17.6943 ... 0.194247 0.203242 0.212319 0.221479 0.230719 0.240041 0.249444 0.258926 0.268489 0.278132 3.1 -17.2964 -17.3193 -17.3421 -17.3646 -17.3869 -17.409 -17.4309 -17.4525 -17.474 -17.4953 ... 0.404498 0.413478 0.42254 0.431683 0.440907 0.450212 0.459598 0.469064 0.478609 0.488234 3.2 -17.0983 -17.1212 -17.1439 -17.1663 -17.1886 -17.2106 -17.2324 -17.254 -17.2754 -17.2966 ... 0.614692 0.623656 0.632702 0.641829 0.651037 0.660326 0.669695 0.679143 0.688671 0.698278 3.3 -16.9006 -16.9234 -16.946 -16.9684 -16.9905 -17.0125 -17.0342 -17.0558 -17.0771 -17.0983 ... 0.824815 0.833764 0.842794 0.851906 0.861097 0.87037 0.879722 0.889153 0.898664 0.908253 3.4 -16.7031 -16.7259 -16.7484 -16.7707 -16.7928 -16.8147 -16.8364 -16.8579 -16.8791 -16.9002 ... 1.03486 1.04379 1.0528 1.0619 1.07108 1.08033 1.08967 1.09908 1.10858 1.11815","title":"Estimating the Price Surface"},{"location":"dev-05-price-moe/#visualising-the-price-surface","text":"We now want to actually visualise the results of our model, in particular the price surface it's fitted. We'll start by plotting all of the price curves overlapping each other. cmap = plt . get_cmap ( 'viridis' ) cbar_ticks = [ 0 , 0.25 , 0.5 , 0.75 , 1 ] # Plotting fig , ax = plt . subplots ( dpi = 250 ) lp = df_pred . loc [:, :: 7 ] . plot ( legend = False , cmap = cmap , linewidth = 0.25 , ax = ax ) cax = fig . add_axes ([ 0.9 , 0.2 , 0.03 , 0.55 ]) cbar = mpl . colorbar . ColorbarBase ( cax , orientation = 'vertical' , cmap = cmap , ticks = cbar_ticks ) cbar . ax . set_yticklabels ([ dt_pred [ min ( int ( len ( dt_pred ) * tick_loc ), len ( dt_pred ) - 1 )] . strftime ( '%b %Y' ) for tick_loc in cbar_ticks ]) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) ax . set_xlim ( df_pred . index [ 0 ]) ax . set_ylim ( 0 , 75 ) ax . set_title ( 'Day-Ahead Market Average Price Curve' ) Text(0.5, 1.0, 'Day-Ahead Market Average Price Curve') Whilst the previous plot might look quite nice it's rather difficult to interpret, an alternative way to visualise how the price curve evolves over time is using a heatmap #exports def construct_dispatchable_lims_df ( s_dispatchable , rolling_w = 3 , daily_quantiles = [ 0.001 , 0.999 ]): \"\"\"Identifies the rolling limits to be used in masking\"\"\" df_dispatchable_lims = ( s_dispatchable . resample ( '1d' ) . quantile ( daily_quantiles ) . unstack () . rolling ( rolling_w * 7 ) . mean () . bfill () . ffill () . iloc [: - 1 , :] ) df_dispatchable_lims . index = pd . to_datetime ( df_dispatchable_lims . index . strftime ( '%Y-%m- %d ' )) return df_dispatchable_lims def construct_pred_mask_df ( df_pred , df_dispatchable_lims ): \"\"\"Constructs a DataFrame mask for the prediction\"\"\" df_pred = df_pred [ df_dispatchable_lims . index ] df_pred_mask = pd . DataFrame ( dict ( zip ( df_pred . columns , [ df_pred . index ] * df_pred . shape [ 1 ])), index = df_pred . index ) df_pred_mask = ( df_pred_mask > df_dispatchable_lims . iloc [:, 0 ] . values ) & ( df_pred_mask < df_dispatchable_lims . iloc [:, 1 ] . values ) df_pred . columns = pd . to_datetime ( df_pred . columns ) df_pred_mask . columns = pd . to_datetime ( df_pred_mask . columns ) return df_pred_mask df_dispatchable_lims = construct_dispatchable_lims_df ( s_dispatchable ) df_pred_mask = construct_pred_mask_df ( df_pred , df_dispatchable_lims ) sns . heatmap ( df_pred . where ( df_pred_mask , np . nan ) . iloc [:: - 1 ]) <AxesSubplot:> The default output for seaborn heatmaps never looks great when one of the axis is a datetime, we'll write a custom class and wrapper to handle this #exports class AxTransformer : \"\"\"Helper class for cleaning axis tick locations and labels\"\"\" def __init__ ( self , datetime_vals = False ): self . datetime_vals = datetime_vals self . lr = linear_model . LinearRegression () return def process_tick_vals ( self , tick_vals ): if not isinstance ( tick_vals , Iterable ) or isinstance ( tick_vals , str ): tick_vals = [ tick_vals ] if self . datetime_vals == True : tick_vals = pd . to_datetime ( tick_vals ) . astype ( int ) . values tick_vals = np . array ( tick_vals ) return tick_vals def fit ( self , ax , axis = 'x' ): axis = getattr ( ax , f 'get_ { axis } axis' )() tick_locs = axis . get_ticklocs () tick_vals = self . process_tick_vals ([ label . _text for label in axis . get_ticklabels ()]) self . lr . fit ( tick_vals . reshape ( - 1 , 1 ), tick_locs ) return def transform ( self , tick_vals ): tick_vals = self . process_tick_vals ( tick_vals ) tick_locs = self . lr . predict ( np . array ( tick_vals ) . reshape ( - 1 , 1 )) return tick_locs def set_ticks ( ax , tick_locs , tick_labels = None , axis = 'y' ): \"\"\"Sets ticks at standard numerical locations\"\"\" if tick_labels is None : tick_labels = tick_locs ax_transformer = AxTransformer () ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( tick_locs )) getattr ( ax , f 'set_ { axis } ticklabels' )( tick_labels ) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax def set_date_ticks ( ax , start_date , end_date , axis = 'y' , date_format = '%Y-%m- %d ' , ** date_range_kwargs ): \"\"\"Sets ticks at datetime locations\"\"\" dt_rng = pd . date_range ( start_date , end_date , ** date_range_kwargs ) ax_transformer = AxTransformer ( datetime_vals = True ) ax_transformer . fit ( ax , axis = axis ) getattr ( ax , f 'set_ { axis } ticks' )( ax_transformer . transform ( dt_rng )) getattr ( ax , f 'set_ { axis } ticklabels' )( dt_rng . strftime ( date_format )) ax . tick_params ( axis = axis , which = 'both' , bottom = True , top = False , labelbottom = True ) return ax %% time fig , ax = plt . subplots ( dpi = 150 , figsize = ( 10 , 6 )) htmp = sns . heatmap ( df_pred [ 10 : 60 ] . where ( df_pred_mask [ 10 : 60 ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Price (\u00a3/MWh)' }) set_ticks ( ax , np . arange ( 0 , 70 , 10 ), axis = 'y' ) set_date_ticks ( ax , '2009-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) Wall time: 1.79 s C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(144.58333333333331, 0.5, 'Demand - [Solar + Wind] (GW)') We also want to visualise specific date ranges, here we'll look towards the end of 2020 %% time center_date = '2020-12-01' dt_min = pd . to_datetime ( center_date ) - pd . Timedelta ( weeks = 4 ) dt_max = pd . to_datetime ( center_date ) + pd . Timedelta ( weeks = 4 ) x = s_dispatchable [ dt_min : dt_max ] . values y = s_price . loc [ s_dispatchable . index ][ dt_min : dt_max ] . values x_pred = np . linspace ( 11 , 40 , 41 ) y_pred = lowess . lowess_fit_and_predict ( x , y , frac = 0.6 , num_fits = 25 , x_pred = x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , linewidth = 1.5 , color = 'r' ) ax . scatter ( x , y , color = 'k' , s = 1 , alpha = 0.2 ) ax . set_title ( f 'November & December 2020' ) # remove in the LaTeX plot and just state in the caption ax . set_xlim ( 11 , 40 ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) Wall time: 58.1 ms C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) Text(0, 0.5, 'Day-Ahead Price (\u00a3/MWh)')","title":"Visualising the Price Surface"},{"location":"dev-05-price-moe/#evaluating-the-price-curve-predictions","text":"#exports def construct_df_pred ( model_fp , x_pred = np . linspace ( - 2 , 61 , 631 ), dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' )): \"\"\"Constructs the prediction surface for the specified pre-fitted model\"\"\" smooth_dates = pickle . load ( open ( model_fp , 'rb' )) df_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred . index = np . round ( df_pred . index , 1 ) return df_pred model_fp = '../data/models/DAM_price_average.pkl' df_pred = construct_df_pred ( model_fp ) df_pred . head () Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -2 -28.4122 -28.4306 -28.4488 -28.4668 -28.4846 -28.5022 -28.5197 -28.5369 -28.554 -28.5708 ... -17.5465 -17.5399 -17.5331 -17.5263 -17.5194 -17.5125 -17.5054 -17.4982 -17.4909 -17.4836 -1.9 -28.2171 -28.2354 -28.2536 -28.2715 -28.2893 -28.3068 -28.3242 -28.3414 -28.3584 -28.3752 ... -17.2979 -17.2913 -17.2846 -17.2778 -17.2709 -17.2639 -17.2568 -17.2496 -17.2424 -17.235 -1.8 -28.022 -28.0403 -28.0583 -28.0762 -28.0939 -28.1115 -28.1288 -28.1459 -28.1629 -28.1796 ... -17.0494 -17.0427 -17.036 -17.0292 -17.0223 -17.0153 -17.0082 -17.001 -16.9938 -16.9864 -1.7 -27.8269 -27.8451 -27.8631 -27.881 -27.8986 -27.9161 -27.9334 -27.9504 -27.9673 -27.984 ... -16.8008 -16.7941 -16.7874 -16.7806 -16.7737 -16.7667 -16.7596 -16.7524 -16.7451 -16.7378 -1.6 -27.6318 -27.65 -27.6679 -27.6857 -27.7033 -27.7207 -27.7379 -27.7549 -27.7718 -27.7884 ... -16.5522 -16.5455 -16.5388 -16.532 -16.5251 -16.5181 -16.511 -16.5038 -16.4965 -16.4891 Now we've created our prediction dataframe we can calculate a time-series for our price prediction N.b. to speed things up every 100th half-hour has been sampled rather than using the full dataset #exports def construct_pred_ts ( s , df_pred ): \"\"\"Uses the time-adaptive LOWESS surface to generate time-series prediction\"\"\" s_pred_ts = pd . Series ( index = s . index , dtype = 'float64' ) for dt_idx , val in track ( s . iteritems (), total = s . size ): s_pred_ts . loc [ dt_idx ] = df_pred . loc [ round ( val , 1 ), dt_idx . strftime ( '%Y-%m- %d ' )] return s_pred_ts s_dispatchable = ( df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 )) . dropna () . loc [: df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts = construct_pred_ts ( s_dispatchable . iloc [:: 100 ] . iloc [: - 1 ], df_pred ) s_pred_ts . head () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 1600/2095 [00:02 < 00:00, 0.00s/it] local_datetime 2009-01-01 00:00:00+00:00 39.026220 2009-01-03 02:00:00+00:00 34.314689 2009-01-05 04:00:00+00:00 32.502689 2009-01-07 06:00:00+00:00 42.705269 2009-01-09 08:00:00+00:00 64.025113 dtype: float64 We'll quickly inspect the error distribution s_price = df_EI [ 'day_ahead_price' ] s_err = s_pred_ts - s_price . loc [ s_pred_ts . index ] print ( s_err . abs () . mean ()) sns . histplot ( s_err ) _ = plt . xlim ( - 75 , 75 ) 6.8625170828105615 We'll quickly calculate the r2 score for this model, for context the UK day-ahead price multiple linear regression model from the Staffell/Green group achieved an adjusted r2 of 0.451 r2_score ( s_price . loc [ s_pred_ts . index ], s_pred_ts ) 0.46527623686663677 Now we'll calculate some error metrics, including the option to remove extreme error outliers #exports def calc_error_metrics ( s_err , max_err_quantile = 1 ): \"\"\"Calculates several error metrics using the passed error series\"\"\" if s_err . isnull () . sum () > 0 : s_err = s_err . dropna () max_err_cutoff = s_err . abs () . quantile ( max_err_quantile ) s_err = s_err [ s_err . abs () <= max_err_cutoff ] metrics = { 'median_abs_err' : s_err . abs () . median (), 'mean_abs_err' : s_err . abs () . mean (), 'root_mean_square_error' : np . sqrt (( s_err ** 2 ) . mean ()) } return metrics metrics = calc_error_metrics ( s_err ) metrics {'median_abs_err': 4.843738852384298, 'mean_abs_err': 6.8625170828105615, 'root_mean_square_error': 12.301327313943641} We'll now create a wrapper for the last few steps and repeat the analysis for four variants of the model #exports def get_model_pred_ts ( s , model_fp , s_demand = None , x_pred = np . linspace ( - 2 , 61 , 631 ), dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' )): \"\"\"Constructs the time-series prediction for the specified pre-fitted model\"\"\" df_pred = construct_df_pred ( model_fp , x_pred = x_pred , dt_pred = dt_pred ) s_cleaned = s . dropna () . loc [ df_pred . columns . min (): df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts = construct_pred_ts ( s_cleaned , df_pred ) if s_demand is None : return s_pred_ts else : s_cleaned = s_demand . dropna () . loc [ df_pred . columns . min (): df_pred . columns . max () + pd . Timedelta ( hours = 23 , minutes = 30 )] s_pred_ts_demand = construct_pred_ts ( s_cleaned , df_pred ) return s_pred_ts , s_pred_ts_demand s_demand = df_EI [ 'demand' ] s_price = df_EI [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) model_runs = { 'demand_p50' : { 'model_fp' : '../data/models/DAM_price_demand_p50.pkl' , 's' : s_demand }, 'demand_avg' : { 'model_fp' : '../data/models/DAM_price_demand_average.pkl' , 's' : s_demand }, 'dispatch_p50' : { 'model_fp' : '../data/models/DAM_price_p50.pkl' , 's' : s_dispatchable }, 'dispatch_avg' : { 'model_fp' : '../data/models/DAM_price_average.pkl' , 's' : s_dispatchable }, } model_outputs = dict () for model_name , model_kwargs in track ( model_runs . items ()): s_pred_ts = get_model_pred_ts ( ** model_kwargs ) s_err = s_pred_ts - s_price . loc [ s_pred_ts . index ] metrics = calc_error_metrics ( s_err ) model_outputs [ model_name ] = { 's_pred_ts' : s_pred_ts , 's_err' : s_err , 'metrics' : metrics } 100% 4/4 [11:27 < 02:57, 171.67s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 192924/209736 [02:23 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/209736 [02:26 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 165505/209533 [03:30 < 00:00, 0.00s/it] C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/209533 [02:53 < 00:00, 0.00s/it] Overall, the dispatch model fitted to the median price appears to be the most accurate df_metrics = pd . DataFrame ({ name : outputs [ 'metrics' ] for name , outputs in model_outputs . items () }) df_metrics Unnamed: 0 demand_p50 demand_avg dispatch_p50 dispatch_avg median_abs_err 4.70534 5.04971 4.47311 4.77768 mean_abs_err 6.97489 7.09014 6.55688 6.6518 root_mean_square_error 12.6323 12.41 12.0539 11.8075 We'll quantify the difference against the standard LOWESS approach old_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_avg' ] new_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_p50' ] print ( f ' { - 100 * ( new_err - old_err ) / old_err : .2f } % mean absolute error reduction using the p50 model rather than the average model' ) 1.43% mean absolute error reduction using the p50 model rather than the average model And also look at the improvement in accuracy when regressing against dispatchable generation instead of total generation old_err = df_metrics . loc [ 'mean_abs_err' , 'demand_avg' ] new_err = df_metrics . loc [ 'mean_abs_err' , 'dispatch_avg' ] print ( f ' { - 100 * ( new_err - old_err ) / old_err : .2f } % mean absolute error reduction using the dispatchable demand model rather than just demand' ) 6.18% mean absolute error reduction using the dispatchable demand model rather than just demand","title":"Evaluating the Price Curve Predictions"},{"location":"dev-05-price-moe/#quantifying-visualising-the-merit-order-effect","text":"To begin we'll load up our dispatchable supply model and make an inference for each half-hour in the dataset, based on the results in the previous section we'll use the p50 model. %% time s_GB_demand = df_EI_model [ 'demand' ] s_GB_price = df_EI_model [ 'day_ahead_price' ] s_GB_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) s_GB_pred_ts_dispatch , s_GB_pred_ts_demand = get_model_pred_ts ( s_GB_dispatchable , GB_model_fp , s_demand = s_GB_demand ) s_dispatch_GB_err = s_GB_pred_ts_dispatch - s_GB_price . loc [ s_GB_pred_ts_dispatch . index ] GB_dispatch_metrics = calc_error_metrics ( s_dispatch_GB_err ) s_demand_GB_err = s_GB_pred_ts_demand - s_GB_price . loc [ s_GB_pred_ts_demand . index ] GB_demand_metrics = calc_error_metrics ( s_demand_GB_err ) s_GB_pred_ts_dispatch Wall time: 92.1 ms local_datetime 2009-01-01 00:00:00+00:00 37.203441 2009-01-01 00:30:00+00:00 37.313379 2009-01-01 01:00:00+00:00 36.768513 2009-01-01 01:30:00+00:00 35.595162 2009-01-01 02:00:00+00:00 34.849422 ... 2020-12-30 21:30:00+00:00 48.066638 2020-12-30 22:00:00+00:00 45.268069 2020-12-30 22:30:00+00:00 42.647597 2020-12-30 23:00:00+00:00 39.520118 2020-12-30 23:30:00+00:00 37.948852 Length: 209533, dtype: float64 The difference between the price forecast when using the dispatchable generation and total generation is due to the merit order effect s_GB_MOE = s_GB_pred_ts_demand - s_GB_pred_ts_dispatch s_GB_MOE = s_GB_MOE . dropna () s_GB_MOE . plot () <AxesSubplot:xlabel='local_datetime'> We'll quickly calculate the averages for 2010 and 2020 s_GB_MOE [ '2010' ] . mean (), s_GB_MOE [ '2020' ] . mean () (0.8813580281178922, 13.888630517942211) We'll also visualise the predictions for a sample day date = '2017-07-01' # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_GB_pred_ts_dispatch [ date ] . plot ( label = 'Prediction with RES' , ax = ax ) s_GB_pred_ts_demand [ date ] . plot ( label = 'Prediction without RES' , ax = ax ) s_GB_price [ date ] . plot ( label = 'Observed' , ax = ax ) ax . legend ( frameon = False , ncol = 3 , bbox_to_anchor = ( 1.075 , - 0.15 )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) eda . hide_spines ( ax ) We can see that there is a clear pattern in the bias over the course of the day s_GB_err . groupby ( s_GB_err . index . hour + s_GB_err . index . minute / 60 ) . mean () . plot . bar () <AxesSubplot:xlabel='local_datetime'> We'll now calculate the half-hourly savings due to the merit order effect. The demand is expressed in terms of GW which we'll convert into MWh to be compatible with the units of the merit order effect. s_GB_saving = s_GB_MOE * s_GB_demand . loc [ s_GB_MOE . index ] * 1000 * 0.5 s_GB_saving . resample ( '2W' ) . mean () . plot () <AxesSubplot:xlabel='local_datetime'> The distribution of the price savings appears to follow a rough Pareto distribution sns . histplot ( s_GB_saving ) plt . xlim ( 0 , 750000 ) eda . hide_spines ( plt . gca ()) We'll calculate the total savings over the dataset period start_date = '2010' end_date = '2020' total_saving = s_GB_saving [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was \u00a3 { total_saving : ,.0f } \" ) The total saving between 2010 and 2020 was \u00a317,454,468,143 To add some context we'll calculate the average half-hourly market volume (note that the DAM volume will be smaller than demand but it acts as a good proxy to cover all markets) s_mkt_cost = df_EI [ 'day_ahead_price' ] * df_EI [ 'demand' ] * 1000 * 0.5 avg_HH_mkt_cost = s_mkt_cost . mean () avg_HH_mkt_cost 784440.8559554707 We'll also calculate the long-term price reduction due to the merit order effect s_GB_MOE [ start_date : end_date ] . mean () / ( df_EI [ 'day_ahead_price' ] + s_GB_MOE )[ start_date : end_date ] . mean () 0.15034496268274428 This only tells part of the story though, lets look at how this value evolves over time s_GB_DAM = s_GB_price . loc [ s_GB_MOE . index ] s_GB_MOE_rolling = s_GB_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_GB_DAM_rolling = s_GB_DAM . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_pct_reduction = s_GB_MOE_rolling / s_GB_DAM_rolling s_GB_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> We're half-way to creating the metrics required for comparing between markets of different sizes, prices and RES penetration. We'll calculate the renewables penetration percentage and then aggregate it on an annual basis alongside the pct price reduction due to the MOE. %% time s_GB_MOE_pct_annual_avg = ( s_GB_MOE / ( df_EI . loc [ s_GB_MOE . index ][ 'day_ahead_price' ] + s_GB_MOE )) . replace ( np . inf , np . nan ) . dropna () . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) s_GB_RES_pct_annual_avg = ( df_EI [[ 'wind' , 'solar' ]] . sum ( axis = 1 ) / df_EI [ 'demand' ]) . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) # Plotting fig , ax = plt . subplots ( dpi = 250 ) sns . regplot ( x = 100 * s_GB_RES_pct_annual_avg , y = 100 * s_GB_MOE_pct_annual_avg , ax = ax , label = 'Incl. 2020 (m=0.86)' ) sns . regplot ( x = 100 * s_GB_RES_pct_annual_avg . loc [: 2019 ], y = 100 * s_GB_MOE_pct_annual_avg . loc [: 2019 ], truncate = False , ax = ax , label = 'Excl. 2020 (m=0.67)' ) ax . scatter ( x = 100 * s_GB_RES_pct_annual_avg , y = 100 * s_GB_MOE_pct_annual_avg , color = 'k' ) eda . hide_spines ( ax ) ax . legend ( frameon = False , loc = 'upper left' ) ax . set_ylim ( 0 ) ax . set_xlabel ( 'Average RES Penetration (%)' ) ax . set_ylabel ( 'Average MOE Price Reduction (%)' ) Wall time: 753 ms Text(0, 0.5, 'Average MOE Price Reduction (%)') We'll fit a linear regression that includes 2020 linreg_results = scipy . stats . linregress ( x = s_GB_RES_pct_annual_avg , y = s_GB_MOE_pct_annual_avg ) linreg_results LinregressResult(slope=0.8635390661954275, intercept=-0.01260770809733558, rvalue=0.9407770535093302, pvalue=5.192107447861512e-06, stderr=0.0984074783808127, intercept_stderr=0.01545140550162944) And one that excludes 2020 max_year = 2019 linreg_results = scipy . stats . linregress ( x = s_GB_RES_pct_annual_avg . loc [: max_year ], y = s_GB_MOE_pct_annual_avg . loc [: max_year ]) linreg_results LinregressResult(slope=0.6675133238057994, intercept=0.004850800918389769, rvalue=0.9588281998847612, pvalue=3.215977627236812e-06, stderr=0.06590160431497857, intercept_stderr=0.009037033037260158) We'll also visualise the how the MOE time-series and trend fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.1 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22ccd2776d0> We can also attribute specific savings from the MOE to the different RES technologies wind_weight = df_EI . loc [ s_GB_MOE . index , 'wind' ] / df_EI . loc [ s_GB_MOE . index ][[ 'wind' , 'solar' ]] . mean ( axis = 1 ) solar_weight = df_EI . loc [ s_GB_MOE . index , 'solar' ] / df_EI . loc [ s_GB_MOE . index ][[ 'wind' , 'solar' ]] . mean ( axis = 1 ) s_wind_MOE = s_GB_MOE * wind_weight s_solar_MOE = s_GB_MOE * solar_weight # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_wind_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Wind' ) s_solar_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Solar' ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) To add some context we'll plot the MOE alongside the actual price, we'll also do this for both years at the extremes of the dataset time range year_1 = '2010' year_2 = '2020' month_num_to_name = dict ( zip ( np . arange ( 1 , 13 ), pd . date_range ( '2010-01-01' , '2010-12-31' , freq = 'M' ) . strftime ( '%b' ))) s_hourly_dam_10 = s_GB_DAM [ year_1 ] . groupby ( s_GB_DAM [ year_1 ] . index . month ) . mean () s_hourly_MOE_10 = s_GB_MOE [ year_1 ] . groupby ( s_GB_MOE [ year_1 ] . index . month ) . mean () s_hourly_dam_20 = s_GB_DAM [ year_2 ] . groupby ( s_GB_DAM [ year_2 ] . index . month ) . mean () s_hourly_MOE_20 = s_GB_MOE [ year_2 ] . groupby ( s_GB_MOE [ year_2 ] . index . month ) . mean () for s in [ s_hourly_dam_10 , s_hourly_MOE_10 , s_hourly_dam_20 , s_hourly_MOE_20 ]: s . index = s . index . map ( month_num_to_name ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 9 , 3 )) ( s_hourly_dam_10 + s_hourly_MOE_10 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 0 ]) s_hourly_dam_10 . plot . bar ( label = 'Price' , ax = axs [ 0 ]) ( s_hourly_dam_20 + s_hourly_MOE_20 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 1 ]) s_hourly_dam_20 . plot . bar ( label = 'Day-Ahead' , ax = axs [ 1 ]) axs [ 0 ] . set_title ( year_1 , y = 0.9 ) axs [ 1 ] . set_title ( year_2 , y = 0.9 ) for ax in axs : eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlabel ( '' ) axs [ 1 ] . legend ( frameon = False , bbox_to_anchor = ( 0.125 , 1.05 )) axs [ 1 ] . set_yticks ([]) eda . hide_spines ( axs [ 1 ], positions = [ 'left' ]) axs [ 0 ] . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') We'll create a combined dataframe of the predicted and observed time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : s_GB_price , 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 Which we'll then save as a csv df_GB_results_ts . to_csv ( '../data/results/GB_price.csv' )","title":"Quantifying &amp; Visualising the Merit Order Effect"},{"location":"dev-05-price-moe/#wind-capture-value-ratio","text":"Now we'll turn our focus to calculating the capture-value ratio of wind over time s_wind = df_EI [[ 'day_ahead_price' , 'wind' ]] . dropna ()[ 'wind' ] s_dam = df_EI [[ 'day_ahead_price' , 'wind' ]] . dropna ()[ 'day_ahead_price' ] We'll do this by weighting the price time-series using the wind generation data #exports def weighted_mean_s ( s , s_weight = None , dt_rng = pd . date_range ( '2009-12-01' , '2021-01-01' , freq = 'W' ), end_dt_delta_days = 7 ): \"\"\"Calculates the weighted average of a series\"\"\" capture_prices = dict () for start_dt in dt_rng : end_dt = start_dt + pd . Timedelta ( days = end_dt_delta_days ) if s_weight is not None : weights = s_weight [ start_dt : end_dt ] else : weights = None capture_prices [ start_dt ] = np . average ( s [ start_dt : end_dt ], weights = weights ) s_capture_prices = pd . Series ( capture_prices ) s_capture_prices . index = pd . to_datetime ( s_capture_prices . index ) return s_capture_prices s_wind_capture_prices = weighted_mean_s ( s_dam , s_wind ) s_dam_prices = weighted_mean_s ( s_dam ) s_wind_capture_prices . plot () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) <AxesSubplot:> We'll look at the distribution of the wind capture value ratio s_wind_capture_value_ratio = ( s_wind_capture_prices - s_dam_prices ) / s_dam_prices print ( round ( s_wind_capture_value_ratio . mean (), 4 )) sns . histplot ( s_wind_capture_value_ratio ) -0.028 <AxesSubplot:ylabel='Count'> We'll look at how this has changed on an annual basis fig , ax = plt . subplots ( dpi = 150 ) ( - 100 * s_wind_capture_value_ratio . groupby ( s_wind_capture_value_ratio . index . year ) . mean ()) . plot . bar ( ax = ax ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Wind Capture Price Suppression (%)' ) Text(0, 0.5, 'Wind Capture Price Suppression (%)') It could be interesting to look at the effect of price suppression on specific wind farms that have CfDs, and then estimate the increased burden on the tax-payer.","title":"Wind Capture-Value Ratio"},{"location":"dev-05-price-moe/#german-model","text":"We'll now repeat the price MOE calculations for Germany, starting by loading in the relevant data %% time df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () Wall time: 2.29 s local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan We'll clean up the data and do a quick plot df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_price = df_DE_model [ 'price' ] s_DE_demand = df_DE_model [ 'demand' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_dispatchable [ '2015-03' : '2015-09' ], s_DE_price [ '2015-03' : '2015-09' ], s = 1 ) ax . scatter ( s_DE_dispatchable [ '2020-03' : '2020-09' ], s_DE_price [ '2020-03' : '2020-09' ], s = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (EUR/MWh)' ) Text(0, 0.5, 'Price (EUR/MWh)') Let's create a visualisation that highlights the importance of regressing against dispatchable instead of total load %% time dt_min = '2020-10' dt_max = '2020-12' # Dispatchable x_dispatch = s_DE_dispatchable [ dt_min : dt_max ] . values y_dispatch = s_DE_price . loc [ s_DE_dispatchable . index ][ dt_min : dt_max ] . values x_pred_dispatch = np . linspace ( 18 , 67 , 41 ) y_pred_dispatch = lowess . lowess_fit_and_predict ( x_dispatch , y_dispatch , frac = 0.25 , num_fits = 50 , x_pred = x_pred_dispatch ) # Demand x_demand = s_DE_demand [ dt_min : dt_max ] . values y_demand = s_DE_price . loc [ s_DE_dispatchable . index ][ dt_min : dt_max ] . values x_pred_demand = np . linspace ( 35 , 85 , 51 ) y_pred_demand = lowess . lowess_fit_and_predict ( x_demand , y_demand , frac = 0.25 , num_fits = 50 , x_pred = x_pred_demand ) # Plotting fig , axs = plt . subplots ( dpi = 250 , ncols = 2 , figsize = ( 12 , 5 )) ax = axs [ 0 ] ax . plot ( x_pred_demand , y_pred_demand , linewidth = 1.5 , color = 'r' ) ax . scatter ( x_demand , y_demand , color = 'k' , s = 0.5 , alpha = 0.5 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) ax = axs [ 1 ] ax . plot ( x_pred_dispatch , y_pred_dispatch , linewidth = 1.5 , color = 'r' ) ax . scatter ( x_dispatch , y_dispatch , color = 'k' , s = 0.5 , alpha = 0.5 ) ax . set_xlim ( 10 , 80 ) eda . hide_spines ( ax , positions = [ 'top' , 'left' , 'right' ]) ax . set_yticks ([]) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) for ax in axs : ax . set_ylim ( - 20 , 120 ) fig . tight_layout () Wall time: 236 ms We'll load in our model %% time if load_existing_DE_model == True : smooth_dates = pickle . load ( open ( DE_model_fp , 'rb' )) else : reg_dates = pd . date_range ( '2015-01-01' , '2021-01-01' , freq = '13W' ) smooth_dates = lowess . SmoothDates () smooth_dates . fit ( s_DE_dispatchable . values , s_DE_price . values , dt_idx = s_DE_dispatchable . index , reg_dates = reg_dates , frac = 0.3 , num_fits = 31 , threshold_value = 26 ) pickle . dump ( smooth_dates , open ( model_fp , 'wb' )) Wall time: 364 ms Generate the regression surface prediction %% time x_pred = np . linspace ( - 5 , 90 , 951 ) dt_pred = pd . date_range ( '2015-01-01' , '2020-12-31' , freq = '1D' ) df_DE_pred = smooth_dates . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_DE_pred . index = np . round ( df_DE_pred . index , 1 ) df_DE_pred . head () Wall time: 276 ms Unnamed: 0 2015-01-01 2015-01-02 2015-01-03 2015-01-04 2015-01-05 2015-01-06 2015-01-07 2015-01-08 2015-01-09 2015-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 -38.4761 -38.4242 -38.3727 -38.3215 -38.2708 -38.2205 -38.1705 -38.1208 -38.0715 -38.0225 ... -48.2642 -48.2766 -48.289 -48.3016 -48.3144 -48.3272 -48.3402 -48.3534 -48.3667 -48.3801 -4.9 -38.3167 -38.2649 -38.2136 -38.1626 -38.112 -38.0618 -38.012 -37.9625 -37.9133 -37.8644 ... -48.0285 -48.0407 -48.0531 -48.0657 -48.0783 -48.0911 -48.104 -48.1171 -48.1304 -48.1437 -4.8 -38.1572 -38.1056 -38.0544 -38.0036 -37.9532 -37.9031 -37.8534 -37.8041 -37.7551 -37.7064 ... -47.7926 -47.8048 -47.8172 -47.8296 -47.8422 -47.8549 -47.8678 -47.8808 -47.894 -47.9073 -4.7 -37.9977 -37.9463 -37.8952 -37.8446 -37.7943 -37.7444 -37.6949 -37.6457 -37.5968 -37.5482 ... -47.5568 -47.5689 -47.5812 -47.5936 -47.6061 -47.6188 -47.6316 -47.6445 -47.6576 -47.6709 -4.6 -37.8382 -37.7869 -37.736 -37.6855 -37.6354 -37.5857 -37.5363 -37.4872 -37.4385 -37.3901 ... -47.3209 -47.333 -47.3452 -47.3575 -47.3699 -47.3825 -47.3953 -47.4082 -47.4212 -47.4344 As with the GB market you can see what is likely the effect of higher gas prices in 2018 df_DE_dispatchable_lims = construct_dispatchable_lims_df ( s_DE_dispatchable , rolling_w = 6 ) df_DE_pred_mask = construct_pred_mask_df ( df_DE_pred , df_DE_dispatchable_lims ) # Plotting min_y = 10 max_y = 70 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Price (EUR/MWh)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2015-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate some metrics for our model s_DE_pred_ts_dispatch , s_DE_pred_ts_demand = get_model_pred_ts ( s_DE_dispatchable , DE_model_fp , s_demand = s_DE_demand , x_pred = x_pred , dt_pred = dt_pred ) s_DE_dispatch_err = s_DE_pred_ts_dispatch - s_DE_price . loc [ s_DE_pred_ts_dispatch . index ] DE_dispatch_metrics = calc_error_metrics ( s_DE_dispatch_err ) s_DE_demand_err = s_DE_pred_ts_demand - s_DE_price . loc [ s_DE_pred_ts_demand . index ] DE_demand_metrics = calc_error_metrics ( s_DE_demand_err ) DE_dispatch_metrics {'median_abs_err': 4.257075090332123, 'mean_abs_err': 5.852023979176648, 'root_mean_square_error': 8.705711313706535} As well as the \\(r^{2}\\) score # 0.733 for Halttunen2021 r2_score ( s_DE_price . loc [ s_DE_pred_ts_dispatch . index ], s_DE_pred_ts_dispatch ) 0.7244797152665161 We'll now calculate the total savings start_date = '2015' end_date = '2020' s_DE_MOE = s_DE_pred_ts_demand - s_DE_pred_ts_dispatch s_DE_MOE = s_DE_MOE . dropna () s_DE_saving = s_DE_MOE * df_DE [ 'demand' ] . loc [ s_DE_MOE . index ] * 1000 total_saving = s_DE_saving [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was \u00a3 { total_saving : ,.0f } \" ) The total saving between 2015 and 2020 was \u00a355,856,316,374 And get some context for the market average and total volumes over the same period s_DE_mkt_cost = df_DE [ 'price' ] * df_DE [ 'demand' ] * 1000 avg_DE_HH_mkt_cost = s_DE_mkt_cost . mean () total_DE_mkt_cost = s_DE_mkt_cost [ start_date : end_date ] . sum () avg_DE_HH_mkt_cost , total_DE_mkt_cost (2076614.8441243432, 109047198694.6575) When we plot the percentage MOE over time we can see the large influence of lowered demand in 2020 s_DE_MOE_rolling = s_DE_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_DE_DAM_rolling = df_DE . loc [ s_DE_MOE . index ][ 'price' ] . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_pct_reduction = s_DE_MOE_rolling / s_DE_DAM_rolling s_DE_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> We'll quickly calculate the average percentage price suppresion s_DE_MOE [ start_date : end_date ] . mean () / ( df_DE [ 'price' ] + s_DE_MOE )[ start_date : end_date ] . mean () 0.3316044469891089 Of note there appeared to be relatively little impact on the MOE from the effects of the covid-19 response fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.3 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22ccaf82700> Interestingly, the forecast made in 2011 by Traber et al as to the German MOE in 2020 was quite close. \"In the absence of expanded deployment of renewable energy, a higher price increase of 20% can be expected\" in 2020 - source s_DE_MOE [ '2015' ] . mean (), s_DE_MOE [ '2020' ] . mean () (11.657604746082448, 22.174503965250132) We'll now disaggregate the MOE based on the wind and solar drivers wind_weight = df_DE . loc [ s_DE_MOE . index , 'Wind' ] / df_DE . loc [ s_DE_MOE . index ][[ 'Wind' , 'Solar' ]] . mean ( axis = 1 ) solar_weight = df_DE . loc [ s_DE_MOE . index , 'Solar' ] / df_DE . loc [ s_DE_MOE . index ][[ 'Wind' , 'Solar' ]] . mean ( axis = 1 ) s_wind_MOE = ( s_DE_MOE * wind_weight ) . dropna () s_solar_MOE = ( s_DE_MOE * solar_weight ) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_wind_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Wind' ) s_solar_MOE . rolling ( 48 * 28 ) . mean () . plot ( ax = ax , label = 'Solar' ) ax . set_ylim ( 0 , 55 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We'll look at how the MOE has changed over time in terms of the seasonal effect year_1 = '2015' year_2 = '2020' s_DE_DAM = df_DE . loc [ s_DE_MOE . index , 'price' ] month_num_to_name = dict ( zip ( np . arange ( 1 , 13 ), pd . date_range ( '2010-01-01' , '2010-12-31' , freq = 'M' ) . strftime ( '%b' ))) s_DE_hourly_dam_10 = s_DE_DAM [ year_1 ] . groupby ( s_DE_DAM [ year_1 ] . index . month ) . mean () s_DE_hourly_MOE_10 = s_DE_MOE [ year_1 ] . groupby ( s_DE_MOE [ year_1 ] . index . month ) . mean () s_DE_hourly_dam_20 = s_DE_DAM [ year_2 ] . groupby ( s_DE_DAM [ year_2 ] . index . month ) . mean () s_DE_hourly_MOE_20 = s_DE_MOE [ year_2 ] . groupby ( s_DE_MOE [ year_2 ] . index . month ) . mean () for s in [ s_DE_hourly_dam_10 , s_DE_hourly_MOE_10 , s_DE_hourly_dam_20 , s_DE_hourly_MOE_20 ]: s . index = s . index . map ( month_num_to_name ) # Plotting fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 9 , 3 )) ( s_DE_hourly_dam_10 + s_DE_hourly_MOE_10 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 0 ]) s_DE_hourly_dam_10 . plot . bar ( label = 'Price' , ax = axs [ 0 ]) ( s_DE_hourly_dam_20 + s_DE_hourly_MOE_20 ) . plot . bar ( color = 'C1' , label = 'MOE' , ax = axs [ 1 ]) s_DE_hourly_dam_20 . plot . bar ( label = 'Day-Ahead' , ax = axs [ 1 ]) axs [ 0 ] . set_title ( year_1 , y = 0.9 ) axs [ 1 ] . set_title ( year_2 , y = 0.9 ) for ax in axs : eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlabel ( '' ) axs [ 1 ] . legend ( frameon = False , bbox_to_anchor = ( 0.125 , 1.05 )) axs [ 1 ] . set_yticks ([]) eda . hide_spines ( axs [ 1 ], positions = [ 'left' ]) axs [ 0 ] . set_ylabel ( 'Price (EUR/MWh)' ) Text(0, 0.5, 'Price (EUR/MWh)') We'll also inspect the predictions on a sample day date = '2020-04-11' # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_DE_pred_ts_dispatch [ date ] . plot ( label = 'Prediction with RES' , ax = ax ) s_DE_pred_ts_demand [ date ] . plot ( label = 'Prediction without RES' , ax = ax ) s_DE_price [ date ] . plot ( label = 'Observed' , ax = ax ) ax . legend ( frameon = False , ncol = 3 , bbox_to_anchor = ( 1.075 , - 0.15 )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Price (EUR/MWh)' ) eda . hide_spines ( ax ) There's clearly a correlation between the annual MOE average and the RES percentage penetration %% time s_DE_MOE_annual_avg = s_DE_MOE . replace ( np . inf , np . nan ) . dropna () . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) s_DE_RES_pct_annual_avg = 100 * ( df_DE [[ 'Wind' , 'Solar' ]] . sum ( axis = 1 ) / df_DE [ 'demand' ]) . pipe ( lambda s : s . groupby ( s . index . year ) . mean ()) . loc [ 2015 :] . dropna () plt . scatter ( s_DE_MOE_annual_avg , s_DE_RES_pct_annual_avg ) Wall time: 51 ms <matplotlib.collections.PathCollection at 0x22c405d2dc0> We'll quickly calculate the gradient linreg_results = scipy . stats . linregress ( x = s_DE_RES_pct_annual_avg , y = s_DE_MOE_annual_avg ) linreg_results LinregressResult(slope=0.7365449501368756, intercept=-3.832555184231513, rvalue=0.9793888213890273, pvalue=0.0006328529981050964, stderr=0.07595057880689415, intercept_stderr=2.196641338586976) We'll get some context from the literature imperial_paper_slope = 0.63 pct_diff = round ( 100 * ( linreg_results . slope - imperial_paper_slope ) / imperial_paper_slope ) print ( f 'In this work the MOE increase per percentage penetration of RES was { pct_diff } % higher (for Germany) than the Imperial study' ) In this work the MOE increase per percentage penetration of RES was 17% higher (for Germany) than the Imperial study","title":"German Model"},{"location":"dev-05-price-moe/#plots","text":"In this section we'll generate some of the plots needed for the paper, starting with the heatmap of the price surfaces fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] min_y = 0 max_y = 60 htmp = sns . heatmap ( df_pred [ min_y : max_y ] . where ( df_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) # DE ax = axs [ 1 ] min_y = 10 max_y = 80 htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) set_date_ticks ( ax , '2015-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) fig . tight_layout () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) We'll also plot the MOE time-series fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.1 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 40 ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (\u00a3/MWh)' ) ax . legend ( frameon = False ) # DE ax = axs [ 1 ] ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.3 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_ylim ( 0 , 80 ) ax . set_xlim ( pd . to_datetime ( '2015' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (EUR/MWh)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x22c22be1040>","title":"Plots"},{"location":"dev-05-price-moe/#saving-results","text":"Additionaly we'll save the time-series predictions and model metrics, starting with the GB time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : s_GB_price , 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 Which we'll save to csv df_GB_results_ts . to_csv ( '../data/results/GB_price.csv' ) Then the DE time-series df_DE_results_ts = pd . DataFrame ({ 'prediction' : s_DE_pred_ts_dispatch , 'counterfactual' : s_DE_pred_ts_demand , 'observed' : s_DE_price , 'moe' : s_DE_MOE }) df_DE_results_ts . to_csv ( '../data/results/DE_price.csv' ) df_DE_results_ts . head () local_datetime prediction counterfactual observed moe 2015-01-04 23:00:00+00:00 24.4963 34.5304 22.34 10.0341 2015-01-05 00:00:00+00:00 23.7584 33.626 17.93 9.86759 2015-01-05 01:00:00+00:00 23.6626 33.9245 15.17 10.2619 2015-01-05 02:00:00+00:00 24.1342 34.5315 16.38 10.3973 2015-01-05 03:00:00+00:00 24.9343 36.2104 17.38 11.276 And finally the model metrics for both model_accuracy_metrics = { 'DE_dispatch' : DE_dispatch_metrics , 'DE_demand' : DE_demand_metrics , 'GB_dispatch' : GB_dispatch_metrics , 'GB_demand' : GB_demand_metrics } with open ( '../data/results/price_model_accuracy_metrics.json' , 'w' ) as fp : json . dump ( model_accuracy_metrics , fp ) JSON ( model_accuracy_metrics ) <IPython.core.display.JSON object>","title":"Saving Results"},{"location":"dev-06-carbon-surface-estimation-and-moe/","text":"Carbon Merit Order Effect Analysis \u00b6 This notebook outlines the analysis required to determine the carbon merit-order-effect of variable renewable generation in the GB and DE power markets. Imports \u00b6 import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from moepy import surface , moe , eda import pickle from sklearn.metrics import r2_score from moepy.surface import PicklableFunction User Inputs \u00b6 models_dir = '../data/models' Germany \u00b6 We'll start by loading in the data df_fuels_DE = pd . read_csv ( '../data/raw/energy_charts.csv' ) df_fuels_DE = df_fuels_DE . set_index ( 'local_datetime' ) df_fuels_DE . index = pd . to_datetime ( df_fuels_DE . index , utc = True ) . tz_convert ( 'Europe/Berlin' ) df_fuels_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance 2010-01-04 00:00:00+01:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 2010-01-04 01:00:00+01:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 2010-01-04 02:00:00+01:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 2010-01-04 03:00:00+01:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 2010-01-04 04:00:00+01:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 We now need to conver the fuel generation time-series into a carbon intensity time-series. We'll use data provided by volker-quaschning . The units are kgCO2 / kWh, equivalent to Tonnes/MWh. N.b. We are looking at the fuel emissions (not avg over lifecycle incl. CAPEX) DE_fuel_to_co2_intensity = { 'Biomass' : 0.39 , 'Brown Coal' : 0.36 , 'Gas' : 0.23 , 'Hard Coal' : 0.34 , 'Hydro Power' : 0 , 'Oil' : 0.28 , 'Others' : 0 , 'Pumped Storage' : 0 , 'Seasonal Storage' : 0 , 'Solar' : 0 , 'Uranium' : 0 , 'Wind' : 0 , 'net_balance' : 0 } s_DE_emissions_tonnes = ( df_fuels_DE . multiply ( 1e3 ) # converting to MWh [ DE_fuel_to_co2_intensity . keys ()] . multiply ( DE_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_DE_emissions_tonnes = s_DE_emissions_tonnes [ s_DE_emissions_tonnes > 2000 ] sns . histplot ( s_DE_emissions_tonnes ) <AxesSubplot:ylabel='Count'> We'll do a quick plot of the change over time df_DE = pd . DataFrame ({ 'demand' : df_fuels_DE . sum ( axis = 1 ), 'dispatchable' : df_fuels_DE . drop ( columns = [ 'Solar' , 'Wind' ]) . sum ( axis = 1 ), 'emissions' : s_DE_emissions_tonnes }) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_DE . loc [ '2010-09' : '2011-03' , 'dispatchable' ], df_DE . loc [ '2010-09' : '2011-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 10/11' ) ax . scatter ( df_DE . loc [ '2019-09' : '2020-03' , 'dispatchable' ], df_DE . loc [ '2019-09' : '2020-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 19/20' ) eda . hide_spines ( ax ) ax . set_xlim ( 10 , 80 ) ax . set_ylim ( 3000 , 20000 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'CO2 Emissions (Tonnes)' ) lgnd = ax . legend ( frameon = False ) # Need to increase the legend marker size lgnd . legendHandles [ 0 ] . _sizes = [ 30 ] lgnd . legendHandles [ 1 ] . _sizes = [ 30 ] for lh in lgnd . legendHandles : lh . set_alpha ( 1 ) Great Britain \u00b6 We'll now do the same for the GB system df_fuels_GB = pd . read_csv ( '../data/raw/electric_insights.csv' ) df_fuels_GB = df_fuels_GB . set_index ( 'local_datetime' ) df_fuels_GB . index = pd . to_datetime ( df_fuels_GB . index , utc = True ) . tz_convert ( 'Europe/Berlin' ) df_fuels_GB . head () local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 01:00:00+01:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 01:30:00+01:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 02:00:00+01:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 02:30:00+01:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 03:00:00+01:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll source the carbon intensity data from DUKES where possible and Electric Insights where it isn't. GB_fuel_to_co2_intensity = { 'nuclear' : 0 , 'biomass' : 0.121 , # from EI 'coal' : 0.921 , # DUKES 2018 value 'gas' : 0.377 , # DUKES 2018 value (lower than many CCGT estimates, let alone OCGT) 'hydro' : 0 , 'pumped_storage' : 0 , 'solar' : 0 , 'wind' : 0 , 'belgian' : 0.4 , 'dutch' : 0.474 , # from EI 'french' : 0.053 , # from EI 'ireland' : 0.458 , # from EI 'northern_ireland' : 0.458 # from EI } s_GB_emissions_tonnes = ( df_fuels_GB . multiply ( 1e3 * 0.5 ) # converting to MWh [ GB_fuel_to_co2_intensity . keys ()] . multiply ( GB_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) sns . histplot ( s_GB_emissions_tonnes ) <AxesSubplot:ylabel='Count'> We'll do the same visualisation for GB of how the carbon intensity has changed over time. Interestly we can see a clear fall in the carbon intensity of the GB dispatchable fleet, whereas with Germany the difference is negligible and if anything has slightly increased. df_GB = pd . DataFrame ({ 'demand' : df_fuels_GB [ GB_fuel_to_co2_intensity . keys ()] . sum ( axis = 1 ), 'dispatchable' : df_fuels_GB [ GB_fuel_to_co2_intensity . keys ()] . drop ( columns = [ 'solar' , 'wind' ]) . sum ( axis = 1 ), 'emissions' : s_GB_emissions_tonnes }) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( df_GB . loc [ '2010-09' : '2011-03' , 'dispatchable' ], df_GB . loc [ '2010-09' : '2011-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 10/11' ) ax . scatter ( df_GB . loc [ '2019-09' : '2020-03' , 'dispatchable' ], df_GB . loc [ '2019-09' : '2020-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 19/20' ) eda . hide_spines ( ax ) ax . set_xlim ( 5 , 60 ) ax . set_ylim ( 0 , 17500 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'CO2 Emissions (Tonnes)' ) lgnd = ax . legend ( frameon = False ) # Need to increase the legend marker size lgnd . legendHandles [ 0 ] . _sizes = [ 30 ] lgnd . legendHandles [ 1 ] . _sizes = [ 30 ] for lh in lgnd . legendHandles : lh . set_alpha ( 1 ) Model Fitting \u00b6 We're ready to define and fit our models model_definitions = { 'carbon_emissions_DE' : { 'dt_idx' : df_DE . index , 'x' : df_DE [ 'dispatchable' ] . values , 'y' : df_DE [ 'emissions' ] . values , 'reg_dates_start' : '2010-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , 'frac' : 0.3 , 'num_fits' : 31 , 'dates_smoothing_value' : 26 , 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : surface . get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'carbon_emissions_GB' : { 'dt_idx' : df_GB . index , 'x' : df_GB [ 'dispatchable' ] . values , 'y' : df_GB [ 'emissions' ] . values , 'reg_dates_start' : '2010-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , 'frac' : 0.3 , 'num_fits' : 31 , 'dates_smoothing_value' : 26 , 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : surface . get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) } } surface . fit_models ( model_definitions , models_dir ) carbon_emissions_DE 100% 4/4 [00:00 < 00:00, 0.00s/it] carbon_emissions_GB 100% 4/4 [00:00 < 00:00, 0.00s/it] German Model Evaluation & Carbon Savings Calculations \u00b6 We'll start by loading in the model %% time DE_model_fp = '../data/models/carbon_emissions_DE_p50.pkl' DE_smooth_dates = pickle . load ( open ( DE_model_fp , 'rb' )) DE_x_pred = np . linspace ( - 5 , 91 , 961 ) DE_dt_pred = pd . date_range ( '2010-01-01' , '2020-12-31' , freq = 'D' ) df_DE_pred = DE_smooth_dates . predict ( x_pred = DE_x_pred , dt_pred = DE_dt_pred ) df_DE_pred . index = np . round ( df_DE_pred . index , 1 ) df_DE_pred . head () Wall time: 2.94 s Unnamed: 0 2010-01-01 2010-01-02 2010-01-03 2010-01-04 2010-01-05 2010-01-06 2010-01-07 2010-01-08 2010-01-09 2010-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 3886.65 3879.82 3873.02 3866.24 3859.49 3852.78 3846.09 3839.43 3832.81 3826.22 ... 90.9967 89.6073 88.2112 86.8081 85.3975 83.9787 82.5515 81.1158 79.6717 78.2193 -4.9 3892.08 3885.26 3878.48 3871.72 3864.99 3858.3 3851.63 3844.99 3838.39 3831.82 ... 109.687 108.304 106.915 105.518 104.113 102.701 101.28 99.8512 98.4136 96.9678 -4.8 3897.58 3890.78 3884.01 3877.28 3870.57 3863.89 3857.24 3850.62 3844.04 3837.48 ... 128.39 127.013 125.63 124.24 122.842 121.436 120.021 118.599 117.168 115.728 -4.7 3903.14 3896.36 3889.62 3882.89 3876.2 3869.54 3862.91 3856.31 3849.75 3843.21 ... 147.105 145.734 144.357 142.973 141.581 140.182 138.774 137.358 135.933 134.501 -4.6 3908.73 3901.97 3895.24 3888.54 3881.86 3875.22 3868.61 3862.03 3855.48 3848.96 ... 165.828 164.463 163.092 161.715 160.33 158.936 157.535 156.125 154.707 153.281 We'll then visualise the surface prediction as a heatmap df_DE_dispatchable_lims = moe . construct_dispatchable_lims_df ( df_DE [ 'dispatchable' ], rolling_w = 6 ) df_DE_pred_mask = moe . construct_pred_mask_df ( df_DE_pred , df_DE_dispatchable_lims ) # Plotting min_y = 10 max_y = 80 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate the model metrics s_DE_pred_ts_dispatch , s_DE_pred_ts_demand = moe . get_model_pred_ts ( df_DE [ 'dispatchable' ], DE_model_fp , s_demand = df_DE [ 'demand' ], x_pred = DE_x_pred , dt_pred = DE_dt_pred ) s_DE_err = s_DE_pred_ts_dispatch - df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ] metrics = moe . calc_error_metrics ( s_DE_err ) metrics C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/96191 [01:24 < 00:00, 0.00s/it] 100% 0/96191 [01:15 < 00:00, 0.00s/it] {'median_abs_err': 603.7669189236494, 'mean_abs_err': 750.7665511092414, 'root_mean_square_error': 967.8069705064318} And \\(r^{2}\\) score r2_score ( df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ], s_DE_pred_ts_dispatch ) 0.9139682818721121 We're now ready to calculate the total savings start_date = '2010' end_date = '2020' s_DE_MOE = s_DE_pred_ts_demand - s_DE_pred_ts_dispatch s_DE_MOE = s_DE_MOE . dropna () total_saving = s_DE_MOE [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was { total_saving : ,.0f } Tonnes\" ) The total saving between 2010 and 2020 was 318,923,308 Tonnes And get some context for the average and total emissions over the same period s_DE_emissions = df_DE [ 'emissions' ] . loc [ s_DE_MOE . index ] avg_DE_HH_emissions = s_DE_emissions . mean () total_DE_emissions = s_DE_emissions [ start_date : end_date ] . sum () avg_DE_HH_emissions , total_DE_emissions (11870.320551662837, 1141818004.185) We'll calculate the average percentage emissions reduction due to the MOE total_saving / ( total_DE_emissions + total_saving ) 0.21832976535024085 Finally we'll generate the MOE percentage time-series s_DE_emissions_rolling = s_DE_emissions . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_rolling = s_DE_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_pct_reduction = s_DE_MOE_rolling / s_DE_emissions_rolling s_DE_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> British Model Evaluation & Carbon Savings Calculations \u00b6 We'll start by loading in the model %% time start_date = '2010-01-01' end_date = '2020-12-31' GB_model_fp = '../data/models/carbon_emissions_GB_p50.pkl' GB_smooth_dates = pickle . load ( open ( GB_model_fp , 'rb' )) GB_x_pred = np . linspace ( - 5 , 91 , 961 ) GB_dt_pred = pd . date_range ( start_date , end_date , freq = 'D' ) df_GB_pred = GB_smooth_dates . predict ( x_pred = GB_x_pred , dt_pred = GB_dt_pred ) df_GB_pred . index = np . round ( df_GB_pred . index , 1 ) df_GB_pred . head () Wall time: 3.42 s Unnamed: 0 2010-01-01 2010-01-02 2010-01-03 2010-01-04 2010-01-05 2010-01-06 2010-01-07 2010-01-08 2010-01-09 2010-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 -3464.32 -3464.5 -3464.68 -3464.85 -3465.03 -3465.2 -3465.38 -3465.55 -3465.72 -3465.89 ... -1132.67 -1132.65 -1132.62 -1132.59 -1132.57 -1132.54 -1132.52 -1132.49 -1132.46 -1132.44 -4.9 -3440 -3440.17 -3440.33 -3440.49 -3440.66 -3440.82 -3440.98 -3441.14 -3441.29 -3441.45 ... -1119.73 -1119.7 -1119.67 -1119.65 -1119.62 -1119.6 -1119.57 -1119.55 -1119.52 -1119.49 -4.8 -3415.64 -3415.79 -3415.94 -3416.1 -3416.24 -3416.39 -3416.54 -3416.68 -3416.83 -3416.97 ... -1106.78 -1106.75 -1106.73 -1106.7 -1106.68 -1106.65 -1106.63 -1106.6 -1106.57 -1106.55 -4.7 -3391.24 -3391.38 -3391.52 -3391.66 -3391.79 -3391.93 -3392.06 -3392.2 -3392.33 -3392.46 ... -1093.84 -1093.81 -1093.78 -1093.76 -1093.73 -1093.71 -1093.68 -1093.66 -1093.63 -1093.6 -4.6 -3366.81 -3366.93 -3367.06 -3367.18 -3367.31 -3367.43 -3367.55 -3367.67 -3367.79 -3367.91 ... -1080.89 -1080.87 -1080.84 -1080.82 -1080.79 -1080.77 -1080.74 -1080.71 -1080.69 -1080.66 We'll then visualise the surface prediction as a heatmap df_GB_dispatchable_lims = moe . construct_dispatchable_lims_df ( df_GB . loc [ start_date : end_date , 'dispatchable' ], rolling_w = 6 ) df_GB_pred_mask = moe . construct_pred_mask_df ( df_GB_pred , df_GB_dispatchable_lims ) # Plotting min_y = 0 max_y = 60 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_GB_pred [ min_y : max_y ] . where ( df_GB_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate the model metrics s_GB_pred_ts_dispatch , s_GB_pred_ts_demand = moe . get_model_pred_ts ( df_GB [ 'dispatchable' ], GB_model_fp , s_demand = df_GB [ 'demand' ], x_pred = GB_x_pred , dt_pred = GB_dt_pred ) s_GB_err = s_GB_pred_ts_dispatch - df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ] metrics = moe . calc_error_metrics ( s_GB_err ) metrics C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 39% 0/192336 [01:00 < 00:00, 0.00s/it] And \\(r^{2}\\) score r2_score ( df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ], s_GB_pred_ts_dispatch ) 0.9557674211115541 We're now ready to calculate the total savings s_GB_MOE = s_GB_pred_ts_demand - s_GB_pred_ts_dispatch s_GB_MOE = s_GB_MOE . dropna () total_saving = s_GB_MOE [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was { total_saving : ,.0f } Tonnes\" ) The total saving between 2010-01-01 and 2020-12-31 was 221,069,470 Tonnes And get some context for the average and total emissions over the same period s_GB_emissions = df_GB [ 'emissions' ] . loc [ s_GB_MOE . index ] avg_GB_HH_emissions = s_GB_emissions . mean () total_GB_emissions = s_GB_emissions [ start_date : end_date ] . sum () avg_GB_HH_emissions , total_GB_emissions (6034.469929827791, 1160645808.423358) We'll calculate the average percentage emissions reduction due to the MOE total_saving / ( total_GB_emissions + total_saving ) 0.15999639957299291 Finally we'll generate the MOE percentage time-series s_GB_emissions_rolling = s_GB_emissions . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_rolling = s_GB_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_pct_reduction = s_GB_MOE_rolling / s_GB_emissions_rolling s_GB_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'> Plots \u00b6 In this section we'll generate some of the plots needed for the paper, starting with the heatmap of the emissions surfaces fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] min_y = 0 max_y = 60 htmp = sns . heatmap ( df_GB_pred [ min_y : max_y ] . where ( df_GB_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) # DE ax = axs [ 1 ] min_y = 10 max_y = 80 htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) fig . tight_layout () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) We'll also plot the MOE time-series fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.2 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) # ax.set_ylim(0, 40) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (Tonnes CO2)' ) ax . legend ( frameon = False ) # DE ax = axs [ 1 ] ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.2 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (Tonnes CO2)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1fc9fa1f130> Saving Results \u00b6 Additionaly we'll save the time-series predictions and model metrics, starting with the GB time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ], 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2010-01-01 01:00:00+01:00 8414.72 8861.95 7949.28 447.23 2010-01-01 01:30:00+01:00 8535.22 8986.13 8030.29 450.914 2010-01-01 02:00:00+01:00 8414.72 8820.73 7974.91 406.012 2010-01-01 02:30:00+01:00 7983.82 8454.75 7707.51 470.937 2010-01-01 03:00:00+01:00 7645.47 8060.81 7456.2 415.345 Which we'll save to csv df_GB_results_ts . to_csv ( '../data/results/GB_carbon.csv' ) Then the DE time-series df_DE_results_ts = pd . DataFrame ({ 'prediction' : s_DE_pred_ts_dispatch , 'counterfactual' : s_DE_pred_ts_demand , 'observed' : df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ], 'moe' : s_DE_MOE }) df_DE_results_ts . to_csv ( '../data/results/DE_carbon.csv' ) df_DE_results_ts . head () local_datetime prediction counterfactual observed moe 2010-01-04 00:00:00+01:00 11981.6 12179 11883.8 197.359 2010-01-04 01:00:00+01:00 11518.7 11694.3 11488.6 175.594 2010-01-04 02:00:00+01:00 11400.5 11577.4 11228.5 176.903 2010-01-04 03:00:00+01:00 11071.5 11251.9 10962.7 180.385 2010-01-04 04:00:00+01:00 11311.5 11518.7 10892.5 207.187","title":"Carbon Curve Estimation & MOE"},{"location":"dev-06-carbon-surface-estimation-and-moe/#carbon-merit-order-effect-analysis","text":"This notebook outlines the analysis required to determine the carbon merit-order-effect of variable renewable generation in the GB and DE power markets.","title":"Carbon Merit Order Effect Analysis"},{"location":"dev-06-carbon-surface-estimation-and-moe/#imports","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from moepy import surface , moe , eda import pickle from sklearn.metrics import r2_score from moepy.surface import PicklableFunction","title":"Imports"},{"location":"dev-06-carbon-surface-estimation-and-moe/#user-inputs","text":"models_dir = '../data/models'","title":"User Inputs"},{"location":"dev-06-carbon-surface-estimation-and-moe/#germany","text":"We'll start by loading in the data df_fuels_DE = pd . read_csv ( '../data/raw/energy_charts.csv' ) df_fuels_DE = df_fuels_DE . set_index ( 'local_datetime' ) df_fuels_DE . index = pd . to_datetime ( df_fuels_DE . index , utc = True ) . tz_convert ( 'Europe/Berlin' ) df_fuels_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance 2010-01-04 00:00:00+01:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 2010-01-04 01:00:00+01:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 2010-01-04 02:00:00+01:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 2010-01-04 03:00:00+01:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 2010-01-04 04:00:00+01:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 We now need to conver the fuel generation time-series into a carbon intensity time-series. We'll use data provided by volker-quaschning . The units are kgCO2 / kWh, equivalent to Tonnes/MWh. N.b. We are looking at the fuel emissions (not avg over lifecycle incl. CAPEX) DE_fuel_to_co2_intensity = { 'Biomass' : 0.39 , 'Brown Coal' : 0.36 , 'Gas' : 0.23 , 'Hard Coal' : 0.34 , 'Hydro Power' : 0 , 'Oil' : 0.28 , 'Others' : 0 , 'Pumped Storage' : 0 , 'Seasonal Storage' : 0 , 'Solar' : 0 , 'Uranium' : 0 , 'Wind' : 0 , 'net_balance' : 0 } s_DE_emissions_tonnes = ( df_fuels_DE . multiply ( 1e3 ) # converting to MWh [ DE_fuel_to_co2_intensity . keys ()] . multiply ( DE_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_DE_emissions_tonnes = s_DE_emissions_tonnes [ s_DE_emissions_tonnes > 2000 ] sns . histplot ( s_DE_emissions_tonnes ) <AxesSubplot:ylabel='Count'> We'll do a quick plot of the change over time df_DE = pd . DataFrame ({ 'demand' : df_fuels_DE . sum ( axis = 1 ), 'dispatchable' : df_fuels_DE . drop ( columns = [ 'Solar' , 'Wind' ]) . sum ( axis = 1 ), 'emissions' : s_DE_emissions_tonnes }) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_DE . loc [ '2010-09' : '2011-03' , 'dispatchable' ], df_DE . loc [ '2010-09' : '2011-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 10/11' ) ax . scatter ( df_DE . loc [ '2019-09' : '2020-03' , 'dispatchable' ], df_DE . loc [ '2019-09' : '2020-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 19/20' ) eda . hide_spines ( ax ) ax . set_xlim ( 10 , 80 ) ax . set_ylim ( 3000 , 20000 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'CO2 Emissions (Tonnes)' ) lgnd = ax . legend ( frameon = False ) # Need to increase the legend marker size lgnd . legendHandles [ 0 ] . _sizes = [ 30 ] lgnd . legendHandles [ 1 ] . _sizes = [ 30 ] for lh in lgnd . legendHandles : lh . set_alpha ( 1 )","title":"Germany"},{"location":"dev-06-carbon-surface-estimation-and-moe/#great-britain","text":"We'll now do the same for the GB system df_fuels_GB = pd . read_csv ( '../data/raw/electric_insights.csv' ) df_fuels_GB = df_fuels_GB . set_index ( 'local_datetime' ) df_fuels_GB . index = pd . to_datetime ( df_fuels_GB . index , utc = True ) . tz_convert ( 'Europe/Berlin' ) df_fuels_GB . head () local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2009-01-01 01:00:00+01:00 58.05 1 74.74 74.74 -0.6 21278 555 6.973 0 17.65 ... 38.329 -0.404 nan nan 0 0 1.977 0 0 -0.161 2009-01-01 01:30:00+01:00 56.33 2 74.89 74.89 -0.6 21442 558 6.968 0 17.77 ... 38.461 -0.527 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 02:00:00+01:00 52.98 3 76.41 76.41 -0.6 21614 569 6.97 0 18.07 ... 37.986 -1.018 nan nan 0 0 1.977 0 0 -0.16 2009-01-01 02:30:00+01:00 50.39 4 37.73 37.73 -0.6 21320 578 6.969 0 18.022 ... 36.864 -1.269 nan nan 0 0 1.746 0 0 -0.16 2009-01-01 03:00:00+01:00 48.7 5 59 59 -0.6 21160 585 6.96 0 17.998 ... 36.18 -1.566 nan nan 0 0 1.73 0 0 -0.16 We'll source the carbon intensity data from DUKES where possible and Electric Insights where it isn't. GB_fuel_to_co2_intensity = { 'nuclear' : 0 , 'biomass' : 0.121 , # from EI 'coal' : 0.921 , # DUKES 2018 value 'gas' : 0.377 , # DUKES 2018 value (lower than many CCGT estimates, let alone OCGT) 'hydro' : 0 , 'pumped_storage' : 0 , 'solar' : 0 , 'wind' : 0 , 'belgian' : 0.4 , 'dutch' : 0.474 , # from EI 'french' : 0.053 , # from EI 'ireland' : 0.458 , # from EI 'northern_ireland' : 0.458 # from EI } s_GB_emissions_tonnes = ( df_fuels_GB . multiply ( 1e3 * 0.5 ) # converting to MWh [ GB_fuel_to_co2_intensity . keys ()] . multiply ( GB_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) sns . histplot ( s_GB_emissions_tonnes ) <AxesSubplot:ylabel='Count'> We'll do the same visualisation for GB of how the carbon intensity has changed over time. Interestly we can see a clear fall in the carbon intensity of the GB dispatchable fleet, whereas with Germany the difference is negligible and if anything has slightly increased. df_GB = pd . DataFrame ({ 'demand' : df_fuels_GB [ GB_fuel_to_co2_intensity . keys ()] . sum ( axis = 1 ), 'dispatchable' : df_fuels_GB [ GB_fuel_to_co2_intensity . keys ()] . drop ( columns = [ 'solar' , 'wind' ]) . sum ( axis = 1 ), 'emissions' : s_GB_emissions_tonnes }) . dropna () # Plotting fig , ax = plt . subplots ( dpi = 250 ) ax . scatter ( df_GB . loc [ '2010-09' : '2011-03' , 'dispatchable' ], df_GB . loc [ '2010-09' : '2011-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 10/11' ) ax . scatter ( df_GB . loc [ '2019-09' : '2020-03' , 'dispatchable' ], df_GB . loc [ '2019-09' : '2020-03' , 'emissions' ], s = 0.1 , alpha = 0.25 , label = 'Winter 19/20' ) eda . hide_spines ( ax ) ax . set_xlim ( 5 , 60 ) ax . set_ylim ( 0 , 17500 ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'CO2 Emissions (Tonnes)' ) lgnd = ax . legend ( frameon = False ) # Need to increase the legend marker size lgnd . legendHandles [ 0 ] . _sizes = [ 30 ] lgnd . legendHandles [ 1 ] . _sizes = [ 30 ] for lh in lgnd . legendHandles : lh . set_alpha ( 1 )","title":"Great Britain"},{"location":"dev-06-carbon-surface-estimation-and-moe/#model-fitting","text":"We're ready to define and fit our models model_definitions = { 'carbon_emissions_DE' : { 'dt_idx' : df_DE . index , 'x' : df_DE [ 'dispatchable' ] . values , 'y' : df_DE [ 'emissions' ] . values , 'reg_dates_start' : '2010-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , 'frac' : 0.3 , 'num_fits' : 31 , 'dates_smoothing_value' : 26 , 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : surface . get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) }, 'carbon_emissions_GB' : { 'dt_idx' : df_GB . index , 'x' : df_GB [ 'dispatchable' ] . values , 'y' : df_GB [ 'emissions' ] . values , 'reg_dates_start' : '2010-01-04' , 'reg_dates_end' : '2021-01-01' , 'reg_dates_freq' : '13W' , 'frac' : 0.3 , 'num_fits' : 31 , 'dates_smoothing_value' : 26 , 'dates_smoothing_units' : 'W' , 'fit_kwarg_sets' : surface . get_fit_kwarg_sets ( qs = [ 0.16 , 0.5 , 0.84 ]) } } surface . fit_models ( model_definitions , models_dir ) carbon_emissions_DE 100% 4/4 [00:00 < 00:00, 0.00s/it] carbon_emissions_GB 100% 4/4 [00:00 < 00:00, 0.00s/it]","title":"Model Fitting"},{"location":"dev-06-carbon-surface-estimation-and-moe/#german-model-evaluation-carbon-savings-calculations","text":"We'll start by loading in the model %% time DE_model_fp = '../data/models/carbon_emissions_DE_p50.pkl' DE_smooth_dates = pickle . load ( open ( DE_model_fp , 'rb' )) DE_x_pred = np . linspace ( - 5 , 91 , 961 ) DE_dt_pred = pd . date_range ( '2010-01-01' , '2020-12-31' , freq = 'D' ) df_DE_pred = DE_smooth_dates . predict ( x_pred = DE_x_pred , dt_pred = DE_dt_pred ) df_DE_pred . index = np . round ( df_DE_pred . index , 1 ) df_DE_pred . head () Wall time: 2.94 s Unnamed: 0 2010-01-01 2010-01-02 2010-01-03 2010-01-04 2010-01-05 2010-01-06 2010-01-07 2010-01-08 2010-01-09 2010-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 3886.65 3879.82 3873.02 3866.24 3859.49 3852.78 3846.09 3839.43 3832.81 3826.22 ... 90.9967 89.6073 88.2112 86.8081 85.3975 83.9787 82.5515 81.1158 79.6717 78.2193 -4.9 3892.08 3885.26 3878.48 3871.72 3864.99 3858.3 3851.63 3844.99 3838.39 3831.82 ... 109.687 108.304 106.915 105.518 104.113 102.701 101.28 99.8512 98.4136 96.9678 -4.8 3897.58 3890.78 3884.01 3877.28 3870.57 3863.89 3857.24 3850.62 3844.04 3837.48 ... 128.39 127.013 125.63 124.24 122.842 121.436 120.021 118.599 117.168 115.728 -4.7 3903.14 3896.36 3889.62 3882.89 3876.2 3869.54 3862.91 3856.31 3849.75 3843.21 ... 147.105 145.734 144.357 142.973 141.581 140.182 138.774 137.358 135.933 134.501 -4.6 3908.73 3901.97 3895.24 3888.54 3881.86 3875.22 3868.61 3862.03 3855.48 3848.96 ... 165.828 164.463 163.092 161.715 160.33 158.936 157.535 156.125 154.707 153.281 We'll then visualise the surface prediction as a heatmap df_DE_dispatchable_lims = moe . construct_dispatchable_lims_df ( df_DE [ 'dispatchable' ], rolling_w = 6 ) df_DE_pred_mask = moe . construct_pred_mask_df ( df_DE_pred , df_DE_dispatchable_lims ) # Plotting min_y = 10 max_y = 80 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate the model metrics s_DE_pred_ts_dispatch , s_DE_pred_ts_demand = moe . get_model_pred_ts ( df_DE [ 'dispatchable' ], DE_model_fp , s_demand = df_DE [ 'demand' ], x_pred = DE_x_pred , dt_pred = DE_dt_pred ) s_DE_err = s_DE_pred_ts_dispatch - df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ] metrics = moe . calc_error_metrics ( s_DE_err ) metrics C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 100% 0/96191 [01:24 < 00:00, 0.00s/it] 100% 0/96191 [01:15 < 00:00, 0.00s/it] {'median_abs_err': 603.7669189236494, 'mean_abs_err': 750.7665511092414, 'root_mean_square_error': 967.8069705064318} And \\(r^{2}\\) score r2_score ( df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ], s_DE_pred_ts_dispatch ) 0.9139682818721121 We're now ready to calculate the total savings start_date = '2010' end_date = '2020' s_DE_MOE = s_DE_pred_ts_demand - s_DE_pred_ts_dispatch s_DE_MOE = s_DE_MOE . dropna () total_saving = s_DE_MOE [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was { total_saving : ,.0f } Tonnes\" ) The total saving between 2010 and 2020 was 318,923,308 Tonnes And get some context for the average and total emissions over the same period s_DE_emissions = df_DE [ 'emissions' ] . loc [ s_DE_MOE . index ] avg_DE_HH_emissions = s_DE_emissions . mean () total_DE_emissions = s_DE_emissions [ start_date : end_date ] . sum () avg_DE_HH_emissions , total_DE_emissions (11870.320551662837, 1141818004.185) We'll calculate the average percentage emissions reduction due to the MOE total_saving / ( total_DE_emissions + total_saving ) 0.21832976535024085 Finally we'll generate the MOE percentage time-series s_DE_emissions_rolling = s_DE_emissions . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_rolling = s_DE_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_DE_MOE_pct_reduction = s_DE_MOE_rolling / s_DE_emissions_rolling s_DE_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'>","title":"German Model Evaluation &amp; Carbon Savings Calculations"},{"location":"dev-06-carbon-surface-estimation-and-moe/#british-model-evaluation-carbon-savings-calculations","text":"We'll start by loading in the model %% time start_date = '2010-01-01' end_date = '2020-12-31' GB_model_fp = '../data/models/carbon_emissions_GB_p50.pkl' GB_smooth_dates = pickle . load ( open ( GB_model_fp , 'rb' )) GB_x_pred = np . linspace ( - 5 , 91 , 961 ) GB_dt_pred = pd . date_range ( start_date , end_date , freq = 'D' ) df_GB_pred = GB_smooth_dates . predict ( x_pred = GB_x_pred , dt_pred = GB_dt_pred ) df_GB_pred . index = np . round ( df_GB_pred . index , 1 ) df_GB_pred . head () Wall time: 3.42 s Unnamed: 0 2010-01-01 2010-01-02 2010-01-03 2010-01-04 2010-01-05 2010-01-06 2010-01-07 2010-01-08 2010-01-09 2010-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 -5 -3464.32 -3464.5 -3464.68 -3464.85 -3465.03 -3465.2 -3465.38 -3465.55 -3465.72 -3465.89 ... -1132.67 -1132.65 -1132.62 -1132.59 -1132.57 -1132.54 -1132.52 -1132.49 -1132.46 -1132.44 -4.9 -3440 -3440.17 -3440.33 -3440.49 -3440.66 -3440.82 -3440.98 -3441.14 -3441.29 -3441.45 ... -1119.73 -1119.7 -1119.67 -1119.65 -1119.62 -1119.6 -1119.57 -1119.55 -1119.52 -1119.49 -4.8 -3415.64 -3415.79 -3415.94 -3416.1 -3416.24 -3416.39 -3416.54 -3416.68 -3416.83 -3416.97 ... -1106.78 -1106.75 -1106.73 -1106.7 -1106.68 -1106.65 -1106.63 -1106.6 -1106.57 -1106.55 -4.7 -3391.24 -3391.38 -3391.52 -3391.66 -3391.79 -3391.93 -3392.06 -3392.2 -3392.33 -3392.46 ... -1093.84 -1093.81 -1093.78 -1093.76 -1093.73 -1093.71 -1093.68 -1093.66 -1093.63 -1093.6 -4.6 -3366.81 -3366.93 -3367.06 -3367.18 -3367.31 -3367.43 -3367.55 -3367.67 -3367.79 -3367.91 ... -1080.89 -1080.87 -1080.84 -1080.82 -1080.79 -1080.77 -1080.74 -1080.71 -1080.69 -1080.66 We'll then visualise the surface prediction as a heatmap df_GB_dispatchable_lims = moe . construct_dispatchable_lims_df ( df_GB . loc [ start_date : end_date , 'dispatchable' ], rolling_w = 6 ) df_GB_pred_mask = moe . construct_pred_mask_df ( df_GB_pred , df_GB_dispatchable_lims ) # Plotting min_y = 0 max_y = 60 fig , ax = plt . subplots ( dpi = 150 ) htmp = sns . heatmap ( df_GB_pred [ min_y : max_y ] . where ( df_GB_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) Text(69.58333333333334, 0.5, 'Demand - [Solar + Wind] (GW)') We'll calculate the model metrics s_GB_pred_ts_dispatch , s_GB_pred_ts_demand = moe . get_model_pred_ts ( df_GB [ 'dispatchable' ], GB_model_fp , s_demand = df_GB [ 'demand' ], x_pred = GB_x_pred , dt_pred = GB_dt_pred ) s_GB_err = s_GB_pred_ts_dispatch - df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ] metrics = moe . calc_error_metrics ( s_GB_err ) metrics C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5277: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead. start_slice, end_slice = self.slice_locs(start, end, step=step, kind=kind) 39% 0/192336 [01:00 < 00:00, 0.00s/it] And \\(r^{2}\\) score r2_score ( df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ], s_GB_pred_ts_dispatch ) 0.9557674211115541 We're now ready to calculate the total savings s_GB_MOE = s_GB_pred_ts_demand - s_GB_pred_ts_dispatch s_GB_MOE = s_GB_MOE . dropna () total_saving = s_GB_MOE [ start_date : end_date ] . sum () print ( f \"The total saving between { start_date } and { end_date } was { total_saving : ,.0f } Tonnes\" ) The total saving between 2010-01-01 and 2020-12-31 was 221,069,470 Tonnes And get some context for the average and total emissions over the same period s_GB_emissions = df_GB [ 'emissions' ] . loc [ s_GB_MOE . index ] avg_GB_HH_emissions = s_GB_emissions . mean () total_GB_emissions = s_GB_emissions [ start_date : end_date ] . sum () avg_GB_HH_emissions , total_GB_emissions (6034.469929827791, 1160645808.423358) We'll calculate the average percentage emissions reduction due to the MOE total_saving / ( total_GB_emissions + total_saving ) 0.15999639957299291 Finally we'll generate the MOE percentage time-series s_GB_emissions_rolling = s_GB_emissions . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_rolling = s_GB_MOE . rolling ( 48 * 28 ) . mean () . dropna () s_GB_MOE_pct_reduction = s_GB_MOE_rolling / s_GB_emissions_rolling s_GB_MOE_pct_reduction . plot () <AxesSubplot:xlabel='local_datetime'>","title":"British Model Evaluation &amp; Carbon Savings Calculations"},{"location":"dev-06-carbon-surface-estimation-and-moe/#plots","text":"In this section we'll generate some of the plots needed for the paper, starting with the heatmap of the emissions surfaces fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] min_y = 0 max_y = 60 htmp = sns . heatmap ( df_GB_pred [ min_y : max_y ] . where ( df_GB_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Half-Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) # DE ax = axs [ 1 ] min_y = 10 max_y = 80 htmp = sns . heatmap ( df_DE_pred [ min_y : max_y ] . where ( df_DE_pred_mask [ min_y : max_y ], np . nan ) . iloc [:: - 1 ], ax = ax , cbar_kws = { 'label' : 'Hourly CO2 Emissions (Tonnes)' }) moe . set_ticks ( ax , np . arange ( min_y , max_y , 10 ), axis = 'y' ) moe . set_date_ticks ( ax , '2010-01-01' , '2021-01-01' , freq = '1YS' , date_format = '%Y' , axis = 'x' ) for _ , spine in htmp . spines . items (): spine . set_visible ( True ) eda . hide_spines ( ax ) ax . set_ylabel ( 'Demand - [Solar + Wind] (GW)' ) fig . tight_layout () C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\sklearn\\utils\\validation.py:63: FutureWarning: Arrays of bytes/strings is being converted to decimal numbers if dtype='numeric'. This behavior is deprecated in 0.24 and will be removed in 1.1 (renaming of 0.26). Please convert your data to numeric values explicitly instead. return f(*args, **kwargs) We'll also plot the MOE time-series fig , axs = plt . subplots ( dpi = 150 , ncols = 2 , figsize = ( 14 , 5 )) # GB ax = axs [ 0 ] ax . scatter ( s_GB_MOE . index , s_GB_MOE , s = 0.01 , alpha = 0.2 , color = 'k' , label = None ) s_GB_MOE_rolling . plot ( color = 'r' , linewidth = 1 , ax = ax , label = '28-Day Average' ) eda . hide_spines ( ax ) # ax.set_ylim(0, 40) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (Tonnes CO2)' ) ax . legend ( frameon = False ) # DE ax = axs [ 1 ] ax . scatter ( s_DE_MOE . index , s_DE_MOE , s = 0.05 , alpha = 0.2 , color = 'k' , label = None ) ax . plot ( s_DE_MOE_rolling . index , s_DE_MOE_rolling , color = 'r' , linewidth = 1.5 , label = '28-Day Average' ) eda . hide_spines ( ax ) ax . set_xlim ( pd . to_datetime ( '2010' ), pd . to_datetime ( '2021' )) ax . set_xlabel ( '' ) ax . set_ylabel ( 'Merit Order Effect (Tonnes CO2)' ) ax . legend ( frameon = False ) <matplotlib.legend.Legend at 0x1fc9fa1f130>","title":"Plots"},{"location":"dev-06-carbon-surface-estimation-and-moe/#saving-results","text":"Additionaly we'll save the time-series predictions and model metrics, starting with the GB time-series df_GB_results_ts = pd . DataFrame ({ 'prediction' : s_GB_pred_ts_dispatch , 'counterfactual' : s_GB_pred_ts_demand , 'observed' : df_GB . loc [ s_GB_pred_ts_dispatch . index , 'emissions' ], 'moe' : s_GB_MOE }) df_GB_results_ts . head () local_datetime prediction counterfactual observed moe 2010-01-01 01:00:00+01:00 8414.72 8861.95 7949.28 447.23 2010-01-01 01:30:00+01:00 8535.22 8986.13 8030.29 450.914 2010-01-01 02:00:00+01:00 8414.72 8820.73 7974.91 406.012 2010-01-01 02:30:00+01:00 7983.82 8454.75 7707.51 470.937 2010-01-01 03:00:00+01:00 7645.47 8060.81 7456.2 415.345 Which we'll save to csv df_GB_results_ts . to_csv ( '../data/results/GB_carbon.csv' ) Then the DE time-series df_DE_results_ts = pd . DataFrame ({ 'prediction' : s_DE_pred_ts_dispatch , 'counterfactual' : s_DE_pred_ts_demand , 'observed' : df_DE . loc [ s_DE_pred_ts_dispatch . index , 'emissions' ], 'moe' : s_DE_MOE }) df_DE_results_ts . to_csv ( '../data/results/DE_carbon.csv' ) df_DE_results_ts . head () local_datetime prediction counterfactual observed moe 2010-01-04 00:00:00+01:00 11981.6 12179 11883.8 197.359 2010-01-04 01:00:00+01:00 11518.7 11694.3 11488.6 175.594 2010-01-04 02:00:00+01:00 11400.5 11577.4 11228.5 176.903 2010-01-04 03:00:00+01:00 11071.5 11251.9 10962.7 180.385 2010-01-04 04:00:00+01:00 11311.5 11518.7 10892.5 207.187","title":"Saving Results"},{"location":"dev-07-prediction-confidence-and-intervals/","text":"Prediction & Confidence Intervals \u00b6 This notebook outlines the calculation of the prediction and confidence intervals for the GB and DE price MOE models Imports \u00b6 import numpy as np import pandas as pd import pickle import matplotlib.pyplot as plt from moepy import lowess , eda from moepy.surface import PicklableFunction from ipypb import track Great Britain \u00b6 We'll start by loading and cleaning the data for GB df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) We'll then calculate the estimate for the 68% prediction interval def get_pred_intvl ( low_q_fp , high_q_fp ): \"\"\"Calculates the prediction interval between the low and high quantile models specified\"\"\" smooth_dates_low = pickle . load ( open ( low_q_fp , 'rb' )) smooth_dates_high = pickle . load ( open ( high_q_fp , 'rb' )) x_pred = np . linspace ( 3 , 61 , 581 ) dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' ) df_pred_low = smooth_dates_low . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred_low . index = np . round ( df_pred_low . index , 1 ) df_pred_high = smooth_dates_high . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred_high . index = np . round ( df_pred_high . index , 1 ) df_pred_intvl = df_pred_high - df_pred_low return df_pred_intvl %% time df_pred_68pct_intvl_GB = get_pred_intvl ( '../data/models/DAM_price_GB_p16.pkl' , '../data/models/DAM_price_GB_p84.pkl' ) df_pred_68pct_intvl_GB . head () Wall time: 11.4 s Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 3 -4.77878 -4.80147 -4.82393 -4.84614 -4.86811 -4.88982 -4.91126 -4.93241 -4.95325 -4.97378 ... 41.4778 41.4841 41.4904 41.4967 41.503 41.5093 41.5157 41.522 41.5284 41.5348 3.1 -4.73778 -4.76051 -4.78301 -4.80526 -4.82727 -4.84902 -4.8705 -4.89169 -4.91257 -4.93314 ... 41.3044 41.3107 41.317 41.3233 41.3296 41.3359 41.3422 41.3486 41.3549 41.3613 3.2 -4.69656 -4.71933 -4.74186 -4.76415 -4.7862 -4.80799 -4.82951 -4.85074 -4.87167 -4.89228 ... 41.1312 41.1375 41.1437 41.15 41.1563 41.1626 41.169 41.1753 41.1816 41.188 3.3 -4.65507 -4.67787 -4.70044 -4.72276 -4.74485 -4.76668 -4.78824 -4.80951 -4.83047 -4.85113 ... 40.9582 40.9645 40.9707 40.977 40.9833 40.9896 40.9959 41.0023 41.0086 41.0149 3.4 -4.61326 -4.63609 -4.65869 -4.68105 -4.70317 -4.72504 -4.74664 -4.76794 -4.78895 -4.80964 ... 40.7855 40.7918 40.798 40.8043 40.8106 40.8169 40.8232 40.8295 40.8358 40.8421 We can see that we get some quantile crossing at the extreme ends of the dispatch curve which is why some of our 68% interval values are negative, to counter this we'll weight our prediction interval by how often that part of the dispatch curve is where the price clears at. s_pred_idx_weight = s_dispatchable . round ( 1 ) . value_counts () . sort_index () dispatchable_gen_idxs = sorted ( list ( set ( s_pred_idx_weight . index ) . intersection ( df_pred_68pct_intvl_GB . index ))) pred_68pct_intvl = np . average ( df_pred_68pct_intvl_GB . mean ( axis = 1 ) . loc [ dispatchable_gen_idxs ], weights = s_pred_idx_weight . loc [ dispatchable_gen_idxs ]) print ( f 'The 68% prediction interval for GB is { round ( pred_68pct_intvl , 2 ) } \u00a3/MWh' ) The 68% prediction interval for GB is 16.32 \u00a3/MWh We'll use our bootstrapping helper function to calculate the confidence interval of the GB model %% capture center_dts = pd . date_range ( s_price . index . min (), s_price . index . max (), freq = '3MS' ) + pd . Timedelta ( days = 45 ) all_conf_intvl_95pct = [] for center_dt in track ( center_dts ): s_price_subset = s_price [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] s_dispatchable_subset = s_dispatchable [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] df_bootstrap = lowess . bootstrap_model ( s_price_subset . values , s_dispatchable_subset . values , num_runs = 100 , frac = 0.3 , num_fits = 10 ) conf_intvl_95pct = df_bootstrap . replace ( 0 , np . nan ) . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . mean ( axis = 1 ) . iloc [ 0 ] all_conf_intvl_95pct += [ conf_intvl_95pct ] conf_intvl_95pct_GB = np . array ( all_conf_intvl_95pct ) . mean () print ( f 'The 95% confidence interval for GB is { round ( conf_intvl_95pct_GB , 2 ) } \u00a3/MWh' ) The 95% confidence interval for GB is 1.03 \u00a3/MWh Germany \u00b6 We'll start by loading and cleaning the data for DE %% time df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_price = df_DE_model [ 'price' ] s_DE_demand = df_DE_model [ 'demand' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) Wall time: 1.72 s We'll then calculate the estimate for the 68% prediction interval %% time df_pred_68pct_intvl_DE = get_pred_intvl ( '../data/models/DAM_price_DE_p16.pkl' , '../data/models/DAM_price_DE_p84.pkl' ) s_pred_idx_weight = s_DE_dispatchable . round ( 1 ) . value_counts () . sort_index () dispatchable_gen_idxs = sorted ( list ( set ( s_pred_idx_weight . index ) . intersection ( df_pred_68pct_intvl_DE . index ))) pred_68pct_intvl = np . average ( df_pred_68pct_intvl_DE . mean ( axis = 1 ) . loc [ dispatchable_gen_idxs ], weights = s_pred_idx_weight . loc [ dispatchable_gen_idxs ]) print ( f 'The 68% prediction interval for DE is { round ( pred_68pct_intvl , 2 ) } EUR/MWh' ) The 68% prediction interval for DE is 13.79 EUR/MWh Wall time: 1.5 s We'll use our bootstrapping helper function to calculate the confidence interval of the GB model %% capture center_dts = pd . date_range ( s_DE_price . index . min (), s_DE_price . index . max (), freq = '3MS' ) + pd . Timedelta ( days = 45 ) all_conf_intvl_95pct = [] for center_dt in track ( center_dts ): s_price_subset = s_DE_price [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] s_dispatchable_subset = s_DE_dispatchable [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] df_bootstrap = lowess . bootstrap_model ( s_price_subset . values , s_dispatchable_subset . values , num_runs = 100 , frac = 0.3 , num_fits = 10 ) conf_intvl_95pct = df_bootstrap . replace ( 0 , np . nan ) . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . mean ( axis = 1 ) . iloc [ 0 ] all_conf_intvl_95pct += [ conf_intvl_95pct ] conf_intvl_95pct_DE = np . array ( all_conf_intvl_95pct ) . mean () print ( f 'The 95% confidence interval for DE is { round ( conf_intvl_95pct_DE , 2 ) } EUR/MWh' ) The 95% confidence interval for DE is 1.69 EUR/MWh","title":"Prediction & Confidence Intervals"},{"location":"dev-07-prediction-confidence-and-intervals/#prediction-confidence-intervals","text":"This notebook outlines the calculation of the prediction and confidence intervals for the GB and DE price MOE models","title":"Prediction &amp; Confidence Intervals"},{"location":"dev-07-prediction-confidence-and-intervals/#imports","text":"import numpy as np import pandas as pd import pickle import matplotlib.pyplot as plt from moepy import lowess , eda from moepy.surface import PicklableFunction from ipypb import track","title":"Imports"},{"location":"dev-07-prediction-confidence-and-intervals/#great-britain","text":"We'll start by loading and cleaning the data for GB df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) We'll then calculate the estimate for the 68% prediction interval def get_pred_intvl ( low_q_fp , high_q_fp ): \"\"\"Calculates the prediction interval between the low and high quantile models specified\"\"\" smooth_dates_low = pickle . load ( open ( low_q_fp , 'rb' )) smooth_dates_high = pickle . load ( open ( high_q_fp , 'rb' )) x_pred = np . linspace ( 3 , 61 , 581 ) dt_pred = pd . date_range ( '2009-01-01' , '2020-12-31' , freq = '1D' ) df_pred_low = smooth_dates_low . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred_low . index = np . round ( df_pred_low . index , 1 ) df_pred_high = smooth_dates_high . predict ( x_pred = x_pred , dt_pred = dt_pred ) df_pred_high . index = np . round ( df_pred_high . index , 1 ) df_pred_intvl = df_pred_high - df_pred_low return df_pred_intvl %% time df_pred_68pct_intvl_GB = get_pred_intvl ( '../data/models/DAM_price_GB_p16.pkl' , '../data/models/DAM_price_GB_p84.pkl' ) df_pred_68pct_intvl_GB . head () Wall time: 11.4 s Unnamed: 0 2009-01-01 2009-01-02 2009-01-03 2009-01-04 2009-01-05 2009-01-06 2009-01-07 2009-01-08 2009-01-09 2009-01-10 ... 2020-12-22 2020-12-23 2020-12-24 2020-12-25 2020-12-26 2020-12-27 2020-12-28 2020-12-29 2020-12-30 2020-12-31 3 -4.77878 -4.80147 -4.82393 -4.84614 -4.86811 -4.88982 -4.91126 -4.93241 -4.95325 -4.97378 ... 41.4778 41.4841 41.4904 41.4967 41.503 41.5093 41.5157 41.522 41.5284 41.5348 3.1 -4.73778 -4.76051 -4.78301 -4.80526 -4.82727 -4.84902 -4.8705 -4.89169 -4.91257 -4.93314 ... 41.3044 41.3107 41.317 41.3233 41.3296 41.3359 41.3422 41.3486 41.3549 41.3613 3.2 -4.69656 -4.71933 -4.74186 -4.76415 -4.7862 -4.80799 -4.82951 -4.85074 -4.87167 -4.89228 ... 41.1312 41.1375 41.1437 41.15 41.1563 41.1626 41.169 41.1753 41.1816 41.188 3.3 -4.65507 -4.67787 -4.70044 -4.72276 -4.74485 -4.76668 -4.78824 -4.80951 -4.83047 -4.85113 ... 40.9582 40.9645 40.9707 40.977 40.9833 40.9896 40.9959 41.0023 41.0086 41.0149 3.4 -4.61326 -4.63609 -4.65869 -4.68105 -4.70317 -4.72504 -4.74664 -4.76794 -4.78895 -4.80964 ... 40.7855 40.7918 40.798 40.8043 40.8106 40.8169 40.8232 40.8295 40.8358 40.8421 We can see that we get some quantile crossing at the extreme ends of the dispatch curve which is why some of our 68% interval values are negative, to counter this we'll weight our prediction interval by how often that part of the dispatch curve is where the price clears at. s_pred_idx_weight = s_dispatchable . round ( 1 ) . value_counts () . sort_index () dispatchable_gen_idxs = sorted ( list ( set ( s_pred_idx_weight . index ) . intersection ( df_pred_68pct_intvl_GB . index ))) pred_68pct_intvl = np . average ( df_pred_68pct_intvl_GB . mean ( axis = 1 ) . loc [ dispatchable_gen_idxs ], weights = s_pred_idx_weight . loc [ dispatchable_gen_idxs ]) print ( f 'The 68% prediction interval for GB is { round ( pred_68pct_intvl , 2 ) } \u00a3/MWh' ) The 68% prediction interval for GB is 16.32 \u00a3/MWh We'll use our bootstrapping helper function to calculate the confidence interval of the GB model %% capture center_dts = pd . date_range ( s_price . index . min (), s_price . index . max (), freq = '3MS' ) + pd . Timedelta ( days = 45 ) all_conf_intvl_95pct = [] for center_dt in track ( center_dts ): s_price_subset = s_price [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] s_dispatchable_subset = s_dispatchable [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] df_bootstrap = lowess . bootstrap_model ( s_price_subset . values , s_dispatchable_subset . values , num_runs = 100 , frac = 0.3 , num_fits = 10 ) conf_intvl_95pct = df_bootstrap . replace ( 0 , np . nan ) . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . mean ( axis = 1 ) . iloc [ 0 ] all_conf_intvl_95pct += [ conf_intvl_95pct ] conf_intvl_95pct_GB = np . array ( all_conf_intvl_95pct ) . mean () print ( f 'The 95% confidence interval for GB is { round ( conf_intvl_95pct_GB , 2 ) } \u00a3/MWh' ) The 95% confidence interval for GB is 1.03 \u00a3/MWh","title":"Great Britain"},{"location":"dev-07-prediction-confidence-and-intervals/#germany","text":"We'll start by loading and cleaning the data for DE %% time df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_price = df_DE_model [ 'price' ] s_DE_demand = df_DE_model [ 'demand' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) Wall time: 1.72 s We'll then calculate the estimate for the 68% prediction interval %% time df_pred_68pct_intvl_DE = get_pred_intvl ( '../data/models/DAM_price_DE_p16.pkl' , '../data/models/DAM_price_DE_p84.pkl' ) s_pred_idx_weight = s_DE_dispatchable . round ( 1 ) . value_counts () . sort_index () dispatchable_gen_idxs = sorted ( list ( set ( s_pred_idx_weight . index ) . intersection ( df_pred_68pct_intvl_DE . index ))) pred_68pct_intvl = np . average ( df_pred_68pct_intvl_DE . mean ( axis = 1 ) . loc [ dispatchable_gen_idxs ], weights = s_pred_idx_weight . loc [ dispatchable_gen_idxs ]) print ( f 'The 68% prediction interval for DE is { round ( pred_68pct_intvl , 2 ) } EUR/MWh' ) The 68% prediction interval for DE is 13.79 EUR/MWh Wall time: 1.5 s We'll use our bootstrapping helper function to calculate the confidence interval of the GB model %% capture center_dts = pd . date_range ( s_DE_price . index . min (), s_DE_price . index . max (), freq = '3MS' ) + pd . Timedelta ( days = 45 ) all_conf_intvl_95pct = [] for center_dt in track ( center_dts ): s_price_subset = s_DE_price [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] s_dispatchable_subset = s_DE_dispatchable [ center_dt - pd . Timedelta ( days = 45 ): center_dt + pd . Timedelta ( days = 45 )] df_bootstrap = lowess . bootstrap_model ( s_price_subset . values , s_dispatchable_subset . values , num_runs = 100 , frac = 0.3 , num_fits = 10 ) conf_intvl_95pct = df_bootstrap . replace ( 0 , np . nan ) . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . mean ( axis = 1 ) . iloc [ 0 ] all_conf_intvl_95pct += [ conf_intvl_95pct ] conf_intvl_95pct_DE = np . array ( all_conf_intvl_95pct ) . mean () print ( f 'The 95% confidence interval for DE is { round ( conf_intvl_95pct_DE , 2 ) } EUR/MWh' ) The 95% confidence interval for DE is 1.69 EUR/MWh","title":"Germany"},{"location":"dev-08-hyper-parameter-tuning/","text":"Hyper-Parameter Tuning \u00b6 This notebook outlines the hyper-parameter optimisation procedure used to tune the models Imports \u00b6 import numpy as np import pandas as pd from sklearn.metrics import mean_absolute_error , make_scorer from sklearn.model_selection import train_test_split from skopt.plots import plot_objective from skopt.space import Real , Integer import matplotlib.pyplot as plt from moepy import lowess , eda Data Loading \u00b6 We'll start with the GB data df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) s_dispatchable . head () local_datetime 2009-01-01 00:00:00+00:00 38.181 2009-01-01 00:30:00+00:00 38.304 2009-01-01 01:00:00+00:00 37.839 2009-01-01 01:30:00+00:00 36.716 2009-01-01 02:00:00+00:00 36.020 dtype: float64 then also load in the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_demand = df_DE_model [ 'demand' ] s_DE_price = df_DE_model [ 'price' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 ) Monkey Patching skopt \u00b6 Due to some changes in the latest release of scikit-learn several classes and functions in skopt were broken at the time this research was carried out. This section provides code for monkey-patching skopt to ensure that it continues working. We'll start by loading in the relevant imports from joblib import Parallel , delayed from scipy.stats import rankdata from skopt import BayesSearchCV import os import codecs from ipypb import track from warnings import warn from functools import partial from distutils.dir_util import copy_tree from collections.abc import Iterable , Sized from collections import defaultdict import sklearn from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.ensemble import RandomForestRegressor from sklearn.base import is_classifier , clone from sklearn.utils.validation import indexable try : from sklearn.metrics import check_scoring except ImportError : from sklearn.metrics.scorer import check_scoring We'll re-define the bayes_search_CV_init function def bayes_search_CV_init ( self , estimator , search_spaces , optimizer_kwargs = None , n_iter = 50 , scoring = None , fit_params = None , n_jobs = 1 , n_points = 1 , iid = True , refit = True , cv = None , verbose = 0 , pre_dispatch = '2*n_jobs' , random_state = None , error_score = 'raise' , return_train_score = False ): self . search_spaces = search_spaces self . n_iter = n_iter self . n_points = n_points self . random_state = random_state self . optimizer_kwargs = optimizer_kwargs self . _check_search_space ( self . search_spaces ) self . fit_params = fit_params self . iid = None super ( BayesSearchCV , self ) . __init__ ( estimator = estimator , scoring = scoring , n_jobs = n_jobs , refit = refit , cv = cv , verbose = verbose , pre_dispatch = pre_dispatch , error_score = error_score , return_train_score = return_train_score ) BayesSearchCV . __init__ = bayes_search_CV_init As well as the bayes_search_CV__fit function def bayes_search_CV__fit ( self , X , y , groups , parameter_iterable ): \"\"\" Actual fitting, performing the search over parameters. Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X .../sklearn/model_selection/_search.py \"\"\" estimator = self . estimator cv = sklearn . model_selection . _validation . check_cv ( self . cv , y , classifier = is_classifier ( estimator )) self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) X , y , groups = indexable ( X , y , groups ) n_splits = cv . get_n_splits ( X , y , groups ) if self . verbose > 0 and isinstance ( parameter_iterable , Sized ): n_candidates = len ( parameter_iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits )) base_estimator = clone ( self . estimator ) pre_dispatch = self . pre_dispatch cv_iter = list ( cv . split ( X , y , groups )) out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch )( delayed ( sklearn . model_selection . _validation . _fit_and_score )( clone ( base_estimator ), X , y , self . scorer_ , train , test , self . verbose , parameters , fit_params = self . fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv_iter ) # if one choose to see train score, \"out\" will contain train score info if self . return_train_score : ( train_scores , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * out ) else : from warnings import warn ( fit_failed , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * [ a . values () for a in out ]) candidate_params = parameters [:: n_splits ] n_candidates = len ( candidate_params ) results = dict () def _store ( key_name , array , weights = None , splits = False , rank = False ): \"\"\"A small helper to store the scores/times to the cv_results_\"\"\" array = np . array ( array , dtype = np . float64 ) . reshape ( n_candidates , n_splits ) if splits : for split_i in range ( n_splits ): results [ \"split %d _ %s \" % ( split_i , key_name )] = array [:, split_i ] array_means = np . average ( array , axis = 1 , weights = weights ) results [ 'mean_ %s ' % key_name ] = array_means # Weighted std is not directly available in numpy array_stds = np . sqrt ( np . average (( array - array_means [:, np . newaxis ]) ** 2 , axis = 1 , weights = weights )) results [ 'std_ %s ' % key_name ] = array_stds if rank : results [ \"rank_ %s \" % key_name ] = np . asarray ( rankdata ( - array_means , method = 'min' ), dtype = np . int32 ) # Computed the (weighted) mean and std for test scores alone # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples n_test_samples = np . array ( n_test_samples [: n_splits ], dtype = np . int ) _store ( 'test_score' , test_scores , splits = True , rank = True , weights = n_test_samples if self . iid else None ) if self . return_train_score : _store ( 'train_score' , train_scores , splits = True ) _store ( 'fit_time' , fit_time ) _store ( 'score_time' , score_time ) best_index = np . flatnonzero ( results [ \"rank_test_score\" ] == 1 )[ 0 ] best_parameters = candidate_params [ best_index ] # Use one MaskedArray and mask all the places where the param is not # applicable for that candidate. Use defaultdict as each candidate may # not contain all the params param_results = defaultdict ( partial ( np . ma . array , np . empty ( n_candidates ,), mask = True , dtype = object )) for cand_i , params in enumerate ( candidate_params ): for name , value in params . items (): # An all masked empty array gets created for the key # `\"param_%s\" % name` at the first occurence of `name`. # Setting the value at an index also unmasks that index param_results [ \"param_ %s \" % name ][ cand_i ] = value results . update ( param_results ) # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params' results [ 'params' ] = candidate_params self . cv_results_ = results self . best_index_ = best_index self . n_splits_ = n_splits if self . refit : # fit the best estimator using the entire dataset # clone first to work around broken estimators best_estimator = clone ( base_estimator ) . set_params ( ** best_parameters ) if y is not None : best_estimator . fit ( X , y , ** self . fit_params ) else : best_estimator . fit ( X , ** self . fit_params ) self . best_estimator_ = best_estimator return self BayesSearchCV . _fit = bayes_search_CV__fit Optimisation \u00b6 We're now ready to carry out our model optimisation %% time start_date = '2017-01-01' end_date = '2019-01-01' x = s_DE_dispatchable [ start_date : end_date ] y = s_DE_price [ start_date : end_date ] pred_reg_dates = pd . date_range ( start_date , end_date , freq = 'D' ) lowess_dates = lowess . LowessDates ( frac = 0.5 , threshold_value = 26 , pred_reg_dates = pred_reg_dates ) search_spaces = { 'frac' : Real ( 0.35 , 1 , 'uniform' ), 'threshold_value' : Integer ( 10 , 52 , 'uniform' ) } fit_params = { 'reg_dates' : pd . date_range ( start_date , end_date , freq = '7W' ), 'num_fits' : 10 , 'reg_anchors' : np . round ( np . arange ( np . floor ( x . min ()) - 5 , np . ceil ( x . max ()) + 5 , 0.1 ), 1 ) } opt = BayesSearchCV ( lowess_dates , search_spaces , optimizer_kwargs = { 'random_state' : 42 }, n_iter = 20 , verbose = 0 , cv = 4 , # 8 works well for me as that's how many concurrent workers I can use fit_params = fit_params , n_jobs = 5 # -1 ) fit_BayesSearchCV = True if fit_BayesSearchCV == True : opt . fit ( x . round ( 1 ), y ) print ( f 'Cross-validation score: { opt . best_score_ : .2f } ' ) print ( f ' \\n Best params: \\n { opt . best_params_ } ' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" 100% 15/15 [02:25 < 00:09, 9.69s/it] Cross-validation score: 0.40 Best params: OrderedDict([('frac', 0.35), ('threshold_value', 21)]) Wall time: 1h 48min 47s We'll visualise the fitted objective surface axs = plot_objective ( opt . optimizer_results_ [ 0 ], cmap = 'magma_r' , show_points = False ) fig = plt . gcf () fig . set_dpi ( 250 ) fig . delaxes ( axs [ 0 ][ 0 ]) fig . delaxes ( axs [ 0 ][ 1 ]) fig . delaxes ( axs [ 1 ][ 1 ]) ax = axs [ 1 ][ 0 ] ax . set_xlabel ( 'Dispatchable Generation \\n Bandwidth (Fraction)' ) ax . set_ylabel ( 'Date Smoothing \\n Bandwidth (Weeks)' ) Text(0, 0.5, 'Date Smoothing\\nBandwidth (Weeks)')","title":"Hyper-Parameter Tuning"},{"location":"dev-08-hyper-parameter-tuning/#hyper-parameter-tuning","text":"This notebook outlines the hyper-parameter optimisation procedure used to tune the models","title":"Hyper-Parameter Tuning"},{"location":"dev-08-hyper-parameter-tuning/#imports","text":"import numpy as np import pandas as pd from sklearn.metrics import mean_absolute_error , make_scorer from sklearn.model_selection import train_test_split from skopt.plots import plot_objective from skopt.space import Real , Integer import matplotlib.pyplot as plt from moepy import lowess , eda","title":"Imports"},{"location":"dev-08-hyper-parameter-tuning/#data-loading","text":"We'll start with the GB data df_EI = eda . load_EI_df ( '../data/raw/electric_insights.csv' ) df_EI_model = df_EI [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () s_price = df_EI_model [ 'day_ahead_price' ] s_dispatchable = df_EI_model [ 'demand' ] - df_EI_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) s_dispatchable . head () local_datetime 2009-01-01 00:00:00+00:00 38.181 2009-01-01 00:30:00+00:00 38.304 2009-01-01 01:00:00+00:00 37.839 2009-01-01 01:30:00+00:00 36.716 2009-01-01 02:00:00+00:00 36.020 dtype: float64 then also load in the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE_model = df_DE [[ 'price' , 'demand' , 'Solar' , 'Wind' ]] . dropna () s_DE_demand = df_DE_model [ 'demand' ] s_DE_price = df_DE_model [ 'price' ] s_DE_dispatchable = df_DE_model [ 'demand' ] - df_DE_model [[ 'Solar' , 'Wind' ]] . sum ( axis = 1 )","title":"Data Loading"},{"location":"dev-08-hyper-parameter-tuning/#monkey-patching-skopt","text":"Due to some changes in the latest release of scikit-learn several classes and functions in skopt were broken at the time this research was carried out. This section provides code for monkey-patching skopt to ensure that it continues working. We'll start by loading in the relevant imports from joblib import Parallel , delayed from scipy.stats import rankdata from skopt import BayesSearchCV import os import codecs from ipypb import track from warnings import warn from functools import partial from distutils.dir_util import copy_tree from collections.abc import Iterable , Sized from collections import defaultdict import sklearn from sklearn import linear_model from sklearn.metrics import r2_score from sklearn.ensemble import RandomForestRegressor from sklearn.base import is_classifier , clone from sklearn.utils.validation import indexable try : from sklearn.metrics import check_scoring except ImportError : from sklearn.metrics.scorer import check_scoring We'll re-define the bayes_search_CV_init function def bayes_search_CV_init ( self , estimator , search_spaces , optimizer_kwargs = None , n_iter = 50 , scoring = None , fit_params = None , n_jobs = 1 , n_points = 1 , iid = True , refit = True , cv = None , verbose = 0 , pre_dispatch = '2*n_jobs' , random_state = None , error_score = 'raise' , return_train_score = False ): self . search_spaces = search_spaces self . n_iter = n_iter self . n_points = n_points self . random_state = random_state self . optimizer_kwargs = optimizer_kwargs self . _check_search_space ( self . search_spaces ) self . fit_params = fit_params self . iid = None super ( BayesSearchCV , self ) . __init__ ( estimator = estimator , scoring = scoring , n_jobs = n_jobs , refit = refit , cv = cv , verbose = verbose , pre_dispatch = pre_dispatch , error_score = error_score , return_train_score = return_train_score ) BayesSearchCV . __init__ = bayes_search_CV_init As well as the bayes_search_CV__fit function def bayes_search_CV__fit ( self , X , y , groups , parameter_iterable ): \"\"\" Actual fitting, performing the search over parameters. Taken from https://github.com/scikit-learn/scikit-learn/blob/0.18.X .../sklearn/model_selection/_search.py \"\"\" estimator = self . estimator cv = sklearn . model_selection . _validation . check_cv ( self . cv , y , classifier = is_classifier ( estimator )) self . scorer_ = check_scoring ( self . estimator , scoring = self . scoring ) X , y , groups = indexable ( X , y , groups ) n_splits = cv . get_n_splits ( X , y , groups ) if self . verbose > 0 and isinstance ( parameter_iterable , Sized ): n_candidates = len ( parameter_iterable ) print ( \"Fitting {0} folds for each of {1} candidates, totalling\" \" {2} fits\" . format ( n_splits , n_candidates , n_candidates * n_splits )) base_estimator = clone ( self . estimator ) pre_dispatch = self . pre_dispatch cv_iter = list ( cv . split ( X , y , groups )) out = Parallel ( n_jobs = self . n_jobs , verbose = self . verbose , pre_dispatch = pre_dispatch )( delayed ( sklearn . model_selection . _validation . _fit_and_score )( clone ( base_estimator ), X , y , self . scorer_ , train , test , self . verbose , parameters , fit_params = self . fit_params , return_train_score = self . return_train_score , return_n_test_samples = True , return_times = True , return_parameters = True , error_score = self . error_score ) for parameters in parameter_iterable for train , test in cv_iter ) # if one choose to see train score, \"out\" will contain train score info if self . return_train_score : ( train_scores , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * out ) else : from warnings import warn ( fit_failed , test_scores , n_test_samples , fit_time , score_time , parameters ) = zip ( * [ a . values () for a in out ]) candidate_params = parameters [:: n_splits ] n_candidates = len ( candidate_params ) results = dict () def _store ( key_name , array , weights = None , splits = False , rank = False ): \"\"\"A small helper to store the scores/times to the cv_results_\"\"\" array = np . array ( array , dtype = np . float64 ) . reshape ( n_candidates , n_splits ) if splits : for split_i in range ( n_splits ): results [ \"split %d _ %s \" % ( split_i , key_name )] = array [:, split_i ] array_means = np . average ( array , axis = 1 , weights = weights ) results [ 'mean_ %s ' % key_name ] = array_means # Weighted std is not directly available in numpy array_stds = np . sqrt ( np . average (( array - array_means [:, np . newaxis ]) ** 2 , axis = 1 , weights = weights )) results [ 'std_ %s ' % key_name ] = array_stds if rank : results [ \"rank_ %s \" % key_name ] = np . asarray ( rankdata ( - array_means , method = 'min' ), dtype = np . int32 ) # Computed the (weighted) mean and std for test scores alone # NOTE test_sample counts (weights) remain the same for all candidates n_test_samples n_test_samples = np . array ( n_test_samples [: n_splits ], dtype = np . int ) _store ( 'test_score' , test_scores , splits = True , rank = True , weights = n_test_samples if self . iid else None ) if self . return_train_score : _store ( 'train_score' , train_scores , splits = True ) _store ( 'fit_time' , fit_time ) _store ( 'score_time' , score_time ) best_index = np . flatnonzero ( results [ \"rank_test_score\" ] == 1 )[ 0 ] best_parameters = candidate_params [ best_index ] # Use one MaskedArray and mask all the places where the param is not # applicable for that candidate. Use defaultdict as each candidate may # not contain all the params param_results = defaultdict ( partial ( np . ma . array , np . empty ( n_candidates ,), mask = True , dtype = object )) for cand_i , params in enumerate ( candidate_params ): for name , value in params . items (): # An all masked empty array gets created for the key # `\"param_%s\" % name` at the first occurence of `name`. # Setting the value at an index also unmasks that index param_results [ \"param_ %s \" % name ][ cand_i ] = value results . update ( param_results ) # Store a list of param dicts at est_sample_counts = np.array(n_test_samples[:n_splits], key 'params' results [ 'params' ] = candidate_params self . cv_results_ = results self . best_index_ = best_index self . n_splits_ = n_splits if self . refit : # fit the best estimator using the entire dataset # clone first to work around broken estimators best_estimator = clone ( base_estimator ) . set_params ( ** best_parameters ) if y is not None : best_estimator . fit ( X , y , ** self . fit_params ) else : best_estimator . fit ( X , ** self . fit_params ) self . best_estimator_ = best_estimator return self BayesSearchCV . _fit = bayes_search_CV__fit","title":"Monkey Patching skopt"},{"location":"dev-08-hyper-parameter-tuning/#optimisation","text":"We're now ready to carry out our model optimisation %% time start_date = '2017-01-01' end_date = '2019-01-01' x = s_DE_dispatchable [ start_date : end_date ] y = s_DE_price [ start_date : end_date ] pred_reg_dates = pd . date_range ( start_date , end_date , freq = 'D' ) lowess_dates = lowess . LowessDates ( frac = 0.5 , threshold_value = 26 , pred_reg_dates = pred_reg_dates ) search_spaces = { 'frac' : Real ( 0.35 , 1 , 'uniform' ), 'threshold_value' : Integer ( 10 , 52 , 'uniform' ) } fit_params = { 'reg_dates' : pd . date_range ( start_date , end_date , freq = '7W' ), 'num_fits' : 10 , 'reg_anchors' : np . round ( np . arange ( np . floor ( x . min ()) - 5 , np . ceil ( x . max ()) + 5 , 0.1 ), 1 ) } opt = BayesSearchCV ( lowess_dates , search_spaces , optimizer_kwargs = { 'random_state' : 42 }, n_iter = 20 , verbose = 0 , cv = 4 , # 8 works well for me as that's how many concurrent workers I can use fit_params = fit_params , n_jobs = 5 # -1 ) fit_BayesSearchCV = True if fit_BayesSearchCV == True : opt . fit ( x . round ( 1 ), y ) print ( f 'Cross-validation score: { opt . best_score_ : .2f } ' ) print ( f ' \\n Best params: \\n { opt . best_params_ } ' ) C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" C:\\Users\\Ayrto\\anaconda3\\envs\\MOE\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before. warnings.warn(\"The objective has been evaluated \" 100% 15/15 [02:25 < 00:09, 9.69s/it] Cross-validation score: 0.40 Best params: OrderedDict([('frac', 0.35), ('threshold_value', 21)]) Wall time: 1h 48min 47s We'll visualise the fitted objective surface axs = plot_objective ( opt . optimizer_results_ [ 0 ], cmap = 'magma_r' , show_points = False ) fig = plt . gcf () fig . set_dpi ( 250 ) fig . delaxes ( axs [ 0 ][ 0 ]) fig . delaxes ( axs [ 0 ][ 1 ]) fig . delaxes ( axs [ 1 ][ 1 ]) ax = axs [ 1 ][ 0 ] ax . set_xlabel ( 'Dispatchable Generation \\n Bandwidth (Fraction)' ) ax . set_ylabel ( 'Date Smoothing \\n Bandwidth (Weeks)' ) Text(0, 0.5, 'Date Smoothing\\nBandwidth (Weeks)')","title":"Optimisation"},{"location":"dev-09-tables-and-figures/","text":"Tables & Figures Generation \u00b6 This notebook provides a programmatic workflow for generating the tables used in the MOE paper, as well as the diagram to show the time-adaptive smoothing weights. Imports \u00b6 import json import numpy as np import pandas as pd import matplotlib.pyplot as plt from IPython.display import Latex , JSON from moepy import eda , lowess Tables \u00b6 Power Systems Overview \u00b6 We'll first load in the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan Clean it up then calculate the relevant summary statistics s_DE_RES_output = df_DE [[ 'Wind' , 'Solar' ]] . sum ( axis = 1 ) s_DE_demand = df_DE [ 'demand' ] s_DE_price = df_DE [ 'price' ] s_DE_RES_pct = s_DE_RES_output / s_DE_demand DE_2020_RES_pct = s_DE_RES_pct [ '2020' ] . mean () DE_2020_demand_avg = s_DE_demand [ '2020' ] . mean () DE_2020_price_avg = s_DE_price [ '2020' ] . mean () DE_2020_RES_pct , DE_2020_demand_avg , DE_2020_price_avg (0.3593124152992342, 55.956133452868855, 30.469415917112606) We'll also estimate the carbon intensity DE_fuel_to_co2_intensity = { 'Biomass' : 0.39 , 'Brown Coal' : 0.36 , 'Gas' : 0.23 , 'Hard Coal' : 0.34 , 'Hydro Power' : 0 , 'Oil' : 0.28 , 'Others' : 0 , 'Pumped Storage' : 0 , 'Seasonal Storage' : 0 , 'Solar' : 0 , 'Uranium' : 0 , 'Wind' : 0 , 'net_balance' : 0 } s_DE_emissions_tonnes = ( df_DE [ DE_fuel_to_co2_intensity . keys ()] . multiply ( 1e3 ) # converting to MWh . multiply ( DE_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_DE_emissions_tonnes = s_DE_emissions_tonnes [ s_DE_emissions_tonnes > 2000 ] s_DE_carbon_intensity = s_DE_emissions_tonnes / s_DE_demand . loc [ s_DE_emissions_tonnes . index ] DE_2020_emissions_tonnes = s_DE_emissions_tonnes [ '2020' ] . mean () DE_2020_ci_avg = s_DE_carbon_intensity [ '2020' ] . mean () DE_2020_emissions_tonnes , DE_2020_ci_avg (8448.292069623136, 153.80385402105972) We'll do the same for GB # Loading in df_EI = pd . read_csv ( '../data/raw/electric_insights.csv' ) df_EI = df_EI . set_index ( 'local_datetime' ) df_EI . index = pd . to_datetime ( df_EI . index , utc = True ) # Extracting RES, demand, and price series s_GB_RES = df_EI [[ 'wind' , 'solar' ]] . sum ( axis = 1 ) s_GB_demand = df_EI [ 'demand' ] s_GB_price = df_EI [ 'day_ahead_price' ] # Generating carbon intensity series GB_fuel_to_co2_intensity = { 'nuclear' : 0 , 'biomass' : 0.121 , # from EI 'coal' : 0.921 , # DUKES 2018 value 'gas' : 0.377 , # DUKES 2018 value (lower than many CCGT estimates, let alone OCGT) 'hydro' : 0 , 'pumped_storage' : 0 , 'solar' : 0 , 'wind' : 0 , 'belgian' : 0.4 , 'dutch' : 0.474 , # from EI 'french' : 0.053 , # from EI 'ireland' : 0.458 , # from EI 'northern_ireland' : 0.458 # from EI } s_GB_emissions_tonnes = ( df_EI [ GB_fuel_to_co2_intensity . keys ()] . multiply ( 1e3 * 0.5 ) # converting to MWh . multiply ( GB_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_GB_emissions_tonnes = s_GB_emissions_tonnes [ s_GB_emissions_tonnes > 2000 ] s_GB_carbon_intensity = s_GB_emissions_tonnes / s_GB_demand . loc [ s_GB_emissions_tonnes . index ] # Calculating 2020 averages GB_2020_emissions_tonnes = s_GB_emissions_tonnes [ '2020' ] . mean () GB_2020_ci_avg = s_GB_carbon_intensity [ '2020' ] . mean () GB_2020_RES_pct = ( s_GB_RES [ '2020' ] / s_GB_demand [ '2020' ]) . mean () GB_2020_demand_avg = s_GB_demand [ '2020' ] . mean () GB_2020_price_avg = s_GB_price [ '2020' ] . mean () Then combine the results in a single table system_overview_data = { 'Germany' : { 'Average Solar/Wind Generation (%)' : round ( 100 * DE_2020_RES_pct , 2 ), 'Average Demand (GW)' : round ( DE_2020_demand_avg , 2 ), 'Average Price ([EUR,GBP]/MWh)' : round ( DE_2020_price_avg , 2 ), 'Average Carbon Intensity (gCO2/kWh)' : round ( DE_2020_ci_avg , 2 ), }, 'Great Britain' : { 'Average Solar/Wind Generation (%)' : round ( 100 * GB_2020_RES_pct , 2 ), 'Average Demand (GW)' : round ( GB_2020_demand_avg , 2 ), 'Average Price ([EUR,GBP]/MWh)' : round ( GB_2020_price_avg , 2 ), 'Average Carbon Intensity (gCO2/kWh)' : round ( GB_2020_ci_avg , 2 ), } } df_system_overview = pd . DataFrame ( system_overview_data ) . T df_system_overview . head () Unnamed: 0 Average Solar/Wind Generation (%) Average Demand (GW) Average Price ([EUR,GBP]/MWh) Average Carbon Intensity (gCO2/kWh) Germany 35.93 55.96 30.47 153.8 Great Britain 29.83 30.61 33.77 101.17 Which we'll then output as a LaTeX table get_lined_column_format = lambda n_cols : '' . join ( n_cols * [ '|l' ]) + '|' caption = 'Systems overview for 2020' label = 'overview_table' column_format = get_lined_column_format ( df_system_overview . shape [ 1 ] + 1 ) latex_str = df_system_overview . to_latex ( column_format = column_format , caption = caption , label = label ) latex_replacements = { 'CO2' : 'CO \\\\ textsubscript {2} ' , ' \\\\\\\\\\n ' : ' \\\\\\\\ \\\\ midrule \\n ' , 'midrule' : 'hline' , 'toprule' : 'hline' , 'bottomrule' : '' , ' \\n\\\\\\n ' : ' \\n ' , ' \\\\ hline \\n\\\\ hline' : ' \\\\ hline' } for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Systems overview for 2020} \\label{overview_table} \\begin{tabular}{|l|l|l|l|l|} \\hline {} & Average Solar/Wind Generation (\\%) & Average Demand (GW) & Average Price ([EUR,GBP]/MWh) & Average Carbon Intensity (gCO\\textsubscript{2}/kWh) \\\\ \\hline Germany & 35.93 & 55.96 & 30.47 & 153.80 \\\\ \\hline Great Britain & 29.83 & 30.61 & 33.77 & 101.17 \\\\ \\hline \\end{tabular} \\end{table}\\] Carbon Intensity Estimates \u00b6 We'll clean up our GB carbon intensity estimates def clean_idxs ( s ): s . index = s . index . str . replace ( '_' , ' ' ) . str . title () return s df_GB_non0_co2_intensity = ( pd . Series ( GB_fuel_to_co2_intensity ) . replace ( 0 , np . nan ) . dropna () . drop ([ 'belgian' , 'northern_ireland' ]) . pipe ( clean_idxs ) . multiply ( 1e3 ) . astype ( int ) . to_frame () . T . rename ({ 0 : 'gCO2/kWh' }) ) df_GB_non0_co2_intensity Unnamed: 0 Biomass Coal Gas Dutch French Ireland gCO2/kWh 121 921 377 474 53 458 And output them as a LaTeX table caption = 'Carbon intensity factors for fuel-types and interconnection on the GB power system' label = 'GB_co2_intensity_table' column_format = get_lined_column_format ( df_GB_non0_co2_intensity . shape [ 1 ] + 1 ) latex_str = df_GB_non0_co2_intensity . to_latex ( column_format = column_format , caption = caption , label = label ) latex_replacements = { 'CO2' : 'CO \\\\ textsubscript {2} ' , ' \\\\\\\\\\n ' : ' \\\\\\\\ \\\\ midrule \\n ' , 'midrule' : 'hline' , 'toprule' : 'hline' , 'bottomrule' : '' , ' \\n\\\\\\n ' : ' \\n ' , ' \\\\ hline \\n\\\\ hline' : ' \\\\ hline' } for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Carbon intensity factors for fuel-types and interconnection on the GB power system} \\label{GB_co2_intensity_table} \\begin{tabular}{|l|l|l|l|l|l|l|} \\hline {} & Biomass & Coal & Gas & Dutch & French & Ireland \\\\ \\hline gCO\\textsubscript{2}/kWh & 121 & 921 & 377 & 474 & 53 & 458 \\\\ \\hline \\end{tabular} \\end{table}\\] We'll then do the same for DE df_DE_non0_co2_intensity = ( pd . Series ( DE_fuel_to_co2_intensity ) . replace ( 0 , np . nan ) . dropna () [[ 'Biomass' , 'Brown Coal' , 'Hard Coal' , 'Gas' , 'Oil' ]] . pipe ( clean_idxs ) . multiply ( 1e3 ) . astype ( int ) . to_frame () . T . rename ({ 0 : 'gCO2/kWh' }) ) df_DE_non0_co2_intensity Unnamed: 0 Biomass Brown Coal Hard Coal Gas Oil gCO2/kWh 390 360 340 230 280 caption = 'Carbon intensity factors for fuel-types and interconnection on the DE power system' label = 'DE_co2_intensity_table' column_format = get_lined_column_format ( df_DE_non0_co2_intensity . shape [ 1 ] + 1 ) latex_str = df_DE_non0_co2_intensity . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Carbon intensity factors for fuel-types and interconnection on the DE power system} \\label{DE_co2_intensity_table} \\begin{tabular}{|l|l|l|l|l|l|} \\hline {} & Biomass & Brown Coal & Hard Coal & Gas & Oil \\\\ \\hline gCO\\textsubscript{2}/kWh & 390 & 360 & 340 & 230 & 280 \\\\ \\hline \\end{tabular} \\end{table}\\] Electricity Price Forecasting Metrics \u00b6 We'll start by loading in our previously saved model metrics with open ( '../data/results/price_model_accuracy_metrics.json' , 'r' ) as fp : model_accuracy_metrics = json . load ( fp ) JSON ( model_accuracy_metrics ) <IPython.core.display.JSON object> We'll parse the MAE results into a new table model_accuracy_data = { 'Germany' : { 'Dispatchable Load' : round ( model_accuracy_metrics [ 'DE_dispatch' ][ 'mean_abs_err' ], 2 ), 'Total Load' : round ( model_accuracy_metrics [ 'DE_demand' ][ 'mean_abs_err' ], 2 ), }, 'Great Britain' : { 'Dispatchable Load' : round ( model_accuracy_metrics [ 'GB_dispatch' ][ 'mean_abs_err' ], 2 ), 'Total Load' : round ( model_accuracy_metrics [ 'GB_demand' ][ 'mean_abs_err' ], 2 ), } } df_model_accuracy = pd . DataFrame ( model_accuracy_data ) . T df_model_accuracy . head () Unnamed: 0 Dispatchable Load Total Load Germany 5.85 18.28 Great Britain 6.56 8.42 Which we'll output as a LaTeX table caption = 'Price forecasting model accuracy when regressing against dispatchable and total load for GB and DE.' label = 'model_accuracy_table' column_format = get_lined_column_format ( df_model_accuracy . shape [ 1 ] + 1 ) latex_str = df_model_accuracy . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Price forecasting model accuracy when regressing against dispatchable and total load for GB and DE.} \\label{model_accuracy_table} \\begin{tabular}{|l|l|l|} \\hline {} & Dispatchable Load & Total Load \\\\ \\hline Germany & 5.85 & 18.28 \\\\ \\hline Great Britain & 6.56 & 8.42 \\\\ \\hline \\end{tabular} \\end{table}\\] Price and CO2 MOE Results \u00b6 We'll first load in all of the price and carbon MOE time-series def set_dt_idx ( df , dt_idx_col = 'local_datetime' ): df = df . set_index ( dt_idx_col ) df . index = pd . to_datetime ( df . index , utc = True ) return df df_GB_price_results_ts = pd . read_csv ( '../data/results/GB_price.csv' ) . pipe ( set_dt_idx ) df_DE_price_results_ts = pd . read_csv ( '../data/results/DE_price.csv' ) . pipe ( set_dt_idx ) df_GB_carbon_results_ts = pd . read_csv ( '../data/results/GB_carbon.csv' ) . pipe ( set_dt_idx ) df_DE_carbon_results_ts = pd . read_csv ( '../data/results/DE_carbon.csv' ) . pipe ( set_dt_idx ) df_GB_price_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 We'll then calculate their summary statistics MOE_results_data = { 'Germany' : { 'Price ([EUR,GBP]/MWh)' : round ( df_DE_price_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Price (%)' : round ( 100 * ( df_DE_price_results_ts . loc [ '2020' , 'moe' ] * df_DE [ 'demand' ]) . sum () / (( df_DE_price_results_ts . loc [ '2020' , 'observed' ] + df_DE_price_results_ts . loc [ '2020' , 'moe' ]) * df_DE [ 'demand' ]) . sum (), 2 ), 'Carbon (Tonnes/h)' : round ( df_DE_carbon_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Carbon (%)' : round ( 100 * ( df_DE_carbon_results_ts . loc [ '2020' , 'moe' ] . sum () / ( df_DE_carbon_results_ts . loc [ '2020' , 'observed' ] + df_DE_carbon_results_ts . loc [ '2020' , 'moe' ]) . sum ()) . mean (), 2 ) }, 'Great Britain' : { 'Price ([EUR,GBP]/MWh)' : round ( df_GB_price_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Price (%)' : round ( 100 * ( df_GB_price_results_ts . loc [ '2020' , 'moe' ] * df_EI [ 'demand' ]) . sum () / (( df_GB_price_results_ts . loc [ '2020' , 'observed' ] + df_GB_price_results_ts . loc [ '2020' , 'moe' ]) * df_EI [ 'demand' ]) . sum (), 2 ), 'Carbon (Tonnes/h)' : round ( df_GB_carbon_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), # doubled to make it the same hourly rate as DE 'Carbon (%)' : round ( 100 * ( df_GB_carbon_results_ts . loc [ '2020' , 'moe' ] . sum () / ( df_GB_carbon_results_ts . loc [ '2020' , 'observed' ] + df_GB_carbon_results_ts . loc [ '2020' , 'moe' ]) . sum ()) . mean (), 2 ) } } df_MOE_results = ( pd . DataFrame ( MOE_results_data ) ) df_MOE_results . head () Unnamed: 0 Germany Great Britain Price ([EUR,GBP]/MWh) 22.17 13.89 Price (%) 43.43 29.66 Carbon (Tonnes/h) 5563.22 1657.88 Carbon (%) 39.7 37.89 And export the output as a LaTeX table caption = '2020 Merit Order Effect results overview (weighted by volume).' label = 'moe_results_table' column_format = get_lined_column_format ( df_MOE_results . shape [ 1 ] + 1 ) latex_str = df_MOE_results . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{2020 Merit Order Effect results overview (weighted by volume).} \\label{moe_results_table} \\begin{tabular}{|l|l|l|} \\hline {} & Germany & Great Britain \\\\ \\hline Price ([EUR,GBP]/MWh) & 22.17 & 13.89 \\\\ \\hline Price (\\%) & 43.43 & 29.66 \\\\ \\hline Carbon (Tonnes/h) & 5563.22 & 1657.88 \\\\ \\hline Carbon (\\%) & 39.70 & 37.89 \\\\ \\hline \\end{tabular} \\end{table}\\] Literature Review \u00b6 Lastly we'll create our largest table, containing results from across the literature lit_results_data = [ { 'Study' : 'Sensfuss et al. (2008)' , 'MOE' : '7.83 \u20ac/MWh' , 'Period' : '2006' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Weigt (2009)' , 'MOE' : '10 \u20ac/MWh' , 'Period' : '2006-2008' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Keles et al. (2013)' , 'MOE' : '5.90 \u20ac/MWh' , 'Period' : '2006\u20132009' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Mulder and Scholtens (2013)' , 'MOE' : '0.03% price decrease per p.p increase in wind speeds' , 'Period' : '2006\u20132011' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Tveten et al. (2013)' , 'MOE' : '5.25 \u20ac/MWh (solar)' , 'Period' : '2006-2011' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Wurzburg et al. (2013)' , 'MOE' : '2% price decrease' , 'Period' : '2010-2012' , 'Region' : 'Germany & Austria' , 'Method' : 'RPR' , }, { 'Study' : 'Cludius et al. (2014)' , 'MOE' : '8 \u20ac/MWh' , 'Period' : '2010-2012' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Ketterer (2014)' , 'MOE' : '0.1-1.46% price decrease per p.p increase in wind generation' , 'Period' : '2006-2012' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Ederer (2015)' , 'MOE' : '1.3% price decrease per annual TWh of wind' , 'Period' : '2006-2014' , 'Region' : 'Germany' , 'Method' : 'MSS' , }, { 'Study' : 'Kyritsis et al. (2017)' , 'MOE' : '-' , 'Period' : '2010-2015' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Bublitz et al. (2017)' , 'MOE' : '5.40 \u20ac/MWh' , 'Period' : '2011-2015' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Bublitz et al. (2017)' , 'MOE' : '6.80 \u20ac/MWh' , 'Period' : '2011-2015' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'de Miera et al. (2008)' , 'MOE' : '8.6-25.1% price decrease' , 'Period' : '2005-2007' , 'Region' : 'Spain' , 'Method' : 'ESS' , }, { 'Study' : 'Gelabert et al. (2011)' , 'MOE' : '3.7% price decrease' , 'Period' : '2005-2012' , 'Region' : 'Spain' , 'Method' : 'RPR' , }, { 'Study' : 'Ciarreta et al. (2014)' , 'MOE' : '25-45 \u20ac/MWh' , 'Period' : '2008\u20132012' , 'Region' : 'Spain' , 'Method' : 'ESS' , }, { 'Study' : 'Clo et al. (2015)' , 'MOE' : '2.3 \u20ac/MWh (solar), 4.2 \u20ac/MWh (wind)' , 'Period' : '2005\u20132013' , 'Region' : 'Italy' , 'Method' : 'RPR' , }, { 'Study' : 'Munksgaard and Morthorst (2008)' , 'MOE' : '1-4 \u20ac/MWh' , 'Period' : '2004-2006' , 'Region' : 'Denmark' , 'Method' : 'RPR' , }, { 'Study' : 'Jonsson et al. (2010)' , 'MOE' : '-' , 'Period' : '2006-2007' , 'Region' : 'Denmark' , 'Method' : 'RPR' , }, { 'Study' : 'Denny et al. (2017)' , 'MOE' : '3.40 \u20ac/MWh per GWh (wind)' , 'Period' : '2009' , 'Region' : 'Ireland' , 'Method' : 'RPR' , }, { 'Study' : 'Lunackova et al. (2017)' , 'MOE' : '1.2% price decrease per 10 % i ncrease in RES' , 'Period' : '2010-2015' , 'Region' : 'Czech Republic' , 'Method' : 'RPR' , }, { 'Study' : 'Dillig et al. (2016)' , 'MOE' : '50.29 \u20ac/MWh' , 'Period' : '2011-2013' , 'Region' : 'Germany' , 'Method' : 'MSS' , }, { 'Study' : 'McConnell et al. (2013)' , 'MOE' : '8.6% price decrease' , 'Period' : '2009-2010' , 'Region' : 'Australia' , 'Method' : 'MSS' , }, { 'Study' : 'Moreno et al. (2012)' , 'MOE' : '0.018% price increase per p.p. increase in RES penetration' , 'Period' : '1998\u20132009' , 'Region' : 'EU-27' , 'Method' : 'RPR' , }, { 'Study' : 'Woo et al. (2011)' , 'MOE' : '0.32-1.53 $/MWh' , 'Period' : '2007-2010' , 'Region' : 'Texas' , 'Method' : 'RPR' , }, { 'Study' : 'Kaufmann and Vaid (2016)' , 'MOE' : '0.26-1.86 $/MWh (solar)' , 'Period' : '2010-2012' , 'Region' : 'Massachusetts' , 'Method' : 'RPR' , }, { 'Study' : 'Woo et al. (2016)' , 'MOE' : '5.3 \\$/MWh (solar) and 3.3 \\$/MWh (wind) per GWh of RES' , 'Period' : '2012-2015' , 'Region' : 'California' , 'Method' : 'RPR' , }, { 'Study' : 'Paraschiv et al. (2014)' , 'MOE' : '0.15% price decrease per MWh of RES' , 'Period' : '2010-2013' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'O \\' Mahoney and Denny (2011)' , 'MOE' : '12% price decrease' , 'Period' : '2009' , 'Region' : 'Ireland' , 'Method' : 'RPR' , }, { 'Study' : 'Hildmann et al. (2015)' , 'MOE' : '13.4-18.6 \u20ac/MWh' , 'Period' : '2011-2013' , 'Region' : 'Germany and Austria' , 'Method' : 'MSS' , }, { 'Study' : 'Gil et al. (2012)' , 'MOE' : '9.72 \u20ac/MWh' , 'Period' : '2007-2010' , 'Region' : 'Spain' , 'Method' : 'RPR' , }, # { # Removed due to language barrier preventing method from being discerned # 'Study': 'Weber and Woll (2007)', # 'MOE': '4 \u20ac/MWh', # 'Period': '2006', # 'Region': 'Germany', # 'Method': '-', # }, { 'Study' : 'Halttunen et al. (2021)' , 'MOE' : '0.631 \u20ac/MWh per p.p. increase in RES penetration' , 'Period' : '2012-2019' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Halttunen et al. (2021)' , 'MOE' : '0.482 \u20ac/MWh per p.p. increase in RES penetration' , 'Period' : '2010-2019' , 'Region' : 'Germany' , 'Method' : 'RPR' , } ] df_lit_results = pd . DataFrame ( lit_results_data ) df_lit_results [ 'Study Year' ] = df_lit_results [ 'Study' ] . str . split ( '(' ) . str [ 1 ] . str . replace ( ')' , '' ) . astype ( int ) df_lit_results = df_lit_results . sort_values ([ 'Method' , 'Study Year' , 'Study' ]) . drop ( columns = [ 'Study Year' ]) . reset_index ( drop = True ) df_lit_results . head () <ipython-input-20-15bc63a4c27e>:237: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. df_lit_results['Study Year'] = df_lit_results['Study'].str.split('(').str[1].str.replace(')', '').astype(int) Unnamed: 0 Study MOE Period Region Method 0 Sensfuss et al. (2008) 7.83 \u20ac/MWh 2006 Germany ESS 1 de Miera et al. (2008) 8.6-25.1% price decrease 2005-2007 Spain ESS 2 Weigt (2009) 10 \u20ac/MWh 2006-2008 Germany ESS 3 Ciarreta et al. (2014) 25-45 \u20ac/MWh 2008\u20132012 Spain ESS 4 Bublitz et al. (2017) 5.40 \u20ac/MWh 2011-2015 Germany ESS We'll also export this as a LaTeX table caption = 'Results overview from the MOE literature' label = 'lit_results_table' column_format = get_lined_column_format ( df_lit_results . shape [ 1 ] + 1 ) latex_str = df_lit_results . to_latex ( column_format = column_format , caption = caption , label = label , index = False ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Results overview from the MOE literature} \\label{lit_results_table} \\begin{tabular}{|l|l|l|l|l|l|} \\hline Study & MOE & Period & Region & Method \\\\ \\hline Sensfuss et al. (2008) & 7.83 \u20ac/MWh & 2006 & Germany & ESS \\\\ \\hline de Miera et al. (2008) & 8.6-25.1\\% price decrease & 2005-2007 & Spain & ESS \\\\ \\hline Weigt (2009) & 10 \u20ac/MWh & 2006-2008 & Germany & ESS \\\\ \\hline Ciarreta et al. (2014) & 25-45 \u20ac/MWh & 2008\u20132012 & Spain & ESS \\\\ \\hline Bublitz et al. (2017) & 5.40 \u20ac/MWh & 2011-2015 & Germany & ESS \\\\ \\hline McConnell et al. (2013) & 8.6\\% price decrease & 2009-2010 & Australia & MSS \\\\ \\hline Ederer (2015) & 1.3\\% price decrease per annual TWh of wind & 2006-2014 & Germany & MSS \\\\ \\hline Hildmann et al. (2015) & 13.4-18.6 \u20ac/MWh & 2011-2013 & Germany and Austria & MSS \\\\ \\hline Dillig et al. (2016) & 50.29 \u20ac/MWh & 2011-2013 & Germany & MSS \\\\ \\hline Munksgaard and Morthorst (2008) & 1-4 \u20ac/MWh & 2004-2006 & Denmark & RPR \\\\ \\hline Jonsson et al. (2010) & - & 2006-2007 & Denmark & RPR \\\\ \\hline Gelabert et al. (2011) & 3.7\\% price decrease & 2005-2012 & Spain & RPR \\\\ \\hline O'Mahoney and Denny (2011) & 12\\% price decrease & 2009 & Ireland & RPR \\\\ \\hline Woo et al. (2011) & 0.32-1.53 \\$/MWh & 2007-2010 & Texas & RPR \\\\ \\hline Gil et al. (2012) & 9.72 \u20ac/MWh & 2007-2010 & Spain & RPR \\\\ \\hline Moreno et al. (2012) & 0.018\\% price increase per p.p. increase in RES ... & 1998\u20132009 & EU-27 & RPR \\\\ \\hline Keles et al. (2013) & 5.90 \u20ac/MWh & 2006\u20132009 & Germany & RPR \\\\ \\hline Mulder and Scholtens (2013) & 0.03\\% price decrease per p.p increase in wind s... & 2006\u20132011 & Germany & RPR \\\\ \\hline Tveten et al. (2013) & 5.25 \u20ac/MWh (solar) & 2006-2011 & Germany & RPR \\\\ \\hline Wurzburg et al. (2013) & 2\\% price decrease & 2010-2012 & Germany \\& Austria & RPR \\\\ \\hline Cludius et al. (2014) & 8 \u20ac/MWh & 2010-2012 & Germany & RPR \\\\ \\hline Ketterer (2014) & 0.1-1.46\\% price decrease per p.p increase in wi... & 2006-2012 & Germany & RPR \\\\ \\hline Paraschiv et al. (2014) & 0.15\\% price decrease per MWh of RES & 2010-2013 & Germany & RPR \\\\ \\hline Clo et al. (2015) & 2.3 \u20ac/MWh (solar), 4.2 \u20ac/MWh (wind) & 2005\u20132013 & Italy & RPR \\\\ \\hline Kaufmann and Vaid (2016) & 0.26-1.86 \\$/MWh (solar) & 2010-2012 & Massachusetts & RPR \\\\ \\hline Woo et al. (2016) & 5.3 \\textbackslash \\$/MWh (solar) and 3.3 \\textbackslash \\$/MWh (wind) per GW... & 2012-2015 & California & RPR \\\\ \\hline Bublitz et al. (2017) & 6.80 \u20ac/MWh & 2011-2015 & Germany & RPR \\\\ \\hline Denny et al. (2017) & 3.40 \u20ac/MWh per GWh (wind) & 2009 & Ireland & RPR \\\\ \\hline Kyritsis et al. (2017) & - & 2010-2015 & Germany & RPR \\\\ \\hline Lunackova et al. (2017) & 1.2\\% price decrease per 10\\% increase in RES & 2010-2015 & Czech Republic & RPR \\\\ \\hline Halttunen et al. (2021) & 0.631 \u20ac/MWh per p.p. increase in RES penetration & 2012-2019 & Germany & RPR \\\\ \\hline Halttunen et al. (2021) & 0.482 \u20ac/MWh per p.p. increase in RES penetration & 2010-2019 & Germany & RPR \\\\ \\hline \\end{tabular} \\end{table}\\] Figures \u00b6 Time Dimension Hyper-Parameters \u00b6 We'll create a plot showing an example of how regression dates are converted into weightings for the time-series x = np . linspace ( 0 , 1 , 150 ) centers = [ 0.3 , 0.5 , 0.7 ] # Plotting fig , ax = plt . subplots ( dpi = 250 , figsize = ( 8 , 4 )) for center in centers : dist = lowess . get_dist ( x , center ) dist_threshold = lowess . get_dist_threshold ( dist , frac = 0.3 ) weights = lowess . dist_to_weights ( dist , dist_threshold ) ax . plot ( x , weights , color = 'k' ) x_pos = 0.4 ax . annotate ( 'Interval' , xy = ( x_pos , 0.95 ), xytext = ( x_pos , 1.00 ), xycoords = 'axes fraction' , fontsize = 6.5 , ha = 'center' , va = 'bottom' , bbox = dict ( boxstyle = 'square' , fc = 'white' ), arrowprops = dict ( arrowstyle = '-[, widthB=7.0, lengthB=1.5' , lw = 1.0 )) x_pos = 0.5 ax . annotate ( 'Bandwidth' , xy = ( x_pos , 0.06 ), xytext = ( x_pos , 0.11 ), xycoords = 'axes fraction' , fontsize = 9.5 , ha = 'center' , va = 'bottom' , bbox = dict ( boxstyle = 'square' , fc = 'white' ), arrowprops = dict ( arrowstyle = '-[, widthB=7.0, lengthB=1.5' , lw = 1.0 )) ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1.1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Data Fraction' ) ax . set_ylabel ( 'Relative Weighting' ) Text(0, 0.5, 'Relative Weighting')","title":"Tables & Figures"},{"location":"dev-09-tables-and-figures/#tables-figures-generation","text":"This notebook provides a programmatic workflow for generating the tables used in the MOE paper, as well as the diagram to show the time-adaptive smoothing weights.","title":"Tables &amp; Figures Generation"},{"location":"dev-09-tables-and-figures/#imports","text":"import json import numpy as np import pandas as pd import matplotlib.pyplot as plt from IPython.display import Latex , JSON from moepy import eda , lowess","title":"Imports"},{"location":"dev-09-tables-and-figures/#tables","text":"","title":"Tables"},{"location":"dev-09-tables-and-figures/#power-systems-overview","text":"We'll first load in the DE data df_DE = eda . load_DE_df ( '../data/raw/energy_charts.csv' , '../data/raw/ENTSOE_DE_price.csv' ) df_DE . head () local_datetime Biomass Brown Coal Gas Hard Coal Hydro Power Oil Others Pumped Storage Seasonal Storage Solar Uranium Wind net_balance demand price 2010-01-03 23:00:00+00:00 3.637 16.533 4.726 10.078 2.331 0 0 0.052 0.068 0 16.826 0.635 -1.229 53.657 nan 2010-01-04 00:00:00+00:00 3.637 16.544 4.856 8.816 2.293 0 0 0.038 0.003 0 16.841 0.528 -1.593 51.963 nan 2010-01-04 01:00:00+00:00 3.637 16.368 5.275 7.954 2.299 0 0 0.032 0 0 16.846 0.616 -1.378 51.649 nan 2010-01-04 02:00:00+00:00 3.637 15.837 5.354 7.681 2.299 0 0 0.027 0 0 16.699 0.63 -1.624 50.54 nan 2010-01-04 03:00:00+00:00 3.637 15.452 5.918 7.498 2.301 0.003 0 0.02 0 0 16.635 0.713 -0.731 51.446 nan Clean it up then calculate the relevant summary statistics s_DE_RES_output = df_DE [[ 'Wind' , 'Solar' ]] . sum ( axis = 1 ) s_DE_demand = df_DE [ 'demand' ] s_DE_price = df_DE [ 'price' ] s_DE_RES_pct = s_DE_RES_output / s_DE_demand DE_2020_RES_pct = s_DE_RES_pct [ '2020' ] . mean () DE_2020_demand_avg = s_DE_demand [ '2020' ] . mean () DE_2020_price_avg = s_DE_price [ '2020' ] . mean () DE_2020_RES_pct , DE_2020_demand_avg , DE_2020_price_avg (0.3593124152992342, 55.956133452868855, 30.469415917112606) We'll also estimate the carbon intensity DE_fuel_to_co2_intensity = { 'Biomass' : 0.39 , 'Brown Coal' : 0.36 , 'Gas' : 0.23 , 'Hard Coal' : 0.34 , 'Hydro Power' : 0 , 'Oil' : 0.28 , 'Others' : 0 , 'Pumped Storage' : 0 , 'Seasonal Storage' : 0 , 'Solar' : 0 , 'Uranium' : 0 , 'Wind' : 0 , 'net_balance' : 0 } s_DE_emissions_tonnes = ( df_DE [ DE_fuel_to_co2_intensity . keys ()] . multiply ( 1e3 ) # converting to MWh . multiply ( DE_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_DE_emissions_tonnes = s_DE_emissions_tonnes [ s_DE_emissions_tonnes > 2000 ] s_DE_carbon_intensity = s_DE_emissions_tonnes / s_DE_demand . loc [ s_DE_emissions_tonnes . index ] DE_2020_emissions_tonnes = s_DE_emissions_tonnes [ '2020' ] . mean () DE_2020_ci_avg = s_DE_carbon_intensity [ '2020' ] . mean () DE_2020_emissions_tonnes , DE_2020_ci_avg (8448.292069623136, 153.80385402105972) We'll do the same for GB # Loading in df_EI = pd . read_csv ( '../data/raw/electric_insights.csv' ) df_EI = df_EI . set_index ( 'local_datetime' ) df_EI . index = pd . to_datetime ( df_EI . index , utc = True ) # Extracting RES, demand, and price series s_GB_RES = df_EI [[ 'wind' , 'solar' ]] . sum ( axis = 1 ) s_GB_demand = df_EI [ 'demand' ] s_GB_price = df_EI [ 'day_ahead_price' ] # Generating carbon intensity series GB_fuel_to_co2_intensity = { 'nuclear' : 0 , 'biomass' : 0.121 , # from EI 'coal' : 0.921 , # DUKES 2018 value 'gas' : 0.377 , # DUKES 2018 value (lower than many CCGT estimates, let alone OCGT) 'hydro' : 0 , 'pumped_storage' : 0 , 'solar' : 0 , 'wind' : 0 , 'belgian' : 0.4 , 'dutch' : 0.474 , # from EI 'french' : 0.053 , # from EI 'ireland' : 0.458 , # from EI 'northern_ireland' : 0.458 # from EI } s_GB_emissions_tonnes = ( df_EI [ GB_fuel_to_co2_intensity . keys ()] . multiply ( 1e3 * 0.5 ) # converting to MWh . multiply ( GB_fuel_to_co2_intensity . values ()) . sum ( axis = 1 ) ) s_GB_emissions_tonnes = s_GB_emissions_tonnes [ s_GB_emissions_tonnes > 2000 ] s_GB_carbon_intensity = s_GB_emissions_tonnes / s_GB_demand . loc [ s_GB_emissions_tonnes . index ] # Calculating 2020 averages GB_2020_emissions_tonnes = s_GB_emissions_tonnes [ '2020' ] . mean () GB_2020_ci_avg = s_GB_carbon_intensity [ '2020' ] . mean () GB_2020_RES_pct = ( s_GB_RES [ '2020' ] / s_GB_demand [ '2020' ]) . mean () GB_2020_demand_avg = s_GB_demand [ '2020' ] . mean () GB_2020_price_avg = s_GB_price [ '2020' ] . mean () Then combine the results in a single table system_overview_data = { 'Germany' : { 'Average Solar/Wind Generation (%)' : round ( 100 * DE_2020_RES_pct , 2 ), 'Average Demand (GW)' : round ( DE_2020_demand_avg , 2 ), 'Average Price ([EUR,GBP]/MWh)' : round ( DE_2020_price_avg , 2 ), 'Average Carbon Intensity (gCO2/kWh)' : round ( DE_2020_ci_avg , 2 ), }, 'Great Britain' : { 'Average Solar/Wind Generation (%)' : round ( 100 * GB_2020_RES_pct , 2 ), 'Average Demand (GW)' : round ( GB_2020_demand_avg , 2 ), 'Average Price ([EUR,GBP]/MWh)' : round ( GB_2020_price_avg , 2 ), 'Average Carbon Intensity (gCO2/kWh)' : round ( GB_2020_ci_avg , 2 ), } } df_system_overview = pd . DataFrame ( system_overview_data ) . T df_system_overview . head () Unnamed: 0 Average Solar/Wind Generation (%) Average Demand (GW) Average Price ([EUR,GBP]/MWh) Average Carbon Intensity (gCO2/kWh) Germany 35.93 55.96 30.47 153.8 Great Britain 29.83 30.61 33.77 101.17 Which we'll then output as a LaTeX table get_lined_column_format = lambda n_cols : '' . join ( n_cols * [ '|l' ]) + '|' caption = 'Systems overview for 2020' label = 'overview_table' column_format = get_lined_column_format ( df_system_overview . shape [ 1 ] + 1 ) latex_str = df_system_overview . to_latex ( column_format = column_format , caption = caption , label = label ) latex_replacements = { 'CO2' : 'CO \\\\ textsubscript {2} ' , ' \\\\\\\\\\n ' : ' \\\\\\\\ \\\\ midrule \\n ' , 'midrule' : 'hline' , 'toprule' : 'hline' , 'bottomrule' : '' , ' \\n\\\\\\n ' : ' \\n ' , ' \\\\ hline \\n\\\\ hline' : ' \\\\ hline' } for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Systems overview for 2020} \\label{overview_table} \\begin{tabular}{|l|l|l|l|l|} \\hline {} & Average Solar/Wind Generation (\\%) & Average Demand (GW) & Average Price ([EUR,GBP]/MWh) & Average Carbon Intensity (gCO\\textsubscript{2}/kWh) \\\\ \\hline Germany & 35.93 & 55.96 & 30.47 & 153.80 \\\\ \\hline Great Britain & 29.83 & 30.61 & 33.77 & 101.17 \\\\ \\hline \\end{tabular} \\end{table}\\]","title":"Power Systems Overview"},{"location":"dev-09-tables-and-figures/#carbon-intensity-estimates","text":"We'll clean up our GB carbon intensity estimates def clean_idxs ( s ): s . index = s . index . str . replace ( '_' , ' ' ) . str . title () return s df_GB_non0_co2_intensity = ( pd . Series ( GB_fuel_to_co2_intensity ) . replace ( 0 , np . nan ) . dropna () . drop ([ 'belgian' , 'northern_ireland' ]) . pipe ( clean_idxs ) . multiply ( 1e3 ) . astype ( int ) . to_frame () . T . rename ({ 0 : 'gCO2/kWh' }) ) df_GB_non0_co2_intensity Unnamed: 0 Biomass Coal Gas Dutch French Ireland gCO2/kWh 121 921 377 474 53 458 And output them as a LaTeX table caption = 'Carbon intensity factors for fuel-types and interconnection on the GB power system' label = 'GB_co2_intensity_table' column_format = get_lined_column_format ( df_GB_non0_co2_intensity . shape [ 1 ] + 1 ) latex_str = df_GB_non0_co2_intensity . to_latex ( column_format = column_format , caption = caption , label = label ) latex_replacements = { 'CO2' : 'CO \\\\ textsubscript {2} ' , ' \\\\\\\\\\n ' : ' \\\\\\\\ \\\\ midrule \\n ' , 'midrule' : 'hline' , 'toprule' : 'hline' , 'bottomrule' : '' , ' \\n\\\\\\n ' : ' \\n ' , ' \\\\ hline \\n\\\\ hline' : ' \\\\ hline' } for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Carbon intensity factors for fuel-types and interconnection on the GB power system} \\label{GB_co2_intensity_table} \\begin{tabular}{|l|l|l|l|l|l|l|} \\hline {} & Biomass & Coal & Gas & Dutch & French & Ireland \\\\ \\hline gCO\\textsubscript{2}/kWh & 121 & 921 & 377 & 474 & 53 & 458 \\\\ \\hline \\end{tabular} \\end{table}\\] We'll then do the same for DE df_DE_non0_co2_intensity = ( pd . Series ( DE_fuel_to_co2_intensity ) . replace ( 0 , np . nan ) . dropna () [[ 'Biomass' , 'Brown Coal' , 'Hard Coal' , 'Gas' , 'Oil' ]] . pipe ( clean_idxs ) . multiply ( 1e3 ) . astype ( int ) . to_frame () . T . rename ({ 0 : 'gCO2/kWh' }) ) df_DE_non0_co2_intensity Unnamed: 0 Biomass Brown Coal Hard Coal Gas Oil gCO2/kWh 390 360 340 230 280 caption = 'Carbon intensity factors for fuel-types and interconnection on the DE power system' label = 'DE_co2_intensity_table' column_format = get_lined_column_format ( df_DE_non0_co2_intensity . shape [ 1 ] + 1 ) latex_str = df_DE_non0_co2_intensity . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Carbon intensity factors for fuel-types and interconnection on the DE power system} \\label{DE_co2_intensity_table} \\begin{tabular}{|l|l|l|l|l|l|} \\hline {} & Biomass & Brown Coal & Hard Coal & Gas & Oil \\\\ \\hline gCO\\textsubscript{2}/kWh & 390 & 360 & 340 & 230 & 280 \\\\ \\hline \\end{tabular} \\end{table}\\]","title":"Carbon Intensity Estimates"},{"location":"dev-09-tables-and-figures/#electricity-price-forecasting-metrics","text":"We'll start by loading in our previously saved model metrics with open ( '../data/results/price_model_accuracy_metrics.json' , 'r' ) as fp : model_accuracy_metrics = json . load ( fp ) JSON ( model_accuracy_metrics ) <IPython.core.display.JSON object> We'll parse the MAE results into a new table model_accuracy_data = { 'Germany' : { 'Dispatchable Load' : round ( model_accuracy_metrics [ 'DE_dispatch' ][ 'mean_abs_err' ], 2 ), 'Total Load' : round ( model_accuracy_metrics [ 'DE_demand' ][ 'mean_abs_err' ], 2 ), }, 'Great Britain' : { 'Dispatchable Load' : round ( model_accuracy_metrics [ 'GB_dispatch' ][ 'mean_abs_err' ], 2 ), 'Total Load' : round ( model_accuracy_metrics [ 'GB_demand' ][ 'mean_abs_err' ], 2 ), } } df_model_accuracy = pd . DataFrame ( model_accuracy_data ) . T df_model_accuracy . head () Unnamed: 0 Dispatchable Load Total Load Germany 5.85 18.28 Great Britain 6.56 8.42 Which we'll output as a LaTeX table caption = 'Price forecasting model accuracy when regressing against dispatchable and total load for GB and DE.' label = 'model_accuracy_table' column_format = get_lined_column_format ( df_model_accuracy . shape [ 1 ] + 1 ) latex_str = df_model_accuracy . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Price forecasting model accuracy when regressing against dispatchable and total load for GB and DE.} \\label{model_accuracy_table} \\begin{tabular}{|l|l|l|} \\hline {} & Dispatchable Load & Total Load \\\\ \\hline Germany & 5.85 & 18.28 \\\\ \\hline Great Britain & 6.56 & 8.42 \\\\ \\hline \\end{tabular} \\end{table}\\]","title":"Electricity Price Forecasting Metrics"},{"location":"dev-09-tables-and-figures/#price-and-co2-moe-results","text":"We'll first load in all of the price and carbon MOE time-series def set_dt_idx ( df , dt_idx_col = 'local_datetime' ): df = df . set_index ( dt_idx_col ) df . index = pd . to_datetime ( df . index , utc = True ) return df df_GB_price_results_ts = pd . read_csv ( '../data/results/GB_price.csv' ) . pipe ( set_dt_idx ) df_DE_price_results_ts = pd . read_csv ( '../data/results/DE_price.csv' ) . pipe ( set_dt_idx ) df_GB_carbon_results_ts = pd . read_csv ( '../data/results/GB_carbon.csv' ) . pipe ( set_dt_idx ) df_DE_carbon_results_ts = pd . read_csv ( '../data/results/DE_carbon.csv' ) . pipe ( set_dt_idx ) df_GB_price_results_ts . head () local_datetime prediction counterfactual observed moe 2009-01-01 00:00:00+00:00 37.2034 37.3134 58.05 0.109938 2009-01-01 00:30:00+00:00 37.3134 37.5351 56.33 0.221756 2009-01-01 01:00:00+00:00 36.7685 36.9851 52.98 0.216574 2009-01-01 01:30:00+00:00 35.5952 35.8076 50.39 0.212469 2009-01-01 02:00:00+00:00 34.8494 35.0631 48.7 0.213697 We'll then calculate their summary statistics MOE_results_data = { 'Germany' : { 'Price ([EUR,GBP]/MWh)' : round ( df_DE_price_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Price (%)' : round ( 100 * ( df_DE_price_results_ts . loc [ '2020' , 'moe' ] * df_DE [ 'demand' ]) . sum () / (( df_DE_price_results_ts . loc [ '2020' , 'observed' ] + df_DE_price_results_ts . loc [ '2020' , 'moe' ]) * df_DE [ 'demand' ]) . sum (), 2 ), 'Carbon (Tonnes/h)' : round ( df_DE_carbon_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Carbon (%)' : round ( 100 * ( df_DE_carbon_results_ts . loc [ '2020' , 'moe' ] . sum () / ( df_DE_carbon_results_ts . loc [ '2020' , 'observed' ] + df_DE_carbon_results_ts . loc [ '2020' , 'moe' ]) . sum ()) . mean (), 2 ) }, 'Great Britain' : { 'Price ([EUR,GBP]/MWh)' : round ( df_GB_price_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), 'Price (%)' : round ( 100 * ( df_GB_price_results_ts . loc [ '2020' , 'moe' ] * df_EI [ 'demand' ]) . sum () / (( df_GB_price_results_ts . loc [ '2020' , 'observed' ] + df_GB_price_results_ts . loc [ '2020' , 'moe' ]) * df_EI [ 'demand' ]) . sum (), 2 ), 'Carbon (Tonnes/h)' : round ( df_GB_carbon_results_ts . loc [ '2020' , 'moe' ] . mean (), 2 ), # doubled to make it the same hourly rate as DE 'Carbon (%)' : round ( 100 * ( df_GB_carbon_results_ts . loc [ '2020' , 'moe' ] . sum () / ( df_GB_carbon_results_ts . loc [ '2020' , 'observed' ] + df_GB_carbon_results_ts . loc [ '2020' , 'moe' ]) . sum ()) . mean (), 2 ) } } df_MOE_results = ( pd . DataFrame ( MOE_results_data ) ) df_MOE_results . head () Unnamed: 0 Germany Great Britain Price ([EUR,GBP]/MWh) 22.17 13.89 Price (%) 43.43 29.66 Carbon (Tonnes/h) 5563.22 1657.88 Carbon (%) 39.7 37.89 And export the output as a LaTeX table caption = '2020 Merit Order Effect results overview (weighted by volume).' label = 'moe_results_table' column_format = get_lined_column_format ( df_MOE_results . shape [ 1 ] + 1 ) latex_str = df_MOE_results . to_latex ( column_format = column_format , caption = caption , label = label ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{2020 Merit Order Effect results overview (weighted by volume).} \\label{moe_results_table} \\begin{tabular}{|l|l|l|} \\hline {} & Germany & Great Britain \\\\ \\hline Price ([EUR,GBP]/MWh) & 22.17 & 13.89 \\\\ \\hline Price (\\%) & 43.43 & 29.66 \\\\ \\hline Carbon (Tonnes/h) & 5563.22 & 1657.88 \\\\ \\hline Carbon (\\%) & 39.70 & 37.89 \\\\ \\hline \\end{tabular} \\end{table}\\]","title":"Price and CO2 MOE Results"},{"location":"dev-09-tables-and-figures/#literature-review","text":"Lastly we'll create our largest table, containing results from across the literature lit_results_data = [ { 'Study' : 'Sensfuss et al. (2008)' , 'MOE' : '7.83 \u20ac/MWh' , 'Period' : '2006' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Weigt (2009)' , 'MOE' : '10 \u20ac/MWh' , 'Period' : '2006-2008' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Keles et al. (2013)' , 'MOE' : '5.90 \u20ac/MWh' , 'Period' : '2006\u20132009' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Mulder and Scholtens (2013)' , 'MOE' : '0.03% price decrease per p.p increase in wind speeds' , 'Period' : '2006\u20132011' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Tveten et al. (2013)' , 'MOE' : '5.25 \u20ac/MWh (solar)' , 'Period' : '2006-2011' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Wurzburg et al. (2013)' , 'MOE' : '2% price decrease' , 'Period' : '2010-2012' , 'Region' : 'Germany & Austria' , 'Method' : 'RPR' , }, { 'Study' : 'Cludius et al. (2014)' , 'MOE' : '8 \u20ac/MWh' , 'Period' : '2010-2012' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Ketterer (2014)' , 'MOE' : '0.1-1.46% price decrease per p.p increase in wind generation' , 'Period' : '2006-2012' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Ederer (2015)' , 'MOE' : '1.3% price decrease per annual TWh of wind' , 'Period' : '2006-2014' , 'Region' : 'Germany' , 'Method' : 'MSS' , }, { 'Study' : 'Kyritsis et al. (2017)' , 'MOE' : '-' , 'Period' : '2010-2015' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Bublitz et al. (2017)' , 'MOE' : '5.40 \u20ac/MWh' , 'Period' : '2011-2015' , 'Region' : 'Germany' , 'Method' : 'ESS' , }, { 'Study' : 'Bublitz et al. (2017)' , 'MOE' : '6.80 \u20ac/MWh' , 'Period' : '2011-2015' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'de Miera et al. (2008)' , 'MOE' : '8.6-25.1% price decrease' , 'Period' : '2005-2007' , 'Region' : 'Spain' , 'Method' : 'ESS' , }, { 'Study' : 'Gelabert et al. (2011)' , 'MOE' : '3.7% price decrease' , 'Period' : '2005-2012' , 'Region' : 'Spain' , 'Method' : 'RPR' , }, { 'Study' : 'Ciarreta et al. (2014)' , 'MOE' : '25-45 \u20ac/MWh' , 'Period' : '2008\u20132012' , 'Region' : 'Spain' , 'Method' : 'ESS' , }, { 'Study' : 'Clo et al. (2015)' , 'MOE' : '2.3 \u20ac/MWh (solar), 4.2 \u20ac/MWh (wind)' , 'Period' : '2005\u20132013' , 'Region' : 'Italy' , 'Method' : 'RPR' , }, { 'Study' : 'Munksgaard and Morthorst (2008)' , 'MOE' : '1-4 \u20ac/MWh' , 'Period' : '2004-2006' , 'Region' : 'Denmark' , 'Method' : 'RPR' , }, { 'Study' : 'Jonsson et al. (2010)' , 'MOE' : '-' , 'Period' : '2006-2007' , 'Region' : 'Denmark' , 'Method' : 'RPR' , }, { 'Study' : 'Denny et al. (2017)' , 'MOE' : '3.40 \u20ac/MWh per GWh (wind)' , 'Period' : '2009' , 'Region' : 'Ireland' , 'Method' : 'RPR' , }, { 'Study' : 'Lunackova et al. (2017)' , 'MOE' : '1.2% price decrease per 10 % i ncrease in RES' , 'Period' : '2010-2015' , 'Region' : 'Czech Republic' , 'Method' : 'RPR' , }, { 'Study' : 'Dillig et al. (2016)' , 'MOE' : '50.29 \u20ac/MWh' , 'Period' : '2011-2013' , 'Region' : 'Germany' , 'Method' : 'MSS' , }, { 'Study' : 'McConnell et al. (2013)' , 'MOE' : '8.6% price decrease' , 'Period' : '2009-2010' , 'Region' : 'Australia' , 'Method' : 'MSS' , }, { 'Study' : 'Moreno et al. (2012)' , 'MOE' : '0.018% price increase per p.p. increase in RES penetration' , 'Period' : '1998\u20132009' , 'Region' : 'EU-27' , 'Method' : 'RPR' , }, { 'Study' : 'Woo et al. (2011)' , 'MOE' : '0.32-1.53 $/MWh' , 'Period' : '2007-2010' , 'Region' : 'Texas' , 'Method' : 'RPR' , }, { 'Study' : 'Kaufmann and Vaid (2016)' , 'MOE' : '0.26-1.86 $/MWh (solar)' , 'Period' : '2010-2012' , 'Region' : 'Massachusetts' , 'Method' : 'RPR' , }, { 'Study' : 'Woo et al. (2016)' , 'MOE' : '5.3 \\$/MWh (solar) and 3.3 \\$/MWh (wind) per GWh of RES' , 'Period' : '2012-2015' , 'Region' : 'California' , 'Method' : 'RPR' , }, { 'Study' : 'Paraschiv et al. (2014)' , 'MOE' : '0.15% price decrease per MWh of RES' , 'Period' : '2010-2013' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'O \\' Mahoney and Denny (2011)' , 'MOE' : '12% price decrease' , 'Period' : '2009' , 'Region' : 'Ireland' , 'Method' : 'RPR' , }, { 'Study' : 'Hildmann et al. (2015)' , 'MOE' : '13.4-18.6 \u20ac/MWh' , 'Period' : '2011-2013' , 'Region' : 'Germany and Austria' , 'Method' : 'MSS' , }, { 'Study' : 'Gil et al. (2012)' , 'MOE' : '9.72 \u20ac/MWh' , 'Period' : '2007-2010' , 'Region' : 'Spain' , 'Method' : 'RPR' , }, # { # Removed due to language barrier preventing method from being discerned # 'Study': 'Weber and Woll (2007)', # 'MOE': '4 \u20ac/MWh', # 'Period': '2006', # 'Region': 'Germany', # 'Method': '-', # }, { 'Study' : 'Halttunen et al. (2021)' , 'MOE' : '0.631 \u20ac/MWh per p.p. increase in RES penetration' , 'Period' : '2012-2019' , 'Region' : 'Germany' , 'Method' : 'RPR' , }, { 'Study' : 'Halttunen et al. (2021)' , 'MOE' : '0.482 \u20ac/MWh per p.p. increase in RES penetration' , 'Period' : '2010-2019' , 'Region' : 'Germany' , 'Method' : 'RPR' , } ] df_lit_results = pd . DataFrame ( lit_results_data ) df_lit_results [ 'Study Year' ] = df_lit_results [ 'Study' ] . str . split ( '(' ) . str [ 1 ] . str . replace ( ')' , '' ) . astype ( int ) df_lit_results = df_lit_results . sort_values ([ 'Method' , 'Study Year' , 'Study' ]) . drop ( columns = [ 'Study Year' ]) . reset_index ( drop = True ) df_lit_results . head () <ipython-input-20-15bc63a4c27e>:237: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True. df_lit_results['Study Year'] = df_lit_results['Study'].str.split('(').str[1].str.replace(')', '').astype(int) Unnamed: 0 Study MOE Period Region Method 0 Sensfuss et al. (2008) 7.83 \u20ac/MWh 2006 Germany ESS 1 de Miera et al. (2008) 8.6-25.1% price decrease 2005-2007 Spain ESS 2 Weigt (2009) 10 \u20ac/MWh 2006-2008 Germany ESS 3 Ciarreta et al. (2014) 25-45 \u20ac/MWh 2008\u20132012 Spain ESS 4 Bublitz et al. (2017) 5.40 \u20ac/MWh 2011-2015 Germany ESS We'll also export this as a LaTeX table caption = 'Results overview from the MOE literature' label = 'lit_results_table' column_format = get_lined_column_format ( df_lit_results . shape [ 1 ] + 1 ) latex_str = df_lit_results . to_latex ( column_format = column_format , caption = caption , label = label , index = False ) for old , new in latex_replacements . items (): latex_str = latex_str . replace ( old , new ) Latex ( latex_str ) \\[\\begin{table} \\centering \\caption{Results overview from the MOE literature} \\label{lit_results_table} \\begin{tabular}{|l|l|l|l|l|l|} \\hline Study & MOE & Period & Region & Method \\\\ \\hline Sensfuss et al. (2008) & 7.83 \u20ac/MWh & 2006 & Germany & ESS \\\\ \\hline de Miera et al. (2008) & 8.6-25.1\\% price decrease & 2005-2007 & Spain & ESS \\\\ \\hline Weigt (2009) & 10 \u20ac/MWh & 2006-2008 & Germany & ESS \\\\ \\hline Ciarreta et al. (2014) & 25-45 \u20ac/MWh & 2008\u20132012 & Spain & ESS \\\\ \\hline Bublitz et al. (2017) & 5.40 \u20ac/MWh & 2011-2015 & Germany & ESS \\\\ \\hline McConnell et al. (2013) & 8.6\\% price decrease & 2009-2010 & Australia & MSS \\\\ \\hline Ederer (2015) & 1.3\\% price decrease per annual TWh of wind & 2006-2014 & Germany & MSS \\\\ \\hline Hildmann et al. (2015) & 13.4-18.6 \u20ac/MWh & 2011-2013 & Germany and Austria & MSS \\\\ \\hline Dillig et al. (2016) & 50.29 \u20ac/MWh & 2011-2013 & Germany & MSS \\\\ \\hline Munksgaard and Morthorst (2008) & 1-4 \u20ac/MWh & 2004-2006 & Denmark & RPR \\\\ \\hline Jonsson et al. (2010) & - & 2006-2007 & Denmark & RPR \\\\ \\hline Gelabert et al. (2011) & 3.7\\% price decrease & 2005-2012 & Spain & RPR \\\\ \\hline O'Mahoney and Denny (2011) & 12\\% price decrease & 2009 & Ireland & RPR \\\\ \\hline Woo et al. (2011) & 0.32-1.53 \\$/MWh & 2007-2010 & Texas & RPR \\\\ \\hline Gil et al. (2012) & 9.72 \u20ac/MWh & 2007-2010 & Spain & RPR \\\\ \\hline Moreno et al. (2012) & 0.018\\% price increase per p.p. increase in RES ... & 1998\u20132009 & EU-27 & RPR \\\\ \\hline Keles et al. (2013) & 5.90 \u20ac/MWh & 2006\u20132009 & Germany & RPR \\\\ \\hline Mulder and Scholtens (2013) & 0.03\\% price decrease per p.p increase in wind s... & 2006\u20132011 & Germany & RPR \\\\ \\hline Tveten et al. (2013) & 5.25 \u20ac/MWh (solar) & 2006-2011 & Germany & RPR \\\\ \\hline Wurzburg et al. (2013) & 2\\% price decrease & 2010-2012 & Germany \\& Austria & RPR \\\\ \\hline Cludius et al. (2014) & 8 \u20ac/MWh & 2010-2012 & Germany & RPR \\\\ \\hline Ketterer (2014) & 0.1-1.46\\% price decrease per p.p increase in wi... & 2006-2012 & Germany & RPR \\\\ \\hline Paraschiv et al. (2014) & 0.15\\% price decrease per MWh of RES & 2010-2013 & Germany & RPR \\\\ \\hline Clo et al. (2015) & 2.3 \u20ac/MWh (solar), 4.2 \u20ac/MWh (wind) & 2005\u20132013 & Italy & RPR \\\\ \\hline Kaufmann and Vaid (2016) & 0.26-1.86 \\$/MWh (solar) & 2010-2012 & Massachusetts & RPR \\\\ \\hline Woo et al. (2016) & 5.3 \\textbackslash \\$/MWh (solar) and 3.3 \\textbackslash \\$/MWh (wind) per GW... & 2012-2015 & California & RPR \\\\ \\hline Bublitz et al. (2017) & 6.80 \u20ac/MWh & 2011-2015 & Germany & RPR \\\\ \\hline Denny et al. (2017) & 3.40 \u20ac/MWh per GWh (wind) & 2009 & Ireland & RPR \\\\ \\hline Kyritsis et al. (2017) & - & 2010-2015 & Germany & RPR \\\\ \\hline Lunackova et al. (2017) & 1.2\\% price decrease per 10\\% increase in RES & 2010-2015 & Czech Republic & RPR \\\\ \\hline Halttunen et al. (2021) & 0.631 \u20ac/MWh per p.p. increase in RES penetration & 2012-2019 & Germany & RPR \\\\ \\hline Halttunen et al. (2021) & 0.482 \u20ac/MWh per p.p. increase in RES penetration & 2010-2019 & Germany & RPR \\\\ \\hline \\end{tabular} \\end{table}\\]","title":"Literature Review"},{"location":"dev-09-tables-and-figures/#figures","text":"","title":"Figures"},{"location":"dev-09-tables-and-figures/#time-dimension-hyper-parameters","text":"We'll create a plot showing an example of how regression dates are converted into weightings for the time-series x = np . linspace ( 0 , 1 , 150 ) centers = [ 0.3 , 0.5 , 0.7 ] # Plotting fig , ax = plt . subplots ( dpi = 250 , figsize = ( 8 , 4 )) for center in centers : dist = lowess . get_dist ( x , center ) dist_threshold = lowess . get_dist_threshold ( dist , frac = 0.3 ) weights = lowess . dist_to_weights ( dist , dist_threshold ) ax . plot ( x , weights , color = 'k' ) x_pos = 0.4 ax . annotate ( 'Interval' , xy = ( x_pos , 0.95 ), xytext = ( x_pos , 1.00 ), xycoords = 'axes fraction' , fontsize = 6.5 , ha = 'center' , va = 'bottom' , bbox = dict ( boxstyle = 'square' , fc = 'white' ), arrowprops = dict ( arrowstyle = '-[, widthB=7.0, lengthB=1.5' , lw = 1.0 )) x_pos = 0.5 ax . annotate ( 'Bandwidth' , xy = ( x_pos , 0.06 ), xytext = ( x_pos , 0.11 ), xycoords = 'axes fraction' , fontsize = 9.5 , ha = 'center' , va = 'bottom' , bbox = dict ( boxstyle = 'square' , fc = 'white' ), arrowprops = dict ( arrowstyle = '-[, widthB=7.0, lengthB=1.5' , lw = 1.0 )) ax . set_xlim ( 0 , 1 ) ax . set_ylim ( 0 , 1.1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Data Fraction' ) ax . set_ylabel ( 'Relative Weighting' ) Text(0, 0.5, 'Relative Weighting')","title":"Time Dimension Hyper-Parameters"},{"location":"dev-10-ci-cd/","text":"CI/CD \u00b6 This notebook includes helper functions and processes used in the continuous integration and deployment of the moepy library. Imports \u00b6 #exports import os import re import typer import logging from warnings import warn from configparser import ConfigParser Initialising CLI \u00b6 #exports app = typer . Typer () Incrementing the Package Version \u00b6 We'll start by retrieving the current package version specified in settings.ini #exports @app . command () def get_current_package_version ( settings_fp : str = 'settings.ini' ): config = ConfigParser ( delimiters = [ '=' ]) config . read ( settings_fp ) version = config . get ( 'DEFAULT' , 'version' ) return version settings_fp = '../settings.ini' original_version = get_current_package_version ( settings_fp ) original_version '0.0.3' We'll now increment the package version #exports @app . command () def increment_package_version ( old_version : str , increment_level : str = 'micro' ): increment = lambda rev : str ( int ( rev ) + 1 ) major , minor , micro = old_version . split ( '.' ) # naming from - https://the-hitchhikers-guide-to-packaging.readthedocs.io/en/latest/specification.html#sequence-based-scheme if increment_level == 'major' : major = increment ( major ) elif increment_level == 'minor' : minor = increment ( minor ) elif increment_level == 'micro' : micro = increment ( micro ) new_version = '.' . join ([ major , minor , micro ]) return new_version increment_package_version ( original_version ) '0.0.4' But what about if we've made large changes to the code-base and wish to express the size of these revisions in the version? For that we can specify the increment_level . increment_package_version ( original_version , increment_level = 'major' ) '1.0.3' And finally we can set the version #exports @app . command () def set_current_package_version ( version : str , settings_fp : str = 'settings.ini' ): version = version . replace ( 'v' , '' ) config = ConfigParser ( delimiters = [ '=' ]) config . read ( settings_fp ) config . set ( 'DEFAULT' , 'version' , version ) with open ( settings_fp , 'w' ) as configfile : config . write ( configfile ) logger = logging . getLogger ( 'package_release' ) logger . setLevel ( 'INFO' ) logger . info ( f 'The package version has to be updated to { version } ' ) return set_current_package_version ( '9.9.9' , settings_fp ) get_current_package_version ( settings_fp ) '9.9.9' Before we move on we'll change the version on file back to the original set_current_package_version ( original_version , settings_fp ) get_current_package_version ( settings_fp ) '0.0.3' Finally we need to ensure the CLI app is available when the module is loaded. N.b. we've included the condition '__file__' in globals() to make sure this isn't when inside the notebook #exports if __name__ == '__main__' and '__file__' in globals (): app ()","title":"CI-CD"},{"location":"dev-10-ci-cd/#cicd","text":"This notebook includes helper functions and processes used in the continuous integration and deployment of the moepy library.","title":"CI/CD"},{"location":"dev-10-ci-cd/#imports","text":"#exports import os import re import typer import logging from warnings import warn from configparser import ConfigParser","title":"Imports"},{"location":"dev-10-ci-cd/#initialising-cli","text":"#exports app = typer . Typer ()","title":"Initialising CLI"},{"location":"dev-10-ci-cd/#incrementing-the-package-version","text":"We'll start by retrieving the current package version specified in settings.ini #exports @app . command () def get_current_package_version ( settings_fp : str = 'settings.ini' ): config = ConfigParser ( delimiters = [ '=' ]) config . read ( settings_fp ) version = config . get ( 'DEFAULT' , 'version' ) return version settings_fp = '../settings.ini' original_version = get_current_package_version ( settings_fp ) original_version '0.0.3' We'll now increment the package version #exports @app . command () def increment_package_version ( old_version : str , increment_level : str = 'micro' ): increment = lambda rev : str ( int ( rev ) + 1 ) major , minor , micro = old_version . split ( '.' ) # naming from - https://the-hitchhikers-guide-to-packaging.readthedocs.io/en/latest/specification.html#sequence-based-scheme if increment_level == 'major' : major = increment ( major ) elif increment_level == 'minor' : minor = increment ( minor ) elif increment_level == 'micro' : micro = increment ( micro ) new_version = '.' . join ([ major , minor , micro ]) return new_version increment_package_version ( original_version ) '0.0.4' But what about if we've made large changes to the code-base and wish to express the size of these revisions in the version? For that we can specify the increment_level . increment_package_version ( original_version , increment_level = 'major' ) '1.0.3' And finally we can set the version #exports @app . command () def set_current_package_version ( version : str , settings_fp : str = 'settings.ini' ): version = version . replace ( 'v' , '' ) config = ConfigParser ( delimiters = [ '=' ]) config . read ( settings_fp ) config . set ( 'DEFAULT' , 'version' , version ) with open ( settings_fp , 'w' ) as configfile : config . write ( configfile ) logger = logging . getLogger ( 'package_release' ) logger . setLevel ( 'INFO' ) logger . info ( f 'The package version has to be updated to { version } ' ) return set_current_package_version ( '9.9.9' , settings_fp ) get_current_package_version ( settings_fp ) '9.9.9' Before we move on we'll change the version on file back to the original set_current_package_version ( original_version , settings_fp ) get_current_package_version ( settings_fp ) '0.0.3' Finally we need to ensure the CLI app is available when the module is loaded. N.b. we've included the condition '__file__' in globals() to make sure this isn't when inside the notebook #exports if __name__ == '__main__' and '__file__' in globals (): app ()","title":"Incrementing the Package Version"},{"location":"ug-01-quantile/","text":"Quantile Predictions \u00b6 In this example we'll use power output data from Portugese hydro-plants to demonstrate how the quantile LOWESS model can be used. Imports \u00b6 import pandas as pd import matplotlib.pyplot as plt from moepy import lowess , eda Loading Data \u00b6 We'll start by reading in the Portugese hydro output data df_portugal_hydro = pd . read_csv ( '../data/lowess_examples/portugese_hydro.csv' ) df_portugal_hydro . index = pd . to_datetime ( df_portugal_hydro [ 'datetime' ]) df_portugal_hydro = df_portugal_hydro . drop ( columns = 'datetime' ) df_portugal_hydro [ 'day_of_the_year' ] = df_portugal_hydro . index . dayofyear df_portugal_hydro = df_portugal_hydro . resample ( 'D' ) . mean () df_portugal_hydro = df_portugal_hydro . rename ( columns = { 'power_MW' : 'average_power_MW' }) df_portugal_hydro . head () datetime average_power_MW day_of_the_year 2015-01-01 698.5 1 2015-01-02 1065.75 2 2015-01-03 905.125 3 2015-01-04 795.708 4 2015-01-05 1141.62 5 Quantile LOWESS \u00b6 We now just need to feed this data into our quantile_model wrapper # Estimating the quantiles df_quantiles = lowess . quantile_model ( df_portugal_hydro [ 'day_of_the_year' ] . values , df_portugal_hydro [ 'average_power_MW' ] . values , frac = 0.4 , num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 9/9 [00:16 < 00:02, 1.73s/it] x p90 p80 p70 p60 p50 p40 p30 p20 p10 1 1885.08 1400.78 1006.97 910.769 795.475 693.001 604.221 498.096 407.17 2 1885.93 1406.29 1015.76 917.074 800.255 697.121 607.521 500.673 409.021 3 1886.8 1411.81 1024.54 923.37 805.008 701.225 610.814 503.239 410.866 4 1887.68 1417.32 1033.31 929.659 809.738 705.317 614.105 505.797 412.695 5 1888.57 1422.84 1042.08 935.952 814.456 709.409 617.404 508.359 414.485 We can then visualise the estimated quantile fits of the data fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_portugal_hydro [ 'day_of_the_year' ], df_portugal_hydro [ 'average_power_MW' ], s = 1 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) eda . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.8 )) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Hydro Power Average (MW)' ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) (0.0, 2620.8375) We can also ask questions like: \"what day of a standard year would the lowest power output be recorded?\" scenario = 'p50' print ( f 'In a { scenario } year the lowest hydro power output will most likely fall on day { df_quantiles [ scenario ] . idxmin () } ' ) In a p50 year the lowest hydro power output will most likely fall on day 228 We can also identify the peridos when our predictions will have the greatest uncertainty s_80pct_pred_intvl = df_quantiles [ 'p90' ] - df_quantiles [ 'p10' ] print ( f 'Day { s_80pct_pred_intvl . idxmax () } is most likely to have the greatest variation in hydro power output' ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_80pct_pred_intvl . plot ( ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Hydro Power Output 80% \\n Prediction Interval Size (MW)' ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) Day 115 is most likely to have the greatest variation in hydro power output (0.0, 1724.0724938300584)","title":"Quantile Predictions"},{"location":"ug-01-quantile/#quantile-predictions","text":"In this example we'll use power output data from Portugese hydro-plants to demonstrate how the quantile LOWESS model can be used.","title":"Quantile Predictions"},{"location":"ug-01-quantile/#imports","text":"import pandas as pd import matplotlib.pyplot as plt from moepy import lowess , eda","title":"Imports"},{"location":"ug-01-quantile/#loading-data","text":"We'll start by reading in the Portugese hydro output data df_portugal_hydro = pd . read_csv ( '../data/lowess_examples/portugese_hydro.csv' ) df_portugal_hydro . index = pd . to_datetime ( df_portugal_hydro [ 'datetime' ]) df_portugal_hydro = df_portugal_hydro . drop ( columns = 'datetime' ) df_portugal_hydro [ 'day_of_the_year' ] = df_portugal_hydro . index . dayofyear df_portugal_hydro = df_portugal_hydro . resample ( 'D' ) . mean () df_portugal_hydro = df_portugal_hydro . rename ( columns = { 'power_MW' : 'average_power_MW' }) df_portugal_hydro . head () datetime average_power_MW day_of_the_year 2015-01-01 698.5 1 2015-01-02 1065.75 2 2015-01-03 905.125 3 2015-01-04 795.708 4 2015-01-05 1141.62 5","title":"Loading Data"},{"location":"ug-01-quantile/#quantile-lowess","text":"We now just need to feed this data into our quantile_model wrapper # Estimating the quantiles df_quantiles = lowess . quantile_model ( df_portugal_hydro [ 'day_of_the_year' ] . values , df_portugal_hydro [ 'average_power_MW' ] . values , frac = 0.4 , num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 9/9 [00:16 < 00:02, 1.73s/it] x p90 p80 p70 p60 p50 p40 p30 p20 p10 1 1885.08 1400.78 1006.97 910.769 795.475 693.001 604.221 498.096 407.17 2 1885.93 1406.29 1015.76 917.074 800.255 697.121 607.521 500.673 409.021 3 1886.8 1411.81 1024.54 923.37 805.008 701.225 610.814 503.239 410.866 4 1887.68 1417.32 1033.31 929.659 809.738 705.317 614.105 505.797 412.695 5 1888.57 1422.84 1042.08 935.952 814.456 709.409 617.404 508.359 414.485 We can then visualise the estimated quantile fits of the data fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_portugal_hydro [ 'day_of_the_year' ], df_portugal_hydro [ 'average_power_MW' ], s = 1 , color = 'k' , alpha = 0.5 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) eda . hide_spines ( ax ) ax . legend ( frameon = False , bbox_to_anchor = ( 1 , 0.8 )) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Hydro Power Average (MW)' ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) (0.0, 2620.8375) We can also ask questions like: \"what day of a standard year would the lowest power output be recorded?\" scenario = 'p50' print ( f 'In a { scenario } year the lowest hydro power output will most likely fall on day { df_quantiles [ scenario ] . idxmin () } ' ) In a p50 year the lowest hydro power output will most likely fall on day 228 We can also identify the peridos when our predictions will have the greatest uncertainty s_80pct_pred_intvl = df_quantiles [ 'p90' ] - df_quantiles [ 'p10' ] print ( f 'Day { s_80pct_pred_intvl . idxmax () } is most likely to have the greatest variation in hydro power output' ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) s_80pct_pred_intvl . plot ( ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Day of the Year' ) ax . set_ylabel ( 'Hydro Power Output 80% \\n Prediction Interval Size (MW)' ) ax . set_xlim ( 0 , 365 ) ax . set_ylim ( 0 ) Day 115 is most likely to have the greatest variation in hydro power output (0.0, 1724.0724938300584)","title":"Quantile LOWESS"},{"location":"ug-02-confidence/","text":"Bootstrapped LOWESS for Confidence Intervals \u00b6 This notebook outlines how to use the moepy library to generate confidence intervals around the LOWESS estimates, using the famous LIGO gravitationl wave data as an example. N.b. I have no expertise of signal processing in this particular context, this is merely an example of how LOWESS confidence intervals can be used to limit the domain of your prediction to where you have greatest certainty in it. Imports \u00b6 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from moepy import lowess , eda Loading Data \u00b6 We'll start by loading the LIGO data in df_LIGO = pd . read_csv ( '../data/lowess_examples/LIGO.csv' ) df_LIGO . head () Unnamed: 0 frequency L1 H1 H1_smoothed 0 0 2.38353e-18 2.21569e-20 3.24e-18 1 0.25 1.68562e-18 2.01396e-20 2.6449e-19 2 0.5 1.24297e-21 5.5185e-21 9e-20 3 0.75 6.6778e-22 2.54612e-21 4.48443e-20 4 1 6.80032e-22 3.33945e-21 2.67769e-20 Baseline Fit \u00b6 We'll quickly plot the observed data alongside the smoothed estimate provided in the raw data fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 ) ax . plot ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1_smoothed' ], color = 'r' , alpha = 1 , label = 'Existing Smoothing' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) LOWESS Fit \u00b6 x = df_LIGO [ 'frequency' ] . values y = np . log ( df_LIGO [ 'H1' ]) . values df_bootstrap = lowess . bootstrap_model ( x , y , num_runs = 500 , frac = 0.2 , num_fits = 30 ) df_bootstrap . head () 100% 500/500 [01:37 < 00:00, 0.19s/it] x 0 1 2 3 4 5 6 7 8 9 ... 490 491 492 493 494 495 496 497 498 499 0 -52.5227 -52.724 -52.4039 -52.5183 -52.5897 -52.4715 -52.5306 -52.4727 -52.5534 -52.5212 ... -52.5536 -52.6798 -52.508 -52.554 -52.4346 -52.6022 -52.355 -52.5031 -52.5719 -52.5057 0.25 -52.5235 -52.7245 -52.4049 -52.5192 -52.5904 -52.4723 -52.5314 -52.4737 -52.5542 -52.522 ... -52.5544 -52.6804 -52.5088 -52.5547 -52.4356 -52.6029 -52.3561 -52.504 -52.5727 -52.5066 0.5 -52.5244 -52.725 -52.4059 -52.5201 -52.5912 -52.4732 -52.5322 -52.4746 -52.555 -52.5228 ... -52.5553 -52.681 -52.5096 -52.5555 -52.4365 -52.6037 -52.3573 -52.5048 -52.5735 -52.5075 0.75 -52.5252 -52.7256 -52.4069 -52.521 -52.5919 -52.4741 -52.533 -52.4755 -52.5559 -52.5236 ... -52.5561 -52.6816 -52.5105 -52.5562 -52.4375 -52.6044 -52.3584 -52.5057 -52.5743 -52.5084 1 -52.5261 -52.7261 -52.4079 -52.5218 -52.5927 -52.4749 -52.5338 -52.4764 -52.5567 -52.5244 ... -52.5569 -52.6822 -52.5113 -52.5569 -52.4384 -52.6051 -52.3595 -52.5065 -52.5751 -52.5093 Using df_bootstrap we can calculate the confidence interval of our predictions, the Pandas DataFrame quantile method makes this particularly simple. df_conf_intvl = lowess . get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 , zorder = 1 ) ax . fill_between ( df_conf_intvl . index , np . exp ( df_conf_intvl [ 'min' ]), np . exp ( df_conf_intvl [ 'max' ]), color = 'r' , alpha = 1 , label = '95% Confidence' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We can see that we capture the middle and higher frequencies fairly well but due to the smaller number of data-points in the low frequency region the LOWESS fit is unable to model it as well. The frac could be decreased to improve this but then comes at the cost of processing more inaccurate estimates in the other regions. One way to address this could be to introduce an option where frac can be varied for each local regression model. For now we'll just limit our prediction to the domain where the confidence interval is 'reasonable', we'll start by calculating the 95% confidence interval. s_95pct_conf_intvl = ( df_bootstrap . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . T . rename ( columns = { 0.975 : '95pct_pred_intvl' }) [ '95pct_pred_intvl' ] ) s_95pct_conf_intvl x 0.00 0.404673 0.25 0.404049 0.50 0.403448 0.75 0.402822 1.00 0.402187 ... 2047.00 0.077741 2047.25 0.077827 2047.50 0.077913 2047.75 0.077999 2048.00 0.078085 Name: 95pct_pred_intvl, Length: 8193, dtype: float64 We'll now define the 'reasonable' confidence interval threshold, in this case using the value that icludes 95% of the values. x_max_pct = 0.95 # Plotting fig , ax = plt . subplots ( dpi = 150 ) hist = sns . histplot ( s_95pct_conf_intvl , ax = ax ) y_max = np . ceil ( max ([ h . get_height () for h in hist . patches ]) / 1e2 ) * 1e2 x_max = s_95pct_conf_intvl . quantile ( x_max_pct ) ax . plot ([ x_max , x_max ], [ 0 , y_max ], linestyle = '--' , color = 'k' , label = '95% Coverage' ) ax . set_ylim ( 0 , y_max ) eda . hide_spines ( ax ) We'll then only plot our confidence interval when it's in this 'reasonable' range conf_intvl_idxs_to_keep = ( s_95pct_conf_intvl < x_max ) . replace ( False , np . nan ) . dropna () . index # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 , zorder = 1 ) ax . fill_between ( conf_intvl_idxs_to_keep , np . exp ( df_conf_intvl . loc [ conf_intvl_idxs_to_keep , 'min' ]), np . exp ( df_conf_intvl . loc [ conf_intvl_idxs_to_keep , 'max' ]), color = 'r' , alpha = 1 , label = '95% Confidence' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax )","title":"Confidence Intervals"},{"location":"ug-02-confidence/#bootstrapped-lowess-for-confidence-intervals","text":"This notebook outlines how to use the moepy library to generate confidence intervals around the LOWESS estimates, using the famous LIGO gravitationl wave data as an example. N.b. I have no expertise of signal processing in this particular context, this is merely an example of how LOWESS confidence intervals can be used to limit the domain of your prediction to where you have greatest certainty in it.","title":"Bootstrapped LOWESS for Confidence Intervals"},{"location":"ug-02-confidence/#imports","text":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from moepy import lowess , eda","title":"Imports"},{"location":"ug-02-confidence/#loading-data","text":"We'll start by loading the LIGO data in df_LIGO = pd . read_csv ( '../data/lowess_examples/LIGO.csv' ) df_LIGO . head () Unnamed: 0 frequency L1 H1 H1_smoothed 0 0 2.38353e-18 2.21569e-20 3.24e-18 1 0.25 1.68562e-18 2.01396e-20 2.6449e-19 2 0.5 1.24297e-21 5.5185e-21 9e-20 3 0.75 6.6778e-22 2.54612e-21 4.48443e-20 4 1 6.80032e-22 3.33945e-21 2.67769e-20","title":"Loading Data"},{"location":"ug-02-confidence/#baseline-fit","text":"We'll quickly plot the observed data alongside the smoothed estimate provided in the raw data fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 ) ax . plot ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1_smoothed' ], color = 'r' , alpha = 1 , label = 'Existing Smoothing' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax )","title":"Baseline Fit"},{"location":"ug-02-confidence/#lowess-fit","text":"x = df_LIGO [ 'frequency' ] . values y = np . log ( df_LIGO [ 'H1' ]) . values df_bootstrap = lowess . bootstrap_model ( x , y , num_runs = 500 , frac = 0.2 , num_fits = 30 ) df_bootstrap . head () 100% 500/500 [01:37 < 00:00, 0.19s/it] x 0 1 2 3 4 5 6 7 8 9 ... 490 491 492 493 494 495 496 497 498 499 0 -52.5227 -52.724 -52.4039 -52.5183 -52.5897 -52.4715 -52.5306 -52.4727 -52.5534 -52.5212 ... -52.5536 -52.6798 -52.508 -52.554 -52.4346 -52.6022 -52.355 -52.5031 -52.5719 -52.5057 0.25 -52.5235 -52.7245 -52.4049 -52.5192 -52.5904 -52.4723 -52.5314 -52.4737 -52.5542 -52.522 ... -52.5544 -52.6804 -52.5088 -52.5547 -52.4356 -52.6029 -52.3561 -52.504 -52.5727 -52.5066 0.5 -52.5244 -52.725 -52.4059 -52.5201 -52.5912 -52.4732 -52.5322 -52.4746 -52.555 -52.5228 ... -52.5553 -52.681 -52.5096 -52.5555 -52.4365 -52.6037 -52.3573 -52.5048 -52.5735 -52.5075 0.75 -52.5252 -52.7256 -52.4069 -52.521 -52.5919 -52.4741 -52.533 -52.4755 -52.5559 -52.5236 ... -52.5561 -52.6816 -52.5105 -52.5562 -52.4375 -52.6044 -52.3584 -52.5057 -52.5743 -52.5084 1 -52.5261 -52.7261 -52.4079 -52.5218 -52.5927 -52.4749 -52.5338 -52.4764 -52.5567 -52.5244 ... -52.5569 -52.6822 -52.5113 -52.5569 -52.4384 -52.6051 -52.3595 -52.5065 -52.5751 -52.5093 Using df_bootstrap we can calculate the confidence interval of our predictions, the Pandas DataFrame quantile method makes this particularly simple. df_conf_intvl = lowess . get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 , zorder = 1 ) ax . fill_between ( df_conf_intvl . index , np . exp ( df_conf_intvl [ 'min' ]), np . exp ( df_conf_intvl [ 'max' ]), color = 'r' , alpha = 1 , label = '95% Confidence' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax ) We can see that we capture the middle and higher frequencies fairly well but due to the smaller number of data-points in the low frequency region the LOWESS fit is unable to model it as well. The frac could be decreased to improve this but then comes at the cost of processing more inaccurate estimates in the other regions. One way to address this could be to introduce an option where frac can be varied for each local regression model. For now we'll just limit our prediction to the domain where the confidence interval is 'reasonable', we'll start by calculating the 95% confidence interval. s_95pct_conf_intvl = ( df_bootstrap . quantile ([ 0.025 , 0.975 ], axis = 1 ) . diff () . dropna ( how = 'all' ) . T . rename ( columns = { 0.975 : '95pct_pred_intvl' }) [ '95pct_pred_intvl' ] ) s_95pct_conf_intvl x 0.00 0.404673 0.25 0.404049 0.50 0.403448 0.75 0.402822 1.00 0.402187 ... 2047.00 0.077741 2047.25 0.077827 2047.50 0.077913 2047.75 0.077999 2048.00 0.078085 Name: 95pct_pred_intvl, Length: 8193, dtype: float64 We'll now define the 'reasonable' confidence interval threshold, in this case using the value that icludes 95% of the values. x_max_pct = 0.95 # Plotting fig , ax = plt . subplots ( dpi = 150 ) hist = sns . histplot ( s_95pct_conf_intvl , ax = ax ) y_max = np . ceil ( max ([ h . get_height () for h in hist . patches ]) / 1e2 ) * 1e2 x_max = s_95pct_conf_intvl . quantile ( x_max_pct ) ax . plot ([ x_max , x_max ], [ 0 , y_max ], linestyle = '--' , color = 'k' , label = '95% Coverage' ) ax . set_ylim ( 0 , y_max ) eda . hide_spines ( ax ) We'll then only plot our confidence interval when it's in this 'reasonable' range conf_intvl_idxs_to_keep = ( s_95pct_conf_intvl < x_max ) . replace ( False , np . nan ) . dropna () . index # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( df_LIGO [ 'frequency' ], df_LIGO [ 'H1' ], color = 'k' , linewidth = 0 , s = 1 , zorder = 1 ) ax . fill_between ( conf_intvl_idxs_to_keep , np . exp ( df_conf_intvl . loc [ conf_intvl_idxs_to_keep , 'min' ]), np . exp ( df_conf_intvl . loc [ conf_intvl_idxs_to_keep , 'max' ]), color = 'r' , alpha = 1 , label = '95% Confidence' ) ax . set_yscale ( 'log' ) ax . legend ( frameon = False ) eda . hide_spines ( ax )","title":"LOWESS Fit"},{"location":"ug-03-power-curve/","text":"Power Curves \u00b6 In this notebook we'll look at how we can use our LOWESS methods to first fit a power curve for a wind turbine, then estimate the uncertainty in our results. We'll then utilise a feature of the quantile LOWESS fits to demonstrate how these techniques can also be used for cleaning raw turbine data. Imports \u00b6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from moepy import lowess , eda Power Curve Fitting \u00b6 We'll start by loading in some clean turbine output and wind speed data, this was sourced from the Power Curve Working Group Analysis example data repository. df_pcwga = ( pd . read_csv ( '../data/lowess_examples/turbine_power_wind_speed_clean.csv' ) [[ 'TimeStamp' , 'Turbine Wind Speed Mean' , 'Turbine Power' ]] . replace ( - 99.99 , np . nan ) . dropna () ) df_pcwga . head () Unnamed: 0 TimeStamp Turbine Wind Speed Mean Turbine Power 0 07/10/2011 12:50 15.51 1996.91 1 07/10/2011 13:00 15.7101 1987.74 2 07/10/2011 13:10 16.6708 1991.9 3 07/10/2011 13:20 15.2098 1987.7 4 07/10/2011 13:30 15.44 1991.03 We'll then fit a standard LOWESS and visualise the results %% time x = df_pcwga [ 'Turbine Wind Speed Mean' ] . values y = df_pcwga [ 'Turbine Power' ] . values lowess_model = lowess . Lowess () lowess_model . fit ( x , y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( x , y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 512 ms This looks good but we can do more. In this next step we'll estimate the upper and lower quantiles of the power curve fit that represent a prediction interval of 68%. # Estimating the quantiles df_quantiles = lowess . quantile_model ( x , y , frac = 0.2 , qs = [ 0.16 , 0.84 ], num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 2/2 [00:06 < 00:03, 3.19s/it] x p84 p16 0.323938 -16.2569 -8.34796 0.463498 -14.4719 -8.53898 0.475784 -14.3153 -8.55634 0.503802 -13.9573 -8.59603 0.546645 -13.4059 -8.65677 We'll visualise this fit within the domain where the quantiles do not cross valid_pred_intvl_idx = ( df_quantiles [ 'p84' ] - df_quantiles [ 'p16' ]) . pipe ( lambda s : s [ s > 0 ]) . index # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 , label = 'Observations' ) ax . fill_between ( valid_pred_intvl_idx , df_quantiles . loc [ valid_pred_intvl_idx , 'p16' ], df_quantiles . loc [ valid_pred_intvl_idx , 'p84' ], color = 'r' , alpha = 0.25 , label = '68% Prediction Interval' ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 0 ] . _sizes = [ 20 ] lgnd . legendHandles [ 0 ] . set_alpha ( 1 ) With the prediction interval we've looked at the likely range of power output for given wind speeds, but what if instead of the range of the underlying values we wanted to know the range in our estimate of the average power curve? For this we can use confidence intervals, which express the certainty we have in the particular statistical parameter we're calculating. In order to estimate this uncertainty we'll first bootstrap our model. df_bootstrap = lowess . bootstrap_model ( x , y , num_runs = 500 , frac = 0.2 , num_fits = 30 ) df_bootstrap . head () 100% 500/500 [00:55 < 00:00, 0.11s/it] x 0 1 2 3 4 5 6 7 8 9 ... 490 491 492 493 494 495 496 497 498 499 15.51 1977.78 1978.05 1978.95 1978.33 1979.43 1978.33 1978.78 1979.79 1979.47 1979 ... 1978.14 1978.5 1977.23 1977.66 1976.37 1976.28 1978.61 1978.84 1977.94 1977.22 15.7101 1978.48 1978.6 1979.52 1979.05 1979.96 1979.06 1979.39 1980.34 1980.1 1979.74 ... 1978.92 1979.14 1977.82 1978.38 1977.14 1977.14 1979.45 1979.56 1978.61 1977.96 16.6708 1981.24 1981.17 1981.83 1981.57 1981.87 1981.95 1981.61 1982.23 1982.3 1982.31 ... 1981.98 1981.56 1980.13 1981.42 1980.3 1980.6 1982.85 1981.93 1981.15 1980.83 15.2098 1976.43 1977.04 1977.8 1977.1 1978.41 1976.9 1977.58 1978.67 1978.21 1977.58 ... 1976.64 1977.27 1976.05 1976.45 1974.94 1974.57 1977.02 1977.52 1976.61 1975.82 15.44 1977.49 1977.84 1978.72 1978.03 1979.21 1978.03 1978.52 1979.56 1979.2 1978.69 ... 1977.83 1978.23 1976.99 1977.4 1976.06 1975.93 1978.28 1978.54 1977.66 1976.92 From the bootstrapped results we can then extract the confidence intervals, in our case we'll look at the range covering 95% of our estimates. df_conf_intvl = lowess . get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 , label = 'Observations' ) ax . fill_between ( df_conf_intvl . index , df_conf_intvl [ 'min' ], df_conf_intvl [ 'max' ], color = 'r' , alpha = 1 , label = '95% Confidence' ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 0 ] . _sizes = [ 20 ] lgnd . legendHandles [ 0 ] . set_alpha ( 1 ) We'll now visualise how the width of the confidence interval changes with wind speed. Interestingly the two troughs in the confidence interval width appear to correspond to the cut-in and start of the rated power wind speeds. fig , ax = plt . subplots ( dpi = 150 ) df_conf_intvl . diff ( axis = 1 )[ 'max' ] . plot ( ax = ax , color = 'r' , linewidth = 1 ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( '95% Confidence \\n Interval Width (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) ax . get_legend () . remove () Power Curve Cleaning \u00b6 We'll start by loading the raw turbine wind speed and output data in df_raw_pc = pd . read_csv ( '../data/lowess_examples/turbine_power_wind_speed_raw.csv' ) df_raw_pc . head ( 3 ) Unnamed: 0 UTC active_power wind_speed 0 30/09/2017 18:26 14 4.02 1 30/09/2017 18:56 14 4.02 2 30/09/2017 19:26 14 4.02 We'll then try and fit a LOWESS estimate for the power curve. %% time x = df_raw_pc [ 'wind_speed' ] . values y = df_raw_pc [ 'active_power' ] . values lowess_model = lowess . Lowess () lowess_model . fit ( x , y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( x , y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 788 ms Unfortunately the fit is thrown by the large number of occurences where the farm is under-powered or set to output 0, we want to remove these so that we can estimate the 'standard' power curve. We'll create a quantile LOWESS fit to see if that helps us understand the data any better. # Estimating the quantiles df_quantiles = lowess . quantile_model ( x , y , frac = 0.2 , qs = np . linspace ( 0.025 , 0.975 , 41 ), num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 41/41 [02:52 < 00:05, 4.20s/it] x p97 p95 p92 p90 p88 p85 p83 p80 p78 p76 ... p23 p21 p19 p16 p14 p12 p9 p7 p4 p2 0.25 -31.1389 -36.0637 -48.7875 -50.2252 -54.3752 -61.8818 -64.9263 -67.3705 -68.176 -69.0324 ... -78.7881 -73.9447 -69.322 -63.1938 -58.9408 -54.0337 -50.6578 -49.2366 -50.801 -51.8248 0.33 -24.4137 -30.6108 -43.6044 -45.5205 -49.913 -57.5452 -60.7292 -63.275 -64.2354 -65.2338 ... -77.3919 -72.8946 -68.5794 -62.7402 -58.6804 -53.9613 -50.6087 -49.1445 -50.546 -51.5789 0.41 -17.8378 -25.26 -38.5274 -40.8969 -45.5215 -53.2747 -56.5928 -59.2317 -60.3441 -61.4871 ... -76.014 -71.8585 -67.8522 -62.3073 -58.4534 -53.9134 -50.5746 -49.0702 -50.2914 -51.3353 0.49 -11.3659 -19.9748 -33.5237 -36.3257 -41.1744 -49.0455 -52.4947 -55.22 -56.4821 -57.7727 ... -74.6419 -70.8232 -67.1261 -61.8803 -58.2436 -53.8769 -50.5466 -49.0072 -50.0365 -51.0934 0.57 -4.9476 -14.7154 -28.557 -31.7751 -36.8429 -44.8302 -48.4098 -51.2177 -52.6278 -54.069 ... -73.2619 -69.7745 -66.3859 -61.4438 -58.0338 -53.8375 -50.5151 -48.9481 -49.7806 -50.8524 Plotting these we can see an interesting relationship, where for many of the lower quantiles the estimate peaks then quickly drops to around 0. It is the area below and to the right of these peaks that we want to remove from our power curve estimate. fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.1 , color = 'k' , alpha = 1 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( 0 ) (0.0, 2557.302509884304) We'll first identify the ratio between the peak value and the final value (above speeds of 25 m/s). We'll then remove points after the peak where the ratio between their peak and final values exceeds a defined threshold (in our case 2). N.b. there's probably a much nicer way to do this where the for loop isn't needed, this would be handy to implement as it would be good to have a vector containing all of the indexes that have been removed. exceeded_quantiles = (( df_quantiles . max () / df_quantiles . iloc [ - 1 ] . clip ( 0.1 ) > 2 ) . replace ( False , np . nan ) . dropna () . index ) s_maxs = df_quantiles [ exceeded_quantiles ] . max () s_idxmaxs = df_quantiles [ exceeded_quantiles ] . idxmax () cleaned_x = x cleaned_y = y for exceeded_quantile in exceeded_quantiles : min_x = s_idxmaxs [ exceeded_quantile ] max_y = s_maxs [ exceeded_quantile ] idxs_to_remove = ( cleaned_x > min_x ) & ( cleaned_y < max_y ) cleaned_x = cleaned_x [ ~ idxs_to_remove ] cleaned_y = cleaned_y [ ~ idxs_to_remove ] Visualising the results we can clearly see that the unwanted periods have been removed fig , axs = plt . subplots ( dpi = 250 , ncols = 2 , figsize = ( 10 , 4 )) axs [ 0 ] . scatter ( x , y , s = 0.1 , color = 'k' , alpha = 1 ) axs [ 1 ] . scatter ( cleaned_x , cleaned_y , s = 0.1 , color = 'k' , alpha = 1 ) axs [ 0 ] . set_title ( 'Original' ) axs [ 1 ] . set_title ( 'Cleaned' ) for ax in axs : eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) We're now ready to make our power curve LOWESS estimate again %% time lowess_model = lowess . Lowess () lowess_model . fit ( cleaned_x , cleaned_y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( cleaned_x , cleaned_y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 728 ms","title":"Power Curve Fitting"},{"location":"ug-03-power-curve/#power-curves","text":"In this notebook we'll look at how we can use our LOWESS methods to first fit a power curve for a wind turbine, then estimate the uncertainty in our results. We'll then utilise a feature of the quantile LOWESS fits to demonstrate how these techniques can also be used for cleaning raw turbine data.","title":"Power Curves"},{"location":"ug-03-power-curve/#imports","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from moepy import lowess , eda","title":"Imports"},{"location":"ug-03-power-curve/#power-curve-fitting","text":"We'll start by loading in some clean turbine output and wind speed data, this was sourced from the Power Curve Working Group Analysis example data repository. df_pcwga = ( pd . read_csv ( '../data/lowess_examples/turbine_power_wind_speed_clean.csv' ) [[ 'TimeStamp' , 'Turbine Wind Speed Mean' , 'Turbine Power' ]] . replace ( - 99.99 , np . nan ) . dropna () ) df_pcwga . head () Unnamed: 0 TimeStamp Turbine Wind Speed Mean Turbine Power 0 07/10/2011 12:50 15.51 1996.91 1 07/10/2011 13:00 15.7101 1987.74 2 07/10/2011 13:10 16.6708 1991.9 3 07/10/2011 13:20 15.2098 1987.7 4 07/10/2011 13:30 15.44 1991.03 We'll then fit a standard LOWESS and visualise the results %% time x = df_pcwga [ 'Turbine Wind Speed Mean' ] . values y = df_pcwga [ 'Turbine Power' ] . values lowess_model = lowess . Lowess () lowess_model . fit ( x , y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( x , y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 512 ms This looks good but we can do more. In this next step we'll estimate the upper and lower quantiles of the power curve fit that represent a prediction interval of 68%. # Estimating the quantiles df_quantiles = lowess . quantile_model ( x , y , frac = 0.2 , qs = [ 0.16 , 0.84 ], num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 2/2 [00:06 < 00:03, 3.19s/it] x p84 p16 0.323938 -16.2569 -8.34796 0.463498 -14.4719 -8.53898 0.475784 -14.3153 -8.55634 0.503802 -13.9573 -8.59603 0.546645 -13.4059 -8.65677 We'll visualise this fit within the domain where the quantiles do not cross valid_pred_intvl_idx = ( df_quantiles [ 'p84' ] - df_quantiles [ 'p16' ]) . pipe ( lambda s : s [ s > 0 ]) . index # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 , label = 'Observations' ) ax . fill_between ( valid_pred_intvl_idx , df_quantiles . loc [ valid_pred_intvl_idx , 'p16' ], df_quantiles . loc [ valid_pred_intvl_idx , 'p84' ], color = 'r' , alpha = 0.25 , label = '68% Prediction Interval' ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 0 ] . _sizes = [ 20 ] lgnd . legendHandles [ 0 ] . set_alpha ( 1 ) With the prediction interval we've looked at the likely range of power output for given wind speeds, but what if instead of the range of the underlying values we wanted to know the range in our estimate of the average power curve? For this we can use confidence intervals, which express the certainty we have in the particular statistical parameter we're calculating. In order to estimate this uncertainty we'll first bootstrap our model. df_bootstrap = lowess . bootstrap_model ( x , y , num_runs = 500 , frac = 0.2 , num_fits = 30 ) df_bootstrap . head () 100% 500/500 [00:55 < 00:00, 0.11s/it] x 0 1 2 3 4 5 6 7 8 9 ... 490 491 492 493 494 495 496 497 498 499 15.51 1977.78 1978.05 1978.95 1978.33 1979.43 1978.33 1978.78 1979.79 1979.47 1979 ... 1978.14 1978.5 1977.23 1977.66 1976.37 1976.28 1978.61 1978.84 1977.94 1977.22 15.7101 1978.48 1978.6 1979.52 1979.05 1979.96 1979.06 1979.39 1980.34 1980.1 1979.74 ... 1978.92 1979.14 1977.82 1978.38 1977.14 1977.14 1979.45 1979.56 1978.61 1977.96 16.6708 1981.24 1981.17 1981.83 1981.57 1981.87 1981.95 1981.61 1982.23 1982.3 1982.31 ... 1981.98 1981.56 1980.13 1981.42 1980.3 1980.6 1982.85 1981.93 1981.15 1980.83 15.2098 1976.43 1977.04 1977.8 1977.1 1978.41 1976.9 1977.58 1978.67 1978.21 1977.58 ... 1976.64 1977.27 1976.05 1976.45 1974.94 1974.57 1977.02 1977.52 1976.61 1975.82 15.44 1977.49 1977.84 1978.72 1978.03 1979.21 1978.03 1978.52 1979.56 1979.2 1978.69 ... 1977.83 1978.23 1976.99 1977.4 1976.06 1975.93 1978.28 1978.54 1977.66 1976.92 From the bootstrapped results we can then extract the confidence intervals, in our case we'll look at the range covering 95% of our estimates. df_conf_intvl = lowess . get_confidence_interval ( df_bootstrap , conf_pct = 0.95 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 , label = 'Observations' ) ax . fill_between ( df_conf_intvl . index , df_conf_intvl [ 'min' ], df_conf_intvl [ 'max' ], color = 'r' , alpha = 1 , label = '95% Confidence' ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 0 ] . _sizes = [ 20 ] lgnd . legendHandles [ 0 ] . set_alpha ( 1 ) We'll now visualise how the width of the confidence interval changes with wind speed. Interestingly the two troughs in the confidence interval width appear to correspond to the cut-in and start of the rated power wind speeds. fig , ax = plt . subplots ( dpi = 150 ) df_conf_intvl . diff ( axis = 1 )[ 'max' ] . plot ( ax = ax , color = 'r' , linewidth = 1 ) eda . hide_spines ( ax ) ax . legend ( frameon = False ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( '95% Confidence \\n Interval Width (MW)' ) ax . set_xlim ( 0 , 25 ) ax . set_ylim ( 0 ) ax . get_legend () . remove ()","title":"Power Curve Fitting"},{"location":"ug-03-power-curve/#power-curve-cleaning","text":"We'll start by loading the raw turbine wind speed and output data in df_raw_pc = pd . read_csv ( '../data/lowess_examples/turbine_power_wind_speed_raw.csv' ) df_raw_pc . head ( 3 ) Unnamed: 0 UTC active_power wind_speed 0 30/09/2017 18:26 14 4.02 1 30/09/2017 18:56 14 4.02 2 30/09/2017 19:26 14 4.02 We'll then try and fit a LOWESS estimate for the power curve. %% time x = df_raw_pc [ 'wind_speed' ] . values y = df_raw_pc [ 'active_power' ] . values lowess_model = lowess . Lowess () lowess_model . fit ( x , y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( x , y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 788 ms Unfortunately the fit is thrown by the large number of occurences where the farm is under-powered or set to output 0, we want to remove these so that we can estimate the 'standard' power curve. We'll create a quantile LOWESS fit to see if that helps us understand the data any better. # Estimating the quantiles df_quantiles = lowess . quantile_model ( x , y , frac = 0.2 , qs = np . linspace ( 0.025 , 0.975 , 41 ), num_fits = 40 ) # Cleaning names and sorting for plotting df_quantiles . columns = [ f 'p { int ( col * 100 ) } ' for col in df_quantiles . columns ] df_quantiles = df_quantiles [ df_quantiles . columns [:: - 1 ]] df_quantiles . head () 100% 41/41 [02:52 < 00:05, 4.20s/it] x p97 p95 p92 p90 p88 p85 p83 p80 p78 p76 ... p23 p21 p19 p16 p14 p12 p9 p7 p4 p2 0.25 -31.1389 -36.0637 -48.7875 -50.2252 -54.3752 -61.8818 -64.9263 -67.3705 -68.176 -69.0324 ... -78.7881 -73.9447 -69.322 -63.1938 -58.9408 -54.0337 -50.6578 -49.2366 -50.801 -51.8248 0.33 -24.4137 -30.6108 -43.6044 -45.5205 -49.913 -57.5452 -60.7292 -63.275 -64.2354 -65.2338 ... -77.3919 -72.8946 -68.5794 -62.7402 -58.6804 -53.9613 -50.6087 -49.1445 -50.546 -51.5789 0.41 -17.8378 -25.26 -38.5274 -40.8969 -45.5215 -53.2747 -56.5928 -59.2317 -60.3441 -61.4871 ... -76.014 -71.8585 -67.8522 -62.3073 -58.4534 -53.9134 -50.5746 -49.0702 -50.2914 -51.3353 0.49 -11.3659 -19.9748 -33.5237 -36.3257 -41.1744 -49.0455 -52.4947 -55.22 -56.4821 -57.7727 ... -74.6419 -70.8232 -67.1261 -61.8803 -58.2436 -53.8769 -50.5466 -49.0072 -50.0365 -51.0934 0.57 -4.9476 -14.7154 -28.557 -31.7751 -36.8429 -44.8302 -48.4098 -51.2177 -52.6278 -54.069 ... -73.2619 -69.7745 -66.3859 -61.4438 -58.0338 -53.8375 -50.5151 -48.9481 -49.7806 -50.8524 Plotting these we can see an interesting relationship, where for many of the lower quantiles the estimate peaks then quickly drops to around 0. It is the area below and to the right of these peaks that we want to remove from our power curve estimate. fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( x , y , s = 0.1 , color = 'k' , alpha = 1 ) df_quantiles . plot ( cmap = 'viridis' , legend = False , ax = ax ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( 0 ) (0.0, 2557.302509884304) We'll first identify the ratio between the peak value and the final value (above speeds of 25 m/s). We'll then remove points after the peak where the ratio between their peak and final values exceeds a defined threshold (in our case 2). N.b. there's probably a much nicer way to do this where the for loop isn't needed, this would be handy to implement as it would be good to have a vector containing all of the indexes that have been removed. exceeded_quantiles = (( df_quantiles . max () / df_quantiles . iloc [ - 1 ] . clip ( 0.1 ) > 2 ) . replace ( False , np . nan ) . dropna () . index ) s_maxs = df_quantiles [ exceeded_quantiles ] . max () s_idxmaxs = df_quantiles [ exceeded_quantiles ] . idxmax () cleaned_x = x cleaned_y = y for exceeded_quantile in exceeded_quantiles : min_x = s_idxmaxs [ exceeded_quantile ] max_y = s_maxs [ exceeded_quantile ] idxs_to_remove = ( cleaned_x > min_x ) & ( cleaned_y < max_y ) cleaned_x = cleaned_x [ ~ idxs_to_remove ] cleaned_y = cleaned_y [ ~ idxs_to_remove ] Visualising the results we can clearly see that the unwanted periods have been removed fig , axs = plt . subplots ( dpi = 250 , ncols = 2 , figsize = ( 10 , 4 )) axs [ 0 ] . scatter ( x , y , s = 0.1 , color = 'k' , alpha = 1 ) axs [ 1 ] . scatter ( cleaned_x , cleaned_y , s = 0.1 , color = 'k' , alpha = 1 ) axs [ 0 ] . set_title ( 'Original' ) axs [ 1 ] . set_title ( 'Cleaned' ) for ax in axs : eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) We're now ready to make our power curve LOWESS estimate again %% time lowess_model = lowess . Lowess () lowess_model . fit ( cleaned_x , cleaned_y , frac = 0.2 , num_fits = 100 ) x_pred = np . linspace ( 0 , 25 , 101 ) y_pred = lowess_model . predict ( x_pred ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . plot ( x_pred , y_pred , label = 'Robust LOWESS' , color = 'r' , linewidth = 1 ) ax . scatter ( cleaned_x , cleaned_y , label = 'Observations' , s = 0.5 , color = 'k' , linewidth = 0 , alpha = 1 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Wind Speed (m/s)' ) ax . set_ylabel ( 'Power Output (MW)' ) ax . set_xlim ( 0 , 26 ) ax . set_ylim ( - 25 ) lgnd = ax . legend ( frameon = False ) lgnd . legendHandles [ 1 ] . _sizes = [ 10 ] lgnd . legendHandles [ 1 ] . set_alpha ( 1 ) Wall time: 728 ms","title":"Power Curve Cleaning"},{"location":"ug-04-gb-mcc/","text":"Marginal Cost Curve for Dispatchable Power in Great Britain \u00b6 In this example we'll estimate the marginal cost curve over the last two months for dispatchable power in Great Britain using data from Electric Insights. Imports \u00b6 import numpy as np import pandas as pd import matplotlib.pyplot as plt from moepy import retrieval , eda , lowess Data Loading \u00b6 We'll start by loading in the necessary data from Electric Insights %% time current_dt = pd . Timestamp . now () start_date = ( current_dt - pd . Timedelta ( weeks = 8 )) . strftime ( '%Y-%m- %d %H:%M' ) end_date = current_dt . strftime ( '%Y-%m- %d %H:%M' ) renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } df = retrieval . retrieve_streams_df ( start_date , end_date , renaming_dict = renaming_dict ) df . head () Wall time: 7.75 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2021-02-01 00:00:00+00:00 51.99 1 68.95 68.95 2.9 4797.76 175.8 5.564 1.945 0.465 ... 27.291 0 3.02828 3.51436 0.902 0 1.806 0 0.018 -0.05 2021-02-01 00:30:00+00:00 54.19 2 69 69 2.9 5149.7 186.031 5.559 1.963 0.563 ... 27.682 0 2.90388 3.44746 0.902 0 1.806 0 0.016 0.016 2021-02-01 01:00:00+00:00 55.07 3 75 75 2.9 5177.97 189.309 5.565 2.077 0.68 ... 27.352 0 2.76413 3.36153 0.952 0 1.906 0 0.018 0.018 2021-02-01 01:30:00+00:00 56.3 4 72 72 2.9 5131.08 190.892 5.563 2.122 0.716 ... 26.8796 0 2.62404 3.19386 0.952 0 1.906 0 0.016 0.016 2021-02-01 02:00:00+00:00 56.71 5 75 75 2.9 5105.37 193.368 5.561 2.134 0.718 ... 26.4023 0 2.41751 2.93459 0.926 0 1.906 0 0.018 0.018 We'll quickly visualise the relationship between price and dispatchable load for each half-hour period df_model = df [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () . astype ( float ) s_price = df_model [ 'day_ahead_price' ] s_dispatchable = df_model [ 'demand' ] - df_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable , s_price , s = 0.5 , alpha = 0.25 , color = 'k' ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)') Marginal Cost Curve Estimation \u00b6 We're now ready to fit our LOWESS model x_pred = np . linspace ( 10 , 40 , 301 ) y_pred = lowess . lowess_fit_and_predict ( s_dispatchable . values , s_price . values , frac = 0.25 , num_fits = 25 , x_pred = x_pred ) pd . Series ( y_pred , index = x_pred ) . plot () <AxesSubplot:> We'll then visualise the estimated fit alongside the historical observations fig , ax = plt . subplots ( dpi = 250 ) ax . plot ( x_pred , y_pred , linewidth = 1.5 , color = 'r' ) ax . scatter ( s_dispatchable , s_price , color = 'k' , s = 0.5 , alpha = 0.25 ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) fig . savefig ( '../img/latest_gb_mcc.png' , dpi = 250 )","title":"Marginal Cost Curve"},{"location":"ug-04-gb-mcc/#marginal-cost-curve-for-dispatchable-power-in-great-britain","text":"In this example we'll estimate the marginal cost curve over the last two months for dispatchable power in Great Britain using data from Electric Insights.","title":"Marginal Cost Curve for Dispatchable Power in Great Britain"},{"location":"ug-04-gb-mcc/#imports","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from moepy import retrieval , eda , lowess","title":"Imports"},{"location":"ug-04-gb-mcc/#data-loading","text":"We'll start by loading in the necessary data from Electric Insights %% time current_dt = pd . Timestamp . now () start_date = ( current_dt - pd . Timedelta ( weeks = 8 )) . strftime ( '%Y-%m- %d %H:%M' ) end_date = current_dt . strftime ( '%Y-%m- %d %H:%M' ) renaming_dict = { 'pumpedStorage' : 'pumped_storage' , 'northernIreland' : 'northern_ireland' , 'windOnshore' : 'wind_onshore' , 'windOffshore' : 'wind_offshore' , 'prices_ahead' : 'day_ahead_price' , 'prices' : 'imbalance_price' , 'temperatures' : 'temperature' , 'totalInGperkWh' : 'gCO2_per_kWh' , 'totalInTperh' : 'TCO2_per_h' } df = retrieval . retrieve_streams_df ( start_date , end_date , renaming_dict = renaming_dict ) df . head () Wall time: 7.75 s local_datetime day_ahead_price SP imbalance_price valueSum temperature TCO2_per_h gCO2_per_kWh nuclear biomass coal ... demand pumped_storage wind_onshore wind_offshore belgian dutch french ireland northern_ireland irish 2021-02-01 00:00:00+00:00 51.99 1 68.95 68.95 2.9 4797.76 175.8 5.564 1.945 0.465 ... 27.291 0 3.02828 3.51436 0.902 0 1.806 0 0.018 -0.05 2021-02-01 00:30:00+00:00 54.19 2 69 69 2.9 5149.7 186.031 5.559 1.963 0.563 ... 27.682 0 2.90388 3.44746 0.902 0 1.806 0 0.016 0.016 2021-02-01 01:00:00+00:00 55.07 3 75 75 2.9 5177.97 189.309 5.565 2.077 0.68 ... 27.352 0 2.76413 3.36153 0.952 0 1.906 0 0.018 0.018 2021-02-01 01:30:00+00:00 56.3 4 72 72 2.9 5131.08 190.892 5.563 2.122 0.716 ... 26.8796 0 2.62404 3.19386 0.952 0 1.906 0 0.016 0.016 2021-02-01 02:00:00+00:00 56.71 5 75 75 2.9 5105.37 193.368 5.561 2.134 0.718 ... 26.4023 0 2.41751 2.93459 0.926 0 1.906 0 0.018 0.018 We'll quickly visualise the relationship between price and dispatchable load for each half-hour period df_model = df [[ 'day_ahead_price' , 'demand' , 'solar' , 'wind' ]] . dropna () . astype ( float ) s_price = df_model [ 'day_ahead_price' ] s_dispatchable = df_model [ 'demand' ] - df_model [[ 'solar' , 'wind' ]] . sum ( axis = 1 ) # Plotting fig , ax = plt . subplots ( dpi = 150 ) ax . scatter ( s_dispatchable , s_price , s = 0.5 , alpha = 0.25 , color = 'k' ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Wind + Solar] (GW)' ) ax . set_ylabel ( 'Price (\u00a3/MWh)' ) Text(0, 0.5, 'Price (\u00a3/MWh)')","title":"Data Loading"},{"location":"ug-04-gb-mcc/#marginal-cost-curve-estimation","text":"We're now ready to fit our LOWESS model x_pred = np . linspace ( 10 , 40 , 301 ) y_pred = lowess . lowess_fit_and_predict ( s_dispatchable . values , s_price . values , frac = 0.25 , num_fits = 25 , x_pred = x_pred ) pd . Series ( y_pred , index = x_pred ) . plot () <AxesSubplot:> We'll then visualise the estimated fit alongside the historical observations fig , ax = plt . subplots ( dpi = 250 ) ax . plot ( x_pred , y_pred , linewidth = 1.5 , color = 'r' ) ax . scatter ( s_dispatchable , s_price , color = 'k' , s = 0.5 , alpha = 0.25 ) ax . set_ylim ( - 20 , 150 ) eda . hide_spines ( ax ) ax . set_xlabel ( 'Demand - [Solar + Wind] (GW)' ) ax . set_ylabel ( 'Day-Ahead Price (\u00a3/MWh)' ) fig . savefig ( '../img/latest_gb_mcc.png' , dpi = 250 )","title":"Marginal Cost Curve Estimation"}]}